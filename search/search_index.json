{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"cite/","title":"Citing Neurobagel","text":"<p>If you use Neurobagel in your research, we recommend citing the Zenodo DOI associated with the Neurobagel tool and version you used.</p> Tool Zenodo reference Annotation tool CLI API Query tool <p>Note</p> <p>If you used the query tool, we recommend also citing the API.</p>"},{"location":"getting_help/","title":"Getting Help","text":"<p>There are several ways you can interact with the Neurobagel team:</p> <ul> <li>Join our Discord server to chat with us</li> <li>Create an issue in our GitHub repositories</li> <li>Our tag on the Neurostars forum</li> </ul>"},{"location":"getting_help/#general-usage-questions","title":"General usage questions","text":"<p>If you have a usage question about Neurobagel (feedback on a specific use case), or want to discuss the Neurobagel project in general with the development team or other users, please either:</p> <ul> <li>send us a message in the Neurobagel Discord server (recommended - quickest way to get help)</li> <li>search for or start a new thread on Neurostars with the <code>neurobagel</code> tag: https://neurostars.org/tag/neurobagel</li> </ul>"},{"location":"getting_help/#problem-with-deploying-a-neurobagel-node","title":"Problem with deploying a Neurobagel node","text":"<p>If you encounter a problem while deploying a Neurobagel node, first make sure you are following our getting started guide correctly. If your problem persists, please open a node deployment issue on GitHub.</p>"},{"location":"getting_help/#report-a-bug-or-request-a-feature","title":"Report a bug or request a feature","text":"<p>For bug reports or feature requests, we prefer if you open an issue in the repository for the relevant tool in the Neurobagel GitHub organization.</p> <p>If you are using one of our web tools (https://annotate.neurobagel.org or https://query.neurobagel.org), you may also submit bug reports and feature requests using the feedback widget directly on the app.</p>"},{"location":"getting_help/#documentation-feedback","title":"Documentation feedback","text":"<p>We are always looking to improve our user documentation. To request a change or addition to existing documentation, please open an issue in the <code>neurobagel/documentation</code> repo.</p>"},{"location":"getting_help/#propose-a-new-variable-to-be-added-to-the-neurobagel-data-model","title":"Propose a new variable to be added to the Neurobagel data model","text":"<p>If there is a variable you'd like to be able to harmonize and query using Neurobagel that is not currently supported, please suggest it using a discussion in our dedicated GitHub Discussion category.</p>"},{"location":"glossary/","title":"Glossary","text":"<p>This glossary compiles some key terms used in the Neurobagel documentation and defines them in the context of the Neurobagel ecosystem.</p>"},{"location":"glossary/#data-dictionary","title":"Data dictionary","text":"A JSON file that describes the information contained in columns from a tabular data file, along with the meaning and properties (format of numerical data, unique \u201clevels\u201d of categorical data, etc.) of values in each column. In the context of Neurobagel, the meanings of columns and column values are encoded using terms from standardised vocabularies."},{"location":"glossary/#data-model","title":"Data model","text":"Used interchangeably with: data schema <p>A structure that has been designed with the purpose to represent a specific kind of information. A data model is made up of generic types or classes that are relevant to the data model designers (for Neurobagel, examples include \"Research Participant\" and \"Neuroimaging Dataset\"), the properties these types can have (e.g., \"Age in years\", \"Dataset name\"), and the relationships that can exist between them (e.g., \"is part of\"). The goal of a data model is to give information a structure so that we can write programs that can consume the information.</p> <p>The Neurobagel data model is designed to represent the kind of information that is important to support the most relevant cohort definition queries, and thus models types, properties, and relationships that are important for this purpose. It is not a static thing, and we constantly add new things to the data model as we support new use cases that rely on this information.</p>"},{"location":"glossary/#controlled-term","title":"Controlled term","text":"<p>A unique identifier or code for a concept that is described in a controlled vocabulary.</p> <p>A controlled term has a</p> <ul> <li>a clear definition</li> <li>a unique and persistent identifier</li> <li>from a specific curated list of terms like a vocabulary, taxonomy or ontology</li> </ul> <p>An example is the controlled term for \"Parkinson's disease\" from the ICD-11 taxonomy with the unique code <code>8A00.0</code>.</p>"},{"location":"glossary/#controlled-vocabulary","title":"Controlled vocabulary","text":"Used interchangeably with: taxonomy, and ontology <p>A controlled vocabulary is a collection of controlled terms that are often all about one specific topic. The main benefit of a controlled vocabulary is that it provides unambiguous terms with clear definitions that people have agreed to use to describe their information - removing the need to align variable names and value formats between datasets and enabling interoperability.</p> <p>For example, most websites use the schema.org vocabulary to describe things like products to purchase, events to book, recipes to cook etc. in a consistent way that can be understood by the search spiders of big search engines.</p> Reusing controlled vocabularies <p>Creating a controlled vocabulary is a laborious task that involves deep subject matter expertise, often from many experts, and needs to be maintained to remain relevant. You should therefore almost always reuse an existing vocabulary rather than creating your own.</p> <p>A taxonomy is a more specific form of a controlled vocabulary that organizes terms into hierarchical relationships. For example, a \"Recipe\" in schema.org is a subtype of a \"HowTo\" which itself is a subtype of a \"CreativeWork\". This hierarchy let's you do things like search for \"CreativeWork\" and also find \"Recipe\", even if you have never made this link directly.</p> <p>An ontology is an even more specific form of a taxonomy where terms can have very complex relationships with each other that include logical constraints. In an ontology, you could for example express that for someone to be a \"sister\" to someone else, both the subject and the object of the relationship have to be \"human\", only the subject of the relation has to be \"female\", and both have to have at least one parent in common. These complex expressions are very labour intensive to create but can provide also very rich ways of validating and even inferring information.</p>"},{"location":"glossary/#graph-database","title":"Graph database","text":"Used interchangeably with: knowledge graph store, graph store, graph <p>A type of database, in the same way that a relational databases is a type of database. The main distinguishing feature of graph databases is that they represent entities as nodes in a graph, and relationships between entities as edges between these nodes. This data model makes it easy to easily add new information by drawing a new edge between two nodes.</p> Note <p>A single Neurobagel graph database can contain harmonised information about multiple datasets and their respective subjects. Each subject is represented by a node, and their harmonised phenotypic and imaging data characteristics are described using controlled terms connected to the subject node via a series of edges that individually encode the type of attribute described by the controlled term.</p> <p>Neurobagel uses the RDF graph data model, see also https://en.wikipedia.org/wiki/Graph_database.</p>"},{"location":"glossary/#annotation","title":"Annotation","text":"In the context of Neurobagel, annotation refers to the process of describing tabular demographic, cognitive, and/or clinical (phenotypic) data for a dataset with terms from controlled vocabularies to create machine understandable data dictionaries for the data. You can learn more about this process in our documentation."},{"location":"glossary/#aggregated-results","title":"Aggregated results","text":"If the owner of a Neurobagel node decides that query responses should not include information at the level of individual participants, they can configure their node to only return aggregated results. In this mode, the node will aggregate all participants that match a query at the dataset level and only respond with counts of matching participants."},{"location":"glossary/#data-owner","title":"Data owner","text":"A person or an institute who is responsible in the data governance sense for one or many datasets. In the context of Neurobagel, one data owner can have one or more Neurobagel nodes, but every Neurobagel node can only have one data owner who is responsible for all of the data stored inside the node."},{"location":"glossary/#federation-api","title":"Federation API","text":"Used interchangeably with: f-API <p>A standalone service that allows query users to send a single query and have it automatically sent to many Neurobagel node APIs (n-API) without having to know where these node APIs are located. The f-API takes care of keeping an up to date list of available n-APIs, federating queries, retrieving and combining results, and returning them to the user.</p> <p>Designed to very closely resemble the behaviour and the endpoints of a n-API so that services can be built that are able to work either directly with a single n-API or with an f-API.</p>"},{"location":"glossary/#node-api","title":"Node API","text":"Used interchangeably with: n-API <p>A Neurobagel \"node\" is a locally deployed service that holds information about data for one data owner who controls and manages the node. A node has two core components:</p> <ul> <li>a graph backend to store the harmonised data for querying</li> <li>a RESTful node API that exposes query endpoints for users or programs to send queries and retrieve results</li> </ul> <p>One important purpose of the n-API is to act as a barrier between the user and the graph backend so that the user cannot execute arbitrary queries on the graph, and the data owner can control how detailed the query responses should be.</p>"},{"location":"glossary/#tabular-data","title":"Tabular data","text":"Used interchangeably with: phenotypic data <p>Tabular text files (e.g., .tsv or .csv) that contain information about participants such as their demographic information or data from cognitive or clinical assessments they have completed. We often refer to this information as phenotypic data because they describe observable characteristics of the participant.</p>"},{"location":"glossary/#tsv","title":"TSV","text":"<p>A Tab-Separated Values (TSV) file has the <code>.tsv</code> extension and is a plain text file structured as a table, where values belonging to different columns are separated by a single tab character (<code>\\t</code>). Each column represents a field of interest, and each row represents a single datapoint. The first line of the file (the header) contains the names of each column.</p> <p>A valid TSV should also follow some common formatting guidelines:</p> <ul> <li>Each line must contain the same number of tab-separated fields (columns), even if some are empty</li> <li>Column names must be unique (no duplicates)</li> <li>Do not include completely blank rows or columns</li> <li>Avoid leading or trailing spaces in column names and values</li> </ul> <p>Most spreadsheet software (e.g., Microsoft Excel, Google Sheets) will allow you to save a file as TSV (this webpage has guides on creating TSVs in common applications), but it is your responsibility to ensure that the file is well-structured according to TSV formatting conventions.</p>"},{"location":"contributing/CONTRIBUTING/","title":"Contributing Guidelines","text":"<p>We're so excited that you're interested in contributing to Neurobagel! </p> <p>We appreciate all contributions, and hope that the below guidelines will make it as easy as possible for you to contribute to the Neurobagel codebase and to ensure your contribution can be easily integrated.</p>"},{"location":"contributing/CONTRIBUTING/#contributing-through-github","title":"Contributing through GitHub","text":"<p>In order to contribute to Neurobagel, you'll need to set up a GitHub account and sign in. Here are some instructions to help you get started.</p>"},{"location":"contributing/CONTRIBUTING/#identifying-an-issue-to-contribute-to","title":"Identifying an issue to contribute to","text":"<p>The best way to get started contributing is to explore the list of open issues in one of our GitHub repositories. (For example, see the open issues for the Neurobagel API.)</p> <p>When you are ready to contribute, we welcome you to join the conversation through one of these issues, or open a new issue referencing a change you would like to see or contribute. Ensuring that a relevant issue is open before you start contributing code is important because it allows others in the project to discuss your idea and tell you where your contribution would be the most helpful.</p> <ul> <li> <p>If the issue you want to work on already exists: Comment on the open issue to indicate you would like to work on it, along with any clarification/implementation questions you have</p> <ul> <li>If someone is already assigned to the issue, the task is actively being worked on and a solution will soon be proposed. Feel free to share some helpful resources or pointers that may be interesting to the person who is working the issue, and/or check back in a couple of days.</li> </ul> </li> <li> <p>If the issue you want to work on does not exist: Open a new issue describing your proposed change and why it is necessary/beneficial. The more detail here, the better!</p> </li> </ul> <p>This allows members of the Neurobagel developer team to confirm that you will not be overlapping with currently active work and that everyone is on the same page about the task to be accomplished.</p> <p>If you would like to contribute but are not sure where to start, we recommend looking for open issues with the following labels:</p> <p> Issue that is good for a new or beginner contributor, as it does not involve a steep learning curve or advanced understanding of the codebase. (Please note: if you're a seasoned contributor, we would appreciate if you could select a different issue to work on to keep these available for less experienced folks!)</p> <p> Issue that is not an internal priority, but external pull requests to address it are welcome.</p> <p>: Issue that should involve minimal planning or implementation work, given an understanding of the relevant code.</p>"},{"location":"contributing/CONTRIBUTING/#making-a-change","title":"Making a change","text":"<p>All Neurobagel issues are expected to be addressed through pull requests.</p> <p>As an external contributor, the process you would follow to make your proposed changes should look something like this:</p>"},{"location":"contributing/CONTRIBUTING/#1-fork-the-relevant-neurobagel-repository-to-your-profile","title":"1. Fork the relevant Neurobagel repository to your profile","text":""},{"location":"contributing/CONTRIBUTING/#2-clone-your-fork-of-the-neurobagel-repository-to-your-local-machine","title":"2. Clone your fork of the Neurobagel repository to your local machine","text":"<p>To keep up with changes in the Neurobagel repository while you work and avoid merge conflicts later on, make sure to:</p> <ul> <li>Add the \"upstream\" Neurobagel repository as a remote to your locally cloned repository</li> <li>Keep your fork up to date with the upstream repository</li> </ul>"},{"location":"contributing/CONTRIBUTING/#3-set-up-a-development-environment","title":"3. Set up a development environment","text":"<p>Refer to the README of the Neurobagel repository you are contributing to for instructions on setting up a development environment so you can test any local code changes you make. Note that the steps to set up a development environment (usually under a \"Local installation\", \"Manual installation\", or \"Development environment\" section) are generally different than those used to install the tool purely as a user, which usually appears at the top of the README.</p> <p>For example, for the Neurobagel CLI:</p> <ul> <li>\"Development\" mode installation steps</li> <li>Normal (non-development mode) installation steps</li> </ul>"},{"location":"contributing/CONTRIBUTING/#follow-repository-code-style","title":"Follow repository code style","text":"<p>Most Neurobagel repositories use tools to apply automatic code formatting and linting according to the project's code style, which are typically configured (but not automatically enabled) in the development environment for these repositories. At this point, please follow the repository instructions to set up these tools before you begin your work to ensure your contribution matches the existing code.</p> <p>For example, our repositories written in Python have pre-commit configured for this purpose.</p> <p>To tell pre-commit to run on any local changes you make, run the following from the repository root of your local clone:</p> <pre><code>pre-commit install\n</code></pre> <p>Now, a number of code linters and formatters will run automatically when you attempt to make a commit, which will keep your changes consistent with the rest of the codebase.</p>"},{"location":"contributing/CONTRIBUTING/#4-create-a-new-branch-to-make-the-proposed-changes","title":"4. Create a new branch to make the proposed changes","text":"<p>Please consider using descriptive branch names. Some examples:</p> <ul> <li><code>&lt;username&gt;/&lt;issue-identifier&gt;</code> (<code>jsmith/fix-1234</code>)</li> <li><code>&lt;username&gt;/&lt;brief-change-title&gt;</code> (<code>jsmith/add-logging</code>, <code>jsmith/enh/add-logging</code>)</li> </ul> <p>Once you are satisfied with your local changes, add, commit, and push them to your branch on your forked repository on GitHub.</p>"},{"location":"contributing/CONTRIBUTING/#5-open-a-pull-request","title":"5. Open a pull request","text":"<p>See the below sections for information on how to submit your pull request and what to expect in a pull request review.</p>"},{"location":"contributing/CONTRIBUTING/#pull-request-guidelines","title":"Pull request guidelines","text":"<p>When you first open a pull request, you should automatically see a template in the pull request body that looks something like this that you can fill out. The template is designed to make it easier for maintainers to review your pull request, but feel free to add any additional information that you feel is useful or necessary.</p> <p>Pull request titles should begin with a descriptive prefix (e.g., \"[ENH] Implement check for presence of a session ID column\"):</p> <ul> <li><code>[ENH]:</code> Feature improvements or additions</li> <li><code>[REF]:</code> Refactoring existing code</li> <li><code>[TST]</code>: Updating or adding a test</li> <li><code>[CI]</code>: Automation-related changes</li> <li><code>[MNT]</code>: General maintenance not covered by <code>[REF]</code>, <code>[TST]</code>, or <code>[CI]</code></li> <li><code>[INF]</code>: Software or graph infrastructure-related changes</li> <li><code>[FIX]</code>: Bug fixes</li> <li><code>[MODEL]</code>: Updates or changes related to the Neurobagel data model</li> <li><code>[DOC]</code>: Documentation-only changes to a code repo (READMEs, within-code documentation, etc.)<ul> <li>Exception: changes to the <code>documentation</code> repo should use one of the below PR prefixes instead of <code>[DOC]</code></li> </ul> </li> </ul>"},{"location":"contributing/CONTRIBUTING/#pull-requests-to-the-documentation-repo","title":"Pull requests to the <code>documentation</code> repo","text":"<p>In PRs to the Neurobagel documentation, using the <code>[DOC]</code> title prefix is discouraged as it is too broad. Instead, for documentation content changes, the following prefixes can be used to specify the nature of the change:</p> <ul> <li><code>[ENH]</code>: Updating or adding new documentation</li> <li><code>[REF]</code>: Simplifying or restructuring documentation (i.e., pages, sections, paragraphs)</li> <li><code>[FIX]</code>: Fixing errors in the documentation</li> </ul>"},{"location":"contributing/CONTRIBUTING/#pull-request-reviews","title":"Pull request reviews","text":"<p>A maintainer will review each PR and may provide comments or suggestions for you to address.</p> <p>Neurobagel PR reviews may use the following emoji signifiers:</p> <p>: Ready to merge or approved without suggestions</p> <p>: Some optional/suggested changes that could be nice to have but are not required to merge</p> <p>If (required) changes are requested, please re-request a review from the reviewer once the comments have been addressed.</p>"},{"location":"contributing/CONTRIBUTING/#when-your-pull-request-is-approved","title":"When your pull request is approved","text":"<p>If you do not have write access to the repository: the reviewing Neurobagel maintainer is responsible for merging the PR.</p> <p>If you have write access to the repository: the PR author is responsible for merging the PR.</p>"},{"location":"contributing/CONTRIBUTING/#have-a-question-about-contributing","title":"Have a question about contributing?","text":"<p>At any point during a contribution, please do not hesitate to mention one of the core maintainers if you have a question or need further guidance, in either the issue or pull request.</p> <p>If you have ideas for improving this page, please help us improve it by opening an issue .</p>"},{"location":"contributing/team/","title":"Our team","text":"<p>Neurobagel is a project originating from the ORIGAMI Lab at the Montreal Neurological Institute in collaboration with the Douglas Research Centre.</p>"},{"location":"contributing/team/#developers","title":"Developers","text":"<p>\ud83e\uddd1\u200d\ud83c\udf73 Core maintainer \ud83e\uddd1\u200d\ud83d\udd2c Principal Investigator</p>"},{"location":"contributing/team/#current","title":"Current","text":"<sub>Sebastian Urchs</sub>\ud83e\uddd1\u200d\ud83c\udf73 <sub>Arman Jahanpour</sub>\ud83e\uddd1\u200d\ud83c\udf73 <sub>Alyssa Dai</sub>\ud83e\uddd1\u200d\ud83c\udf73 <sub>Nikhil Bhagwat</sub> <sub>Michelle Wang</sub> <sub>Brent McPherson</sub> <sub>Remi Gau</sub> <sub>Jean-Baptise Poline</sub>\ud83e\uddd1\u200d\ud83d\udd2c"},{"location":"contributing/team/#past","title":"Past","text":"<sub>Jonathan Armoza</sub>\ud83e\uddd1\u200d\ud83c\udf73"},{"location":"data_models/communities/","title":"Community vocabularies for consortia (beta)","text":"<p>Info</p> <p>This section describes an early-stage feature under active development, currently targeted at research consortia.</p> <p>We are actively developing support for Neurobagel \"community\" vocabularies: custom standardized vocabularies of terms which are curated and governed by research consortia. This feature allows you to use more detailed or domain-specific standardized terms than are available in the core set of external ontologies provided by Neurobagel, providing more precision when harmonizing phenotypic data across sites in your consortium.</p> <p>Neurobagel services provide early-stage support for community vocabularies. For example, members of your consortium can use our beta annotation tool to annotate their tabular data with your specific community vocabulary.</p> <p>Example of community vocabulary selection in the beta annotation tool:</p> <p></p> <p>We have developed an initial workflow for creating a community vocabulary of assessment terms for Neurobagel. If your research consortium is interested in being onboarded as a Neurobagel community, we invite you to contact the Neurobagel maintainer team.</p> <p>For more information on creating a community vocabulary for your consortium, see the neurobagel/communities repository.</p>"},{"location":"data_models/dictionaries/","title":"Neurobagel data dictionaries","text":""},{"location":"data_models/dictionaries/#overview","title":"Overview","text":"<p>When you annotate a phenotypic TSV using the Neurobagel annotation tool (see also the section on the annotation tool), your annotations are automatically stored in a JSON data dictionary. A Neurobagel data dictionary essentially describes the meaning and properties of columns and column values using standardized vocabularies.</p> <p>Example</p> <p>A comprehensive example data dictionary containing all currently supported phenotypic attributes and annotations can be found here (corresponding phenotypic .tsv).</p> <p>Importantly, Neurobagel uses a structure for these data dictionaries that is compatible with and expands on BIDS <code>participant.json</code> data dictionaries.</p> <p>Info</p> <p>The specification for how a Neurobagel data dictionary is structured is also called a schema. Because Neurobagel data dictionaries are stored as <code>.json</code> files, we use the <code>jsonschema</code> schema language to write the specification.</p> <p>Neurobagel data dictionaries uniquely include an <code>Annotations</code> attribute for each column entry to store user-provided semantic annotations.</p> <p>Here is an example BIDS data dictionary (<code>participants.json</code>):</p> <pre><code>{\n  \"age\": {\n    \"Description\": \"age of the participant\",\n    \"Units\": \"years\"\n  },\n  \"sex\": {\n    \"Description\": \"sex of the participant as reported by the participant\",\n    \"Levels\": {\n      \"M\": \"male\",\n      \"F\": \"female\"\n    }\n  }\n}\n</code></pre> <p>And here is the same data dictionary augmented with Neurobagel annotations:</p> <pre><code>{\n  \"age\": {\n    \"Description\": \"age of the participant\",\n    \"Units\": \"years\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Age\",\n        \"Label\": \"Age\"\n      },\n      \"Format\": {\n        \"TermURL\": \"nb:FromFloat\",\n        \"Label\": \"Float\"\n      },\n      \"VariableType\": \"Continuous\"\n    }\n  },\n  \"sex\": {\n    \"Description\": \"sex of the participant as reported by the participant\",\n    \"Levels\": {\n      \"M\": \"male\",\n      \"F\": \"female\"\n    },\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Sex\",\n        \"Label\": \"Sex\"\n      },\n      \"Levels\": {\n        \"M\": {\n          \"TermURL\": \"snomed:248153007\",\n          \"Label\": \"Male\"\n        },\n        \"F\": {\n          \"TermURL\": \"snomed:248152002\",\n          \"Label\": \"Female\"\n        }\n      },\n      \"MissingValues\": [\n        \"\",\n        \" \"\n      ],\n      \"VariableType\": \"Categorical\"\n    }\n  }\n}\n</code></pre> <p>Info</p> <p><code>TermURL</code> values in Neurobagel data dictionaries are compact URIs.</p> <p>A custom Neurobagel namespace, defined by the prefix <code>nb</code> (full URI: <code>http://neurobagel.org/vocab/</code>), is used for controlled terms that represent attribute classes modelled by Neurobagel, such as <code>\"Age\"</code> and <code>\"Sex\"</code>, even though these terms may have equivalents in other vocabularies used for annotation.</p> <p>For example, the following terms from the Neurobagel annotations above are conceptually equivalent to terms from the SNOMED CT namespace:</p> Neurobagel namespace term Equivalent external controlled vocabulary term http://neurobagel.org/vocab/Age http://purl.bioontology.org/ontology/SNOMEDCT/397669002 http://neurobagel.org/vocab/Sex http://purl.bioontology.org/ontology/SNOMEDCT/184100006"},{"location":"data_models/dictionaries/#phenotypic-attributes","title":"Phenotypic attributes","text":"<p>The Neurobagel annotation tool generates a data dictionary entry for a given column by augmenting the information recommended by BIDS with unambiguous semantic tags.</p> <p>Below we'll outline several example annotations using the following example <code>participants.tsv</code> file:</p> participant_id session_id group age sex updrs_1 updrs_2 sub-01 ses-01 PAT 25 M 2 sub-01 ses-02 PAT 26 M 3 5 sub-02 ses-01 CTL 28 F 1 1 sub-02 ses-02 CTL 29 F 1 1 <p>Controlled terms in the below examples are shortened using the RDF prefix/context syntax for json-ld:</p> <pre><code>{\n  \"@context\": {\n    \"nb\": \"http://neurobagel.org/vocab/\",\n    \"ncit\": \"http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#\",\n    \"nidm\": \"http://purl.org/nidash/nidm#\",\n    \"snomed\": \"http://purl.bioontology.org/ontology/SNOMEDCT/\"\n  }\n}\n</code></pre>"},{"location":"data_models/dictionaries/#participant-identifier","title":"Participant identifier","text":"<p>Term from the Neurobagel vocabulary.</p> <pre><code>{\n  \"participant_id\": {\n    \"Description\": \"A participant ID\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:ParticipantID\",\n        \"Label\": \"Subject Unique Identifier\"\n      },\n      \"VariableType\": \"Identifier\"\n    }\n  }\n}\n</code></pre> <p>Info</p> <p><code>participant_id</code> is a reserved name in BIDS and BIDS data dictionaries therefore typically don't annotate this column. Neurobagel supports tables containing multiple subject ID columns for studies that employ more than one ID scheme.</p>"},{"location":"data_models/dictionaries/#session-identifier","title":"Session identifier","text":"<p>Term from the Neurobagel vocabulary.</p> <pre><code>{\n  \"session_id\": {\n    \"Description\": \"A session ID\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:SessionID\",\n        \"Label\": \"Run Identifier\"\n      },\n      \"VariableType\": \"Identifier\"\n    }\n  }\n}\n</code></pre> <p>Info</p> <p>Unlike the BIDS specification, Neurobagel supports a <code>participants.tsv</code> file with a <code>session_id</code> field.</p>"},{"location":"data_models/dictionaries/#diagnosis","title":"Diagnosis","text":"<p>Terms for clinical diagnosis are from the SNOMED-CT ontology. Terms for healthy control status are from the National Cancer Institute Thesaurus.</p> <pre><code>{\n  \"group\": {\n    \"Description\": \"Group variable\",\n    \"Levels\": {\n      \"PD\": \"Parkinson's patient\",\n      \"CTRL\": \"Control subject\",\n    },\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Diagnosis\",\n        \"Label\": \"Diagnosis\"\n      },\n      \"Levels\": {\n        \"PD\": {\n          \"TermURL\": \"snomed:49049000\",\n          \"Label\": \"Parkinson's disease\"\n        },\n        \"CTRL\": {\n          \"TermURL\": \"ncit:C94342\",\n          \"Label\": \"Healthy Control\"\n        }\n      },\n      \"VariableType\": \"Categorical\"\n    }\n  }\n}\n</code></pre> <p>The <code>IsAbout</code> relation uses a term from the Neurobagel namespace because <code>\"Diagnosis\"</code> is a standardized term.</p> <p>Info</p> <p>Columns with categorical values (e.g., study groups, diagnoses, sex) require a <code>Levels</code> key in their Neurobagel annotation. The Neurobagel \"Levels\" key is modeled after the BIDS \"Levels\" key for human readable descriptions.</p>"},{"location":"data_models/dictionaries/#sex","title":"Sex","text":"<p>Terms are from the SNOMED-CT ontology, which has controlled terms aligning with BIDS <code>participants.tsv</code> descriptions for sex.  Below are the SNOMED terms for the sex values allowed by BIDS:</p> Sex Controlled term Male http://purl.bioontology.org/ontology/SNOMEDCT/248153007 Female http://purl.bioontology.org/ontology/SNOMEDCT/248152002 Other http://purl.bioontology.org/ontology/SNOMEDCT/32570681000036106 <p>Here is what a sex annotation looks like in practice:</p> <pre><code>{\n  \"sex\": {\n    \"Description\": \"Sex variable\",\n    \"Levels\": {\n      \"M\": \"Male\",\n      \"F\": \"Female\"\n    },\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Sex\",\n        \"Label\": \"Sex\"\n      },\n      \"Levels\": {\n        \"M\": {\n          \"TermURL\": \"snomed:248153007\",\n          \"Label\": \"Male\"\n        },\n        \"F\": {\n          \"TermURL\": \"snomed:248152002\",\n          \"Label\": \"Female\"\n        }\n      },\n      \"VariableType\": \"Categorical\"\n    }\n  }\n}\n</code></pre> <p>The <code>IsAbout</code> relation uses a Neurobagel scoped term for <code>\"Sex\"</code> because this is a Neurobagel common data element.</p>"},{"location":"data_models/dictionaries/#age","title":"Age","text":"<p>Neurobagel has a common data element for <code>\"Age\"</code> describing a continuous column. To ensure age values are represented as floats in Neurobagel graphs, Neurobagel encodes the format of the raw numerical values in a given age column. This is stored in the <code>Format</code> annotation (required for continuous columns) and maps internally to a specific transformation that is then used to convert the raw values to floats.</p> <p>Possible formats:</p> TermURL Label Examples <code>nb:FromFloat</code> float value <code>31.5</code>, <code>31</code> <code>nb:FromEuro</code> european decimal value <code>31,5</code> <code>nb:FromBounded</code> bounded value <code>30+</code> <code>nb:FromRange</code> a range between a minimum and maximum value <code>30-35</code> <code>nb:FromISO8061</code> period of time defined according to the ISO8601 standard <code>31Y6M</code> <pre><code>{\n  \"age\": {\n    \"Description\": \"Participant age\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Age\",\n        \"Label\": \"Chronological age\"\n      },\n      \"Format\": {\n        \"TermURL\": \"nb:FromEuro\",\n        \"Label\": \"European value decimals\"\n      },\n      \"VariableType\": \"Continuous\"\n    }\n  }\n}\n</code></pre>"},{"location":"data_models/dictionaries/#assessment-tool","title":"Assessment tool","text":"<p>Terms are from the SNOMED-CT ontology.</p> <p>For assessment tools like cognitive tests or rating scales, Neurobagel encodes whether a subject has a value/score for at least one item or subscale of the assessment. Because assessment tools often have several subscales or items that can be stored as separate columns in the tabular <code>participant.tsv</code> file, each assessment tool column receives a minimum of two annotations:</p> <ul> <li>one to classify that the column <code>IsAbout</code> the generic category of assessment tools</li> <li>one to classify that the column <code>IsPartOf</code> the specific assessment tool</li> </ul> <p>An optional additional annotation <code>MissingValues</code> can be used to specify value(s) in an assessment tool column which represent that the participant is missing a value/response for that subscale, when instances of missing values are present (see also section Missing values).</p> <pre><code>{\n  \"updrs_1\": {\n    \"Description\": \"item 1 scores for UPDRS\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Assessment\",\n        \"Label\": \"Assessment tool\"\n      },\n      \"IsPartOf\": {\n        \"TermURL\": \"snomed:342061000000106\",\n        \"Label\": \"Unified Parkinsons disease rating scale score\"\n      },\n      \"VariableType\": \"Collection\"\n    }\n  },\n  \"updrs_2\": {\n    \"Description\": \"item 2 scores for UPDRS\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Assessment\",\n        \"Label\": \"Assessment tool\"\n      },\n      \"IsPartOf\": {\n        \"TermURL\": \"snomed:342061000000106\",\n        \"Label\": \"Unified Parkinsons disease rating scale score\"\n      },\n      \"MissingValues\": [\"\"],\n      \"VariableType\": \"Collection\"\n    }\n  }\n}\n</code></pre> <p>To determine whether a specific assessment tool is available for a given participant, we then consider all of the columns that were classified as <code>IsPartOf</code> that specific tool and then apply a simple <code>any()</code> heuristic to check that at least one column does not contain any <code>MissingValues</code>.</p> <p>For the above example, this would be:</p> particpant_id updrs_1 updrs_2 sub-01 2 sub-02 1 1 sub-03 <p>Therefore:</p> particpant_id updrs_available sub-01 True sub-02 True sub-03 False"},{"location":"data_models/dictionaries/#missing-values","title":"Missing values","text":"<p>Missing values are allowed for any phenotypic variable (column) that does not describe a participant or session identifier (e.g., columns like <code>participant_id</code> or <code>session_id</code>). In a Neurobagel data dictionary, missing values for a given column are listed under the <code>\"MissingValues\"</code> annotation for the column (see the Assessment tool section or the comprehensive example data dictionary for examples).</p>"},{"location":"data_models/graph_data/","title":"Neurobagel graph data files","text":""},{"location":"data_models/graph_data/#overview","title":"Overview","text":"<p>Using the Neurobagel CLI (see also the section on the CLI), a Neurobagel data dictionary (<code>.json</code>) for a dataset can be processed together with the corresponding tabular data (<code>.tsv</code>) and BIDS dataset (if available) to generate subject-level linked data that can be encoded in a knowledge graph. The Neurobagel graph-ready data are stored in JSON-LD format (<code>.jsonld</code>), and include a representation of each subject's harmonized phenotypic properties and imaging metadata.</p> <p>Another way to think about the difference between a Neurobagel data dictionary and a graph-ready <code>.jsonld</code> data file is this: more than one dataset can theoretically have the same data dictionary (if the tabular data include the same columns and unique column values), but the <code>.jsonld</code> file for each dataset is unique as long as the actual data of subjects differs across datasets.</p>"},{"location":"data_models/graph_data/#example-jsonld-files","title":"Example <code>.jsonld</code> files","text":"<p>Depending on whether a dataset annotated using Neurobagel includes BIDS imaging data, the <code>.jsonld</code> data for the dataset may or may not include imaging metadata of subjects (extracted automatically with the CLI).</p> <ul> <li>Example <code>.jsonld</code> containing only phenotypic data</li> <li>Example <code>.jsonld</code> containing phenotypic and raw imaging (BIDS) data</li> <li>Example <code>.jsonld</code> containing phenotypic and imaging derivative data</li> <li>Example <code>.jsonld</code> containing phenotypic, raw imaging (BIDS), and imaging derivative data</li> </ul> More info on example dataset <p>The above <code>.jsonld</code> files represent an example dataset used for testing which includes the following:</p> Data Link Phenotypic TSV Neurobagel data dictionary BIDS dataset"},{"location":"data_models/term_naming_standards/","title":"Neurobagel standards for controlled term naming","text":""},{"location":"data_models/term_naming_standards/#naming-conventions","title":"Naming conventions","text":""},{"location":"data_models/term_naming_standards/#namespace-prefixes","title":"Namespace prefixes","text":"<ul> <li>Names should be all lowercase (e.g., <code>nidm</code>, <code>snomed</code>)</li> </ul>"},{"location":"data_models/term_naming_standards/#properties-graph-edges","title":"Properties (graph \"edges\")","text":"<ul> <li>Names should adhere to camelCase (uses capitalized words except for the first word/letter)</li> <li>Should be a compound of:<ul> <li>a verb relevant to the property (e.g., hasAge, isSubjectGroup)</li> <li>the range of the property, (e.g.,hasDiagnosis points to a Diagnosis object)</li> </ul> </li> </ul> <p>What this might look like in semantic triples:</p> <pre><code>&lt;Subject&gt; &lt;nb:hasDiagnosis&gt; &lt;snomed:1234&gt;\n&lt;snomed:1234&gt; &lt;rdf:type&gt; &lt;nb:Diagnosis&gt;\n</code></pre>"},{"location":"data_models/term_naming_standards/#classes-or-resources-graph-nodes","title":"Classes or resources (graph \"nodes\")","text":"<ul> <li>Names should adhere to PascalCase (each word capitalized)</li> <li>Where possible, simplify to a single word (e.g., <code>Diagnosis</code>, <code>Dataset</code>, <code>Sex</code>)</li> </ul> <p>Note</p> <p>Generally, we own the terms for properties and classes (e.g., Diagnosis, Assessment) but not the resources representing instances of classes such as specific diagnosis, sex, or assessment values (these are reused from existing vocabularies).</p> <p>In cases where we reuse a term for a class that comes from an existing controlled vocabulary, and that vocabulary follows a different naming convention (e.g., all lowercase), we should follow the existing naming convention.</p>"},{"location":"data_models/term_naming_standards/#currently-used-namespaces","title":"Currently used namespaces","text":"Prefix IRI Types of terms <code>nb</code> http://neurobagel.org/vocab/ Neurobagel-\"owned\" properties and classes <code>snomed</code> http://purl.bioontology.org/ontology/SNOMEDCT/ diagnoses, sex, and assessments or instruments <code>ncit</code> http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl# group designation (e.g., healthy control) <code>nidm</code> http://purl.org/nidash/nidm# imaging modalities <code>np</code> https://github.com/nipoppy/pipeline-catalog/tree/main/processing/ processing pipeline and derivative metadata"},{"location":"data_models/term_naming_standards/#what-if-an-nb-term-already-exists-in-another-controlled-vocabulary","title":"What if an <code>nb</code> term already exists in another controlled vocabulary?","text":"<p>If there is an equivalent controlled term to one we are defining in a different namespace, we document this and express their equivalence using <code>owl:sameAs</code>.</p> <p>Example: If our term is <code>nb:Subject</code> and <code>nidm:Subject</code> is conceptually equivalent:</p> <pre><code>&lt;nb:12345&gt; a &lt;nb:Subject&gt;\n&lt;nb:Subject&gt; a &lt;rdfs:Resource&gt;;\n    &lt;owl:sameAs&gt; &lt;nidm:Subject&gt;\n</code></pre>"},{"location":"data_models/term_naming_standards/#other-general-guidelines","title":"Other general guidelines","text":"<ul> <li>Each property (edge) should use a single namespace for the resources it corresponds to</li> <li>Where possible, hardcode or refer to identifiers and not human-readable labels</li> </ul>"},{"location":"data_models/variables/","title":"Variables harmonized by Neurobagel","text":"<p>Neurobagel semantically harmonizes a number of variables used to describe different data and metadata for a study participant.</p> <p>The following list summarizes the variables that can currently be modeled and queried using Neurobagel, at the level of a single subject session.</p> <p>We are actively working to expand our subject data model, and welcome feedback on any other variables you would like to be able to query.</p>"},{"location":"data_models/variables/#phenotypic","title":"Phenotypic","text":"<p>For details on how phenotypic attributes are modeled, see the page on Neurobagel data dictionaries.</p> <ul> <li>Age</li> <li>Sex</li> <li>Diagnosis</li> <li>Availability of scores on a particular assessment</li> </ul>"},{"location":"data_models/variables/#imaging","title":"Imaging","text":""},{"location":"data_models/variables/#raw-data","title":"Raw data","text":"<ul> <li>Available MRI acquisition sequences, following the BIDS specification</li> </ul>"},{"location":"data_models/variables/#derived-data","title":"Derived data","text":"<ul> <li>Completed processing pipelines, following the Nipoppy specification</li> </ul>"},{"location":"user_guide/","title":"Ecosystem","text":"<p>The Neurobagel ecosystem comprises four primary tools:</p> <ul> <li>The annotation tool  (annotate.neurobagel.org)<ul> <li>to create harmonized annotations of phenotypic variables</li> <li>intended for use by researchers and domain experts</li> <li>static site, deployed on GitHub Pages</li> </ul> </li> <li>The command-line interface <ul> <li>to extract subject-specific metadata from annotated phenotypic and BIDS data</li> <li>intended for data managers to create graph-ready harmonized data</li> </ul> </li> <li>The knowledge graph store and Neurobagel API  (e.g., api.neurobagel.org)<ul> <li>to store and query extracted metadata using RDF and the SPARQL query language</li> <li>intended for research/data platform owners and for isolated deployments</li> </ul> </li> <li>The query tool  (query.neurobagel.org)<ul> <li>to search across datasets and obtain metadata for subjects based on harmonized subject-level attributes</li> <li>intended to help researchers and scientific data users find cohorts</li> <li>static site, deployed on GitHub Pages</li> </ul> </li> </ul> <p>You can also find official Docker images for our containerized tools on the Neurobagel Docker Hub profile.</p>"},{"location":"user_guide/#what-to-do-next","title":"What to do next","text":"<ul> <li>Learn how to run a cohort query on publicly accessible Neurobagel nodes</li> <li>Deploy your own Neurobagel node using our official Docker Compose recipe</li> <li>Prepare your own dataset for annotation and harmonization with Neurobagel</li> </ul>"},{"location":"user_guide/annotation_tool/","title":"The Neurobagel Annotation Tool","text":"<p>The Neurobagel annotation tool creates standardized, machine-readable data dictionaries for tabular data using curated FAIR vocabularies. The tool helps to harmonize tabular research data and is compatible with BIDS datasets.</p> <p>Workflow summary:</p> <ol> <li>Upload tabular data</li> <li>Column annotation</li> <li>Value annotation</li> <li>Download data dictionary</li> </ol>"},{"location":"user_guide/annotation_tool/#1-upload-tabular-data","title":"1. Upload tabular data","text":"<ul> <li>Upload your data table (.tsv file)<ul> <li>Can be <code>participants.tsv</code> from a BIDS dataset</li> </ul> </li> <li>Optional: Upload an existing data dictionary (.json file) for extra context<ul> <li>Can use <code>participants.json</code> from a BIDS dataset</li> <li>Or continue previous Neurobagel annotation work</li> </ul> </li> </ul> <p>In the following steps, you will annotate your table by first describing the columns and then the values within the columns.</p>"},{"location":"user_guide/annotation_tool/#2-column-annotation","title":"2. Column Annotation","text":"<p>Each column in your uploaded table is represented as a card on this page. For each column, you can:</p> <ul> <li>Add a description</li> <li>Select the standardized variable that best describes the column from the dropdown (if a suitable match exists)</li> <li>Select the data type<ul> <li>Choose \"Categorical\" if the column contains discrete values, \"Continuous\" if it contains numerical measurements, or leave it empty if neither applies</li> <li>Columns mapped to standardized variables will have their data type inferred automatically</li> </ul> </li> </ul> <p>When to manually select data type</p> <p>We recommend manually selecting the data type in two cases:</p> <ol> <li>When your column doesn't match any standardized variable</li> <li>When your column matches the \"Assessment tool\" standardized variable (which does not have a predefined data type since it can represent multi-column measures)</li> </ol>"},{"location":"user_guide/annotation_tool/#if-your-dataset-has-imaging-bids-data","title":"If your dataset has imaging (BIDS) data","text":"<p>The \"Participant ID\" standardized variable must be mapped to a column that contains the BIDS IDs for subjects, following the BIDS naming scheme <code>sub-&lt;label&gt;</code>.</p> <p>For more information, see this section on preparing the phenotypic data table for a BIDS dataset.</p>"},{"location":"user_guide/annotation_tool/#21-multi-column-measure-annotation","title":"2.1 Multi-column measure annotation","text":"<p>Info</p> <p>This step is only available if you have mapped columns in your data table to the \"Assessment tool\" standardized variable.</p> <p></p> <p>The card on the right lists all columns from your data table that you have mapped to the \"Assessment tool\" standardized variable.</p> <ol> <li>Create a card for each assessment or instrument represented in your data by clicking  and then selecting the name of the assessment from the dropdown list.<ul> <li>If no suitable match exists, the available standardized vocabulary likely cannot currently represent your assessment.</li> <li>To avoid incomplete annotations, un-map any column(s) corresponding to missing assessments from the \"Assessment tool\" standardized variable using the  button in the overview card.</li> </ul> </li> <li>Select the column(s) that describe each assessment, grouping together related columns as needed, using the dropdown on the respective assessment card.<ul> <li>You can check remaining, ungrouped columns in the overview on the right.</li> </ul> </li> </ol>"},{"location":"user_guide/annotation_tool/#3-value-annotation","title":"3. Value Annotation","text":"<p>The left sidebar displays the standardized variables that are represented in your tabular data, along with the column names that have been mapped to those variables.</p> <p>Click on a standardized variable (or data type, for unannotated columns) subheading in the sidebar to display the columns corresponding to that variable (or data type). Then, in the column-level view on the right, navigate between the column tabs to annotate the values within each column.</p> Understanding sidebar sections <p>The sidebar organizes your columns by their annotation status:</p> <ul> <li>Annotated contains columns you have mapped to standardized variables</li> <li>Unannotated contains columns you have not mapped to a standardized variable<ul> <li>Within this section, unannotated columns are organized based on whether you have assigned them a data type</li> </ul> </li> </ul>"},{"location":"user_guide/annotation_tool/#columns-with-continuous-data","title":"Columns with continuous data","text":"<p>For a column containing continuous data, you can:</p> <ul> <li>Add a description of the units of measurement</li> <li>Select the format of the numerical values<sup>1</sup></li> <li>Select \"Mark as missing\" for any values that represent missing, unavailable, or invalid data<sup>1</sup><ul> <li>Note: the column-level view will only display unique values in the column</li> </ul> </li> </ul> Units vs. Format <p>Format refers to how the numeric values in your data are expressed (e.g., <code>float</code> for decimal numbers like 25.5, <code>range</code> for numeric ranges like 30-35) whereas Units describe what the numbers represent (e.g., \"years\" for age, \"points\" for test scores, \"mg/dL\" for measurements).</p>"},{"location":"user_guide/annotation_tool/#columns-with-categorical-data","title":"Columns with categorical data","text":"<p>For a column containing categorical data, you will be prompted to annotate the unique values detected in the column. This includes any values that are blank (empty strings) or contain only whitespace.</p> <p>For each unique column value, you can:</p> <ul> <li>Add a free-form description of the value</li> <li>Select a standardized term that best captures the meaning of the value<sup>1</sup></li> <li>Select \"Mark as missing\" if the value:<sup>1</sup><ul> <li>indicates missing, unavailable, or invalid data</li> <li>OR, does not have a suitable match among the standardized term options</li> </ul> </li> </ul> <p>Warning</p> <p>For the value annotation to be considered complete by Neurobagel, all unique values must either be mapped to a standardized term or marked as missing.</p>"},{"location":"user_guide/annotation_tool/#4-download-data-dictionary","title":"4. Download data dictionary","text":"<ul> <li>Preview your annotated data dictionary</li> <li>Download the data dictionary <code>.json</code> file</li> <li>Annotate a new dataset if desired</li> </ul> <p>Tip</p> <p>If you see a warning about \"Incomplete Annotations\", you will need to return to the Value Annotation page to complete any missing annotations before your data dictionary is valid for downstream Neurobagel tools.</p> <p>Your downloaded data dictionary is BIDS-compatible and, if you see the confirmation that you have successfully created a Neurobagel data dictionary, it is ready to be used to generate data for a Neurobagel graph database.</p> <ol> <li> <p>Attribute can only be annotated if the column has been mapped to a standardized variable.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"user_guide/api/","title":"The Neurobagel API","text":""},{"location":"user_guide/api/#introduction","title":"Introduction","text":"<p>Neurobagel has two flavours of APIs that can be deployed: node API and federation API.</p> <ul> <li>A Neurobagel node API (n-API) formulates SPARQL queries based on a set of user-defined parameters to a single connected graph database, and processes returned query results into a user-friendly format.</li> <li>A Neurobagel federation API (f-API) lets the user sends a single query to each node API it is aware of, and collects and combines the decentralized responses into a single set of query results.</li> </ul> <p>Neurobagel's query tool provides a GUI for querying one or more Neurobagel graphs by sending requests to a Neurobagel federation API instance. However, HTTP requests can also be sent directly to any publicly accessible Neurobagel API (node or federation).</p>"},{"location":"user_guide/api/#public-neurobagel-apis","title":"Public Neurobagel APIs","text":"<p>In addition to supporting independent local/institutional deployments (i.e., instances) of the Neurobagel API, which can interface with a local or restricted graph, Neurobagel also hosts its own public instances of a node API and a federation API.</p> <p>https://api.neurobagel.org/ is a public, Neurobagel-hosted node API that interfaces with Neurobagel's own running graph instance containing harmonized datasets from the OpenNeuro platform.</p>"},{"location":"user_guide/api/#sending-a-request-to-a-neurobagel-api-directly","title":"Sending a request to a Neurobagel API directly","text":"<p>Cohort queries of a specific Neurobagel graph database can be submitted via direct requests to the corresponding node API using the <code>/query</code> endpoint, e.g. <code>https://api.neurobagel.org/query</code>. Specific query parameters are defined using key-value pairs in the URL following <code>/query</code>.</p> <p>Example: \"I want to query for only female participants in the OpenNeuro graph.\"</p> <p>The URL for such a query would be <code>https://api.neurobagel.org/query?sex=snomed:248152002</code>, where <code>snomed:248152002</code> is a controlled term from the SNOMED CT vocabulary corresponding to female sex.</p>"},{"location":"user_guide/api/#example-using-a-curl-request","title":"Example using a curl request","text":"<pre><code># To query for female participants in the graph\n\ncurl -X 'GET' \\\n  'https://api.neurobagel.org/query?sex=snomed:248152002' \\\n  -H 'accept: application/json'\n\n# or\ncurl -L https://api.neurobagel.org/query?sex=snomed:248152002\n</code></pre> <p>Avoid trailing slashes in API endpoint URLs</p> <p>Neurobagel APIs have strict requirements regarding trailing slashes. When sending <code>curl</code> requests to an instance of a Neurobagel API, ensure that you do not include trailing slashes in endpoint URLs. For example, requests to <code>https://api.neurobagel.org/query</code> will work, but <code>https://api.neurobagel.org/query/</code> will not.</p>"},{"location":"user_guide/api/#using-the-interactive-neurobagel-api-docs","title":"Using the interactive Neurobagel API docs","text":"<p>Interactive documentation for a Neurobagel API (provided by Swagger UI) is available at the <code>/docs</code> endpoint (e.g., https://api.neurobagel.org/docs) and can also be used to run queries against the graph.</p> <p>Note</p> <p>For convenience, navigating to https://api.neurobagel.org/ in the browser will automatically redirect you to the docs.</p> <p>To send a request to the API from the docs interface, expand the <code>query</code> endpoint tab with the  icon to view the parameters that can be set, and click \"Try it out\" and then \"Execute\" to execute a query.</p> <p>Note</p> <p>Due to limitations of Swagger UI in displaying very large HTTP response bodies, queries with very few parameters sent using the interactive docs UI may be very slow or time out. If this is the case, try using a <code>curl</code> request or the query tool instead.</p>"},{"location":"user_guide/cli/","title":"The Neurobagel CLI","text":"<p>The Neurobagel CLI is a command-line tool that processes a Neurobagel-annotated dataset and produces harmonized subject-level phenotypic and imaging attributes. The resulting harmonized data can be directly integrated into a Neurobagel graph store.</p>"},{"location":"user_guide/cli/#installation","title":"Installation","text":"PythonDockerApptainer <p>The Neurobagel CLI can be installed from PyPI using <code>pip</code>.</p> <ol> <li> <p>Before installing the Python package, we recommend first creating and activating a Python virtual environment (using a tool such as venv).</p> </li> <li> <p>Install the <code>bagel</code> package into your virtual environment: <pre><code>pip install bagel\n</code></pre></p> </li> </ol> <p>Pull the Docker image for the Neurobagel CLI from Docker Hub: <pre><code>docker pull neurobagel/bagelcli\n</code></pre></p> <p>Build a Apptainer image for the Neurobagel CLI using the Docker Hub image: <pre><code>apptainer pull bagel.sif docker://neurobagel/bagelcli\n</code></pre></p>"},{"location":"user_guide/cli/#input-files","title":"Input files","text":"<p>The Neurobagel CLI creates a single harmonized view of each subject's data in a dataset, and can integrate information from several data sources (phenotypic, raw neuroimaging, processed neuroimaging).</p> <p>To run the CLI on a dataset, you will need the following files:</p> <ul> <li> A phenotypic TSV</li> <li> A Neurobagel JSON data dictionary for the TSV</li> <li> A dataset description JSON for the dataset</li> <li> (Optional) A valid BIDS metadata table, if subjects have neuroimaging data available (1)</li> <li> (Optional) A TSV of subject statuses for any image processing pipelines that have been run, following the Nipoppy processing status file schema (2)</li> </ul> <ol> <li>This table can be generated automatically using the CLI's <code>bids2tsv</code> command, and will be used to generate harmonized subject imaging data availability.</li> <li>This file is adapted from the Nipoppy workflow and can be automatically generated using Nipoppy pipeline trackers. It will be used to generate harmonized subject processed imaging data availability.</li> </ol>"},{"location":"user_guide/cli/#running-the-cli","title":"Running the CLI","text":"<p>To view the general CLI help and information about the available commands:</p> PythonDockerApptainer <pre><code>bagel -h\n</code></pre> <pre><code># This is a shorthand for: docker run --rm neurobagel/bagelcli --help\ndocker run --rm neurobagel/bagelcli\n</code></pre> <pre><code># This is a shorthand for: apptainer run bagel.sif --help\napptainer run bagel.sif\n</code></pre>"},{"location":"user_guide/cli/#generate-a-bids-metadata-table","title":"Generate a BIDS metadata table","text":"<p>Info</p> <ul> <li>If your dataset does not have imaging data, skip this step.</li> <li>If your dataset's imaging data are not in BIDS format, you must manually create a BIDS metadata table.</li> </ul> <p>To include BIDS imaging data as part of the harmonized subject data, you must first convert the BIDS metadata into a table.</p> <p>You can do this automatically using the CLI's <code>bids2tsv</code> command.<sup>1</sup></p> <p>Example:</p> <p>If your BIDS directory is located at <code>/data/public/Dataset1_bids</code> and you want the table output to be saved to <code>/home/Neurobagel</code>:</p> PythonDockerApptainer <pre><code>bagel bids2tsv \\\n    --bids-dir \"/data/public/Dataset1_bids\"\n    --output \"/home/Neurobagel/Dataset1_bids.tsv\"\n</code></pre> <pre><code>docker run --rm \\\n    -v \"/data/public:/data/public\" \\\n    -v \"/home/Neurobagel:/home/Neurobagel\" \\\n    neurobagel/bagelcli bids2tsv \\\n    --bids-dir \"/data/public/Dataset1_bids\" \\\n    --output \"/home/Neurobagel/Dataset1_bids.tsv\"\n</code></pre> Mounting input paths using <code>-v</code>/<code>--volume</code> <p>When running the CLI in a container, you must mount any input or output directories to directory paths within the container so that the app can access them. In your CLI options, always refer to the container paths. In the example above, container paths are set to match the host paths for simplicity.</p> <pre><code>apptainer run --no-home \\\n    -B \"/data/public,/home/Neurobagel\" \\\n    bagel.sif bids2tsv \\\n    --bids-dir \"/data/public/Dataset1_bids\" \\\n    --output \"/home/Neurobagel/Dataset1_bids.tsv\"\n</code></pre> Mounting input paths using <code>-B</code>/<code>--bind</code> <p>When running the CLI in a container, you must mount any input or output directories to directory paths within the container so that the app can access them. In your CLI options, always refer to the container paths. In the example above, the container paths are set to match the host paths for simplicity.</p> This command may be slow on large datasets <p>On datasets with more than a few hundred subjects, <code>bids2tsv</code> can take upwards of several minutes due to the time needed for <code>PyBIDS</code> to read the dataset structure.</p> <p>This will produce a BIDS metadata table named <code>Dataset1_bids.tsv</code>, which can then be provided as input to the <code>bids</code> command below.</p>"},{"location":"user_guide/cli/#generate-graph-ready-data-jsonld-files","title":"Generate graph-ready data (JSONLD files)","text":"<p>The Neurobagel CLI provides different commands for generating different types of harmonized subject (meta)data:</p> <ul> <li> <p><code>pheno</code></p> <p>Must be run first</p> <p>Each subject in a Neurobagel graph requires at least phenotypic data. The other metadata are optional and can be added afterward via the <code>bids</code> and/or <code>derivatives</code> commands in any order.</p> </li> <li> <p><code>bids</code></p> </li> <li><code>derivatives</code></li> </ul> <p>If you are using Docker or Apptainer, we strongly recommend placing all the input files for your dataset into a single directory. This avoids the need to mount multiple paths into the container when running CLI commands.</p>"},{"location":"user_guide/cli/#viewing-help-for-a-command","title":"Viewing help for a command","text":"<p>To view the command-line options for a specific command, such as <code>pheno</code>:</p> PythonDockerApptainer <pre><code>bagel pheno -h\n</code></pre> <pre><code>docker run --rm neurobagel/bagelcli pheno -h\n</code></pre> <pre><code>apptainer run bagel.sif pheno -h\n</code></pre> <p>Example:</p> <p>The following example assumes that the input files for your dataset are located in <code>/home/Dataset1/Neurobagel</code>:</p> <pre><code>home/\n\u2514\u2500\u2500 Dataset1/\n    \u251c\u2500\u2500 Neurobagel/\n    \u2502   \u251c\u2500\u2500 Dataset1_pheno.tsv # (1)!\n    \u2502   \u251c\u2500\u2500 Dataset1_pheno.json # (2)!\n    \u2502   \u251c\u2500\u2500 Dataset1_bids.tsv # (3)!\n    \u2502   \u251c\u2500\u2500 Dataset1_proc_status.tsv # (4)!\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 ...\n</code></pre> <ol> <li>The phenotypic TSV</li> <li>The phenotypic data dictionary</li> <li>The BIDS metadata table</li> <li>The processing status file</li> </ol> <p>Navigate to the directory containing your input files, e.g.:</p> <pre><code>cd /home/Dataset1/Neurobagel\n</code></pre> <p>Info</p> <p>In the example commands below, replace the Dataset1 files with the actual input files for your dataset.</p>"},{"location":"user_guide/cli/#1-process-phenotypic-data-using-the-pheno-command-required","title":"1. Process phenotypic data using the <code>pheno</code> command (required)","text":"<p>Run the command below to generate harmonized subject-level phenotypic data for your dataset as a JSONLD file:</p> PythonDockerApptainer <pre><code>bagel pheno \\\n    --pheno \"Dataset1_pheno.tsv\" \\\n    --dictionary \"Dataset1_pheno.json\" \\\n    --dataset-description \"Dataset1_description.json\" \\\n    --output \"Dataset1.jsonld\"\n</code></pre> <pre><code>docker run --rm -v $PWD:$PWD neurobagel/bagelcli pheno \\\n    --pheno \"$PWD/Dataset1_pheno.tsv\" \\\n    --dictionary \"$PWD/Dataset1_pheno.json\" \\\n    --dataset-description \"Dataset1_description.json\" \\\n    --output \"$PWD/Dataset1.jsonld\"\n</code></pre> <pre><code>apptainer run --no-home -B $PWD bagel.sif pheno \\\n    --pheno \"$PWD/Dataset1_pheno.tsv\" \\\n    --dictionary \"$PWD/Dataset1_pheno.json\" \\\n    --dataset-description \"Dataset1_description.json\" \\\n    --output \"$PWD/Dataset1.jsonld\"\n</code></pre>"},{"location":"user_guide/cli/#2-process-raw-imaging-metadata-using-the-bids-command-optional","title":"2. Process raw imaging metadata using the <code>bids</code> command (optional)","text":"<p>If you have a BIDS metadata table, run this command to include subjects' imaging data availability to your dataset JSONLD file:</p> PythonDockerApptainer <pre><code>bagel bids \\\n    --jsonld-path \"Dataset1.jsonld\" \\\n    --bids-table \"Dataset1_bids.tsv\" \\\n    --output \"Dataset1.jsonld\" \\\n    --overwrite\n</code></pre> <pre><code>docker run --rm -v $PWD:$PWD neurobagel/bagelcli bids \\\n    --jsonld-path \"$PWD/Dataset1.jsonld\" \\\n    --bids-table \"$PWD/Dataset1_bids.tsv\" \\\n    --output \"$PWD/Dataset1.jsonld\" \\\n    --overwrite\n</code></pre> <pre><code>apptainer run --no-home -B $PWD bagel.sif bids \\\n    --jsonld-path \"$PWD/Dataset1.jsonld\" \\\n    --bids-table \"$PWD/Dataset1_bids.tsv\" \\\n    --output \"$PWD/Dataset1.jsonld\" \\\n    --overwrite\n</code></pre>"},{"location":"user_guide/cli/#3-process-derived-imaging-metadata-using-the-derivatives-command-optional","title":"3. Process derived imaging metadata using the <code>derivatives</code> command (optional)","text":"<p>If you have a processing status file from Nipoppy, run this command to add subjects' processing pipeline data availability to the dataset JSONLD:</p> PythonDockerApptainer <pre><code>bagel derivatives \\\n    --jsonld-path \"Dataset1.jsonld\" \\\n    --tabular \"Dataset1_proc_status.tsv\" \\\n    --output \"Dataset1.jsonld\" \\\n    --overwrite\n</code></pre> <pre><code>docker run --rm --v $PWD:$PWD neurobagel/bagelcli derivatives \\\n    --jsonld-path \"$PWD/Dataset1.jsonld\" \\\n    --tabular \"$PWD/Dataset1_proc_status.tsv\" \\\n    --output \"$PWD/Dataset1.jsonld\" \\\n    --overwrite\n</code></pre> <pre><code>apptainer run --no-home -B $PWD bagel.sif derivatives \\\n    --jsonld-path \"$PWD/Dataset1.jsonld\" \\\n    --tabular \"$PWD/Dataset1_proc_status.tsv\" \\\n    --output \"$PWD/Dataset1.jsonld\" \\\n    --overwrite\n</code></pre> <p>Tip</p> <p>To see all options for a CLI command, including short forms and optional parameters, refer to the command's help.</p> When to use <code>-f</code>/<code>--overwrite</code> <p>If you're only interested in the final JSONLD with all metadata added (i.e., after all relevant commands have been run), you can safely overwrite intermediate output files by specifying the same output file path each time.</p> <p>These steps have generated a graph-ready JSONLD file for Dataset1 (<code>Dataset1.jsonld</code>) that incorporates all the available subject data sources. The resulting JSONLD is ready to upload to a Neurobagel graph database.</p>"},{"location":"user_guide/cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide/cli/#file-or-directory-does-not-exist-error-when-using-dockerapptainer","title":"<code>File or directory does not exist</code> error when using Docker/Apptainer","text":"<p>This error usually means the container cannot access your input files because the directories were not mounted correctly.</p> <p>The examples assume you are running the CLI from inside the directory containing your inputs. Thus, they mount the current working directory <code>$PWD</code> to the same path inside the container for convenience using the syntax:</p> DockerApptainer <pre><code>docker run --rm -v $PWD:$PWD neurobagel/bagelcli ...\n</code></pre> <p>However, if your inputs are located in a different directory or spread across multiple directories, you must mount each directory explicitly using the Docker option <code>-v /path/on/host:/path/in/container</code>.</p> <p>When passing file paths to the CLI, always use the absolute path inside the container to avoid confusion.</p> <pre><code>apptainer run --no-home -B $PWD bagel.sif ...\n</code></pre> <p>However, if your inputs are located in a different directory or spread across multiple directories, you must mount each directory explicitly using the Apptainer option <code>-B /path/on/host:/path/in/container</code>.</p> <p>When passing file paths to the CLI, always use the absolute path inside the container to avoid confusion.</p>"},{"location":"user_guide/cli/#upgrading-data-to-a-newer-version-of-the-cli","title":"Upgrading data to a newer version of the CLI","text":"<p>Neurobagel is under active development and future CLI releases may introduce breaking changes to the data model used in subject-level <code>.jsonld</code> graph files. Breaking changes are highlighted in the release notes.</p> <p>To upgrade to the latest version of the data model:</p> <ol> <li> <p>Upgrade to the latest CLI version:</p> PythonDockerApptainer <pre><code>pip install --upgrade bagel\n</code></pre> <pre><code>docker pull neurobagel/bagelcli\n</code></pre> <pre><code>apptainer pull bagel.sif docker://neurobagel/bagelcli\n</code></pre> </li> <li> <p>If you have an existing Neurobagel graph database, we recommend regenerating and reuploading all existing <code>.jsonld</code> files in your database using the latest CLI version. This keeps the database internally consistent and avoids conflicts with dataset <code>.jsonld</code> files generated using older CLI versions.</p> </li> </ol> <ol> <li> <p><code>bids2tsv</code> internally uses bids2table.\u00a0\u21a9</p> </li> </ol>"},{"location":"user_guide/data_prep/","title":"Preparing the phenotypic data","text":"Looking for more general guidelines on how to organize your dataset? <p>We recommend also checking out Nipoppy, a protocol for standardized organization and processing of clinical-neuroimaging datasets that extends BIDS. Neurobagel tools are designed to be compatible with data organized according to the Nipoppy specification, although you do not need to use Nipoppy in order to use Neurobagel.</p> <p>To use the Neurobagel annotation tool, you must prepare the tabular data for your dataset as a single tab-separated values (<code>.tsv</code>) file.</p> <p>Within Neurobagel, tabular (or phenotypic) data  refers to any demographic, clinical/behavioural, cognitive, or other non-imaging-derived data of participants which are typically stored in a tabular format.</p>"},{"location":"user_guide/data_prep/#requirements-for-the-phenotypic-tsv","title":"Requirements for the phenotypic TSV","text":"<p>If you're unfamiliar with TSV files or unsure how to format them correctly</p> <p>Please first consult our TSV glossary section for information on creating valid TSV files.</p> <p>In a valid phenotypic TSV file for Neurobagel, rows identify each participant (or participant-session in a longitudinal dataset), and columns describe properties of participants (age, sex, diagnosis, etc.).</p> <p>Each row MUST describe only one participant/session, and each participant/session MUST be described by only one row.</p> <p>The TSV MUST contain:</p> <ul> <li>A minimum of 2 columns</li> <li> <p>At least 1 column containing subject identifiers (IDs). Subject IDs must be unique per row.</p> <ul> <li>If both subject and session ID columns are present, then the combinations of IDs must be unique per row.</li> </ul> <p>Only one subject ID column can be annotated</p> <p>Neurobagel currently does not support linking multiple IDs to a single subject. If your TSV file includes multiple subject ID columns (e.g., study ID, hospital ID), you will choose one column to serve as the primary subject ID during annotation.</p> </li> <li> <p>At least 1 column that describes demographic or other phenotypic information</p> </li> </ul> <p>The TSV MAY contain:</p> <ul> <li> <p>At least 1 column containing session identifiers (e.g., if the dataset is longitudinal)</p> <p>Only one session ID column can be annotated</p> <p>Neurobagel currently does not support linking multiple IDs to a single session. If your TSV file includes multiple session ID columns (e.g., visit number, scan ID), you will choose one column to serve as the primary session ID during annotation.</p> </li> </ul> <p>The TSV MUST NOT contain:</p> <ul> <li>Missing values in the columns you intend to annotate as the primary subject ID or session ID (if available)</li> </ul> <p>For all phenotypic variables currently modeled by Neurobagel, see here.</p>"},{"location":"user_guide/data_prep/#if-your-dataset-has-imaging-bids-data","title":"If your dataset has imaging (BIDS) data","text":"<p>In addition to phenotypic characteristics of subjects, Neurobagel can also harmonize information about subjects' imaging data from a corresponding BIDS dataset (see also Preparing imaging data).</p> <p>To include subjects' BIDS imaging data as part of their representation in Neurobagel, your phenotypic TSV MUST meet the following requirements in addition to the ones listed above:</p> <ul> <li> <p>At least 1 column in the phenotypic TSV contains subject IDs that   exactly match the subject IDs in your BIDS metadata table   (by default, these should be BIDS subject IDs in the form <code>sub-&lt;label&gt;</code>),   AND this must be the column you annotate as the primary subject ID</p> <p>Subject IDs are case-sensitive</p> <p>Subject IDs in the phenotypic TSV must match corresponding BIDS subject IDs exactly for Neurobagel to correctly link phenotypic and BIDS information. For example, a BIDS ID of <code>sub-MNI001</code> would not be matched to subject IDs <code>sub-mni001</code> or <code>mni001</code> in a phenotypic TSV.</p> </li> <li> <p>All BIDS subject IDs must appear in the phenotypic TSV,   even if the subject only has imaging data (columns can be left empty for missing phenotypic values)</p> <ul> <li>Datasets that include subjects in the BIDS dataset but not in the phenotypic TSV are not supported. However, subjects with phenotypic data only (no BIDS data) are allowed.</li> </ul> </li> </ul> <p>If your dataset is longitudinal, the session IDs in the phenotypic TSV MAY match the BIDS session IDs, but this is not required.</p>"},{"location":"user_guide/data_prep/#examples-of-valid-phenotypic-tsvs","title":"Examples of valid phenotypic TSVs","text":"<p>Depending on your dataset, your tabular data may look like one of the following:</p>"},{"location":"user_guide/data_prep/#a-bids-participantstsv-file","title":"A BIDS <code>participants.tsv</code> file","text":"<p>If you have a BIDS compliant <code>participants.tsv</code> that contains all the demographic and clinical/behavioural information for participants, you can annotate this file with Neurobagel's annotation tool to create a data dictionary for the file.</p> <p>Example TSV:</p> participant_id age sex tools sub-01 22 female WASI-2 sub-02 28 male Stroop ... ... ... ..."},{"location":"user_guide/data_prep/#a-longitudinal-data-file","title":"A longitudinal data file","text":"<p>If you have longitudinal tabular data (e.g. age collected at multiple sessions/visits), then the information for all sessions should be combined into a single TSV. Each row must describe a unique combination of subject and session.</p> <p>Example TSV:</p> participant_id session_id age tools sub-01 ses-01 22 WASI-2 sub-01 ses-02 23 sub-02 ses-01 28 Stroop ... ... ... ... <p>Tip</p> <p>A <code>participants.tsv</code> file with multiple sessions is not BIDS compliant. If you want to store multi-session phenotypic data in a BIDS dataset, you could do so in the <code>phenotype/</code> subdirectory (see also the BIDS specification section on Longitudinal and multi-site studies).</p>"},{"location":"user_guide/data_prep/#multiple-participant-or-session-id-columns","title":"Multiple participant or session ID columns","text":"<p>In some cases, there may be a need for more than one set of IDs for participants and/or sessions.</p> <p>For example, if a participant was first enrolled in a behavioural study with one type of ID, and then later joined an imaging study under a different ID, both types of participant IDs may be recorded in the tabular file.</p> <p>For Neurobagel, the only requirement is that the combination of all ID values for a row is unique.</p> <p>Only one subject ID and session ID column can be annotated for Neurobagel</p> <p>If your TSV file contains multiple subject or session ID columns, you will select one to use as the primary ID during annotation. The remaining subject/session ID columns will be ignored by Neurobagel.</p> <p>Example invalid TSV:</p> participant_id alternative_participant_id ... sub-01 SID-1234 ... sub-01 SID-2222 ... sub-02 SID-1234 ... <p>The same rules apply when multiple session IDs are present.</p> <p>Example valid TSV:</p> participant_id alt_participant_id session_id alt_session_id age ... sub-01 SID-1234 ses-01 visit-1 22 ... sub-01 SID-1234 ses-02 visit-2 23 ... sub-02 SID-2222 ses-01 visit-1 28 ... ... ... ... ... ... ..."},{"location":"user_guide/dataset_description/","title":"Preparing the dataset description","text":"<p>To generate harmonized data using Neurobagel, you must prepare a dataset description file. This is a JSON file that includes specific fields describing the dataset and how to access the data. This file is adapted from the <code>dataset_description.json</code> from BIDS.</p> <p>Information from the dataset description will be displayed to a user when they discover your dataset in the Neurobagel query tool.</p>"},{"location":"user_guide/dataset_description/#example","title":"Example","text":"<pre><code>{\n    \"Name\": \"BIDS synthetic\",\n    \"Authors\": [\n        \"Markiewicz, C. J.\",\n        \"Gau, R\u00e9mi\"\n    ],\n    \"ReferencesAndLinks\": [\n        \"https://bids-website.readthedocs.io/en/latest/datasets/examples.html#mri\"\n    ],\n    \"Keywords\": [\"synthetic\", \"nback\"],\n    \"RepositoryURL\": \"https://github.com/bids-standard/bids-examples.git\",\n    \"AccessInstructions\": \"Clone the git repository. The dataset is located in the 'synthetic' subdirectory.\",\n    \"AccessType\": \"public\",\n    \"AccessEmail\": \"bids.curator@gmail.com\",\n    \"AccessLink\": \"https://github.com/bids-standard/bids-examples.git\"\n}\n</code></pre> <p>How this dataset info will appear in the query tool:</p> <p></p>"},{"location":"user_guide/dataset_description/#editable-template","title":"Editable template","text":"<p>Below is an editable and copy-pasteable template for your dataset description JSON. For more information on each field, see Dataset description fields.</p> <p>Tips</p> <ul> <li>Click the underlined fields to edit the placeholder values.</li> <li>Fields wrapped in square brackets <code>[]</code> expect a list of comma-separated strings, e.g. <code>[\"Item 1\", \"Item 2\"]</code>.</li> <li>To leave any fields empty, delete the placeholder value.</li> </ul> <pre><code>{\n    \"Name\": \"DATASET NAME\",\n    \"Authors\": [\"AUTHORS\"],\n    \"ReferencesAndLinks\": [\"LINKS\"],\n    \"Keywords\": [\"KEYWORDS\"],\n    \"RepositoryURL\": \"URL\",\n    \"AccessInstructions\": \"INSTRUCTIONS\",\n    \"AccessType\": \"TYPE\",\n    \"AccessEmail\": \"EMAIL\",\n    \"AccessLink\": \"URL\"\n}\n</code></pre> <p>After editing, copy and save the contents in a file with the extension <code>.json</code>. We recommend using an informative file name like <code>DATASETNAME_description.json</code>.</p> <p>I have a dataset_description.json from my BIDS dataset</p> <p>Your existing valid BIDS <code>dataset_description.json</code> can be directly used for Neurobagel without modification, but we strongly recommend adding the optional Neurobagel-supported keys to improve dataset FAIRness and dataset discoverability in Neurobagel:</p> <ul> <li><code>\"RepositoryURL\"</code></li> <li><code>\"AccessInstructions\"</code></li> <li><code>\"AccessType\"</code></li> <li><code>\"AccessEmail\"</code></li> <li><code>\"AccessLink\"</code></li> </ul> <p>(These additional fields will not make the <code>dataset_description.json</code> invalid and will be safely ignored by the BIDS validator.)</p> <p>NOTE: The editable template does not include all required keys for a valid BIDS <code>dataset_description.json</code>. Do not overwrite your existing BIDS <code>dataset_description.json</code> with the template, only add Neurobagel-supported keys as needed.</p>"},{"location":"user_guide/dataset_description/#dataset-description-fields","title":"Dataset description fields","text":"<p>Neurobagel recognizes the following fields in the dataset description JSON file. Extra keys are ignored. * indicates a key adopted from the BIDS <code>dataset_description.json</code>.</p> Key name Requirement level Data type Description Name* REQUIRED string Name of the dataset. This name will be displayed when users discover the dataset in a Neurobagel query. Authors* RECOMMENDED list of strings List of individuals who contributed to the creation/curation of the dataset. ReferencesAndLinks* RECOMMENDED list of strings List of links or references that contain information on the dataset. NOTE: The first valid URL in this list will also be used as the dataset homepage when the dataset is discovered in the Neurobagel query tool. Keywords* RECOMMENDED list of strings List of keywords describing the content or subject matter of the dataset. RepositoryURL RECOMMENDED string URL to a repository where the dataset can be downloaded or retrieved from (e.g., DataLad, Zenodo, GitHub). AccessInstructions RECOMMENDED string Description of how to access the data. AccessType RECOMMENDED string, one of: <code>\"public\"</code>, <code>\"registered\"</code>, <code>\"restricted\"</code> Level of requirements for accessing the data. <code>public</code>: Immediately accessible without registration, authentication, or approval. <code>registered</code>: Requires authentication or agreement to basic terms of use, but no formal application or review. <code>restricted</code>: Requires formal approval or review of a data access request. AccessEmail RECOMMENDED string (email) Primary email for access requests. AccessLink RECOMMENDED string Primary link for access requests or information."},{"location":"user_guide/getting_started/","title":"Getting started","text":"<p>The following sections will get you started with deploying your own Neurobagel node, a graphical query tool, and a local federation API (everything in blue in the picture below) that lets you search across the data in your node and in public Neurobagel nodes.</p> <p></p> <p>To prepare your Neurobagel node for production use (i.e., for local or other users), and to configure your deployment according to your specific needs, refer to the detailed production deployment documentation.</p>"},{"location":"user_guide/getting_started/#requirements","title":"Requirements","text":"<p>Neurobagel tools are provided as Docker containers and are launched with Docker Compose.</p> <p>Don't install Neurobagel tools directly on your machine</p> <p>Please only use the Docker images provided by Neurobagel (or the third party providers Neurobagel relies on) and only launch them with our provided <code>docker-compose.yml</code> recipe.</p> <p>Do not install GraphDB locally on your computer, as doing so can interfere with the deployment of the Neurobagel tools.</p>"},{"location":"user_guide/getting_started/#docker-and-docker-compose","title":"<code>docker</code> and <code>docker compose</code>","text":"<p>If you haven't yet, please install both <code>docker</code> and <code>docker compose</code> for your operating system:</p> LinuxWindowsMacOS <ol> <li> <p>Install the Docker engine and follow the post-setup instructions</p> </li> <li> <p>Install Docker Compose using the repository option</p> </li> </ol> <ol> <li> <p>Install Docker Desktop on Windows. This will install both <code>docker</code> and <code>docker compose</code>.</p> </li> <li> <p>We strongly recommend also installing Windows Subsystem for Linux to get a Windows-supported Linux installation for a more seamless Neurobagel deployment experience. Simply follow these instructions to make your existing Docker Desktop installation (including Docker and Docker Compose) available when running WSL.</p> </li> </ol> <p>Install Docker Desktop on Mac. This will install both <code>docker</code> and <code>docker compose</code> automatically.</p> Linux is the only supported OS <p>We test and deploy on Linux and ensure that our deployment instructions work on Linux systems.</p> <p>We also try to provide docs and help for different architectures, but as a small team with limited resources we won't be able to help you debug Operating System specific problems.</p> <p>Because we rely on some modern features of these tools, please make sure you have at least the following versions on your machine:</p> <ul> <li> <p><code>docker</code> engine: v20.10.24 or greater</p> <pre><code>docker --version\n</code></pre> </li> <li> <p><code>docker compose</code>: v2.7.0 or above</p> <pre><code>docker compose version\n</code></pre> </li> </ul>"},{"location":"user_guide/getting_started/#quickstart-recipe","title":"Quickstart recipe","text":"<p>The <code>neurobagel/recipes</code> repository on GitHub contains our official Docker Compose recipe and template configuration files for setting up a local Neurobagel node.</p> <p>Production deployments require additional configuration</p> <p>This section provide a minimal configuration for launching Neurobagel so you can get started quickly with trying out all services locally. In most cases, particularly when deploying Neurobagel for other users, additional configurations are necessary.</p> <p>For a complete, production-ready setup containing real-world data, see our detailed instructions for production deployments.</p>"},{"location":"user_guide/getting_started/#clone-the-recipe-repository","title":"Clone the recipe repository","text":"<p>Clone the <code>recipes</code> repository from GitHub and navigate into the cloned local repo.</p> <pre><code>git clone https://github.com/neurobagel/recipes.git\ncd recipes\n</code></pre>"},{"location":"user_guide/getting_started/#copy-the-template-files","title":"Copy the template files","text":"<p>Make copies of the template configuration files to use for your deployment.</p> <pre><code>cp template.env .env\ncp local_nb_nodes.template.json local_nb_nodes.json\n</code></pre> <p>You can leave these files unchanged for a local test deployment. Our guide on setting up a production node has instructions on editing these files for a production server deployment.</p> On a machine with an ARM-based processor? <p>The default Docker Compose recipe assumes that you are launching Neurobagel on a machine with x86_64 (AMD/Intel) architecture (most Linux or Windows machines). If your machine instead uses ARM-based architecture (e.g., certain Macs), additionally change the following line in your <code>docker-compose.yml</code> file: <pre><code>    graph:\n        image: \"ontotext/graphdb:10.8.12\"\n</code></pre> to <pre><code>    graph:\n        image: \"ontotext/graphdb:10.8.12-arm64\"\n</code></pre> You can double check the architecture of your machine in the system settings or using the command <code>lscpu</code>.</p>"},{"location":"user_guide/getting_started/#launch-neurobagel","title":"Launch Neurobagel","text":"<p>Now you can launch your local Neurobagel node using Docker Compose:</p> <pre><code>docker compose up -d\n</code></pre> <p>This will:</p> <ul> <li>pull the required Docker images (if you haven't pulled them before)</li> <li>launch the containers for all the Neurobagel services</li> <li>automatically set up and configure the services based on your configuration files</li> <li>automatically upload example data to the Neurobagel graph</li> </ul> <p>You can check that your docker containers have launched correctly by running:</p> <pre><code>docker ps\n</code></pre> <p>and you will want to see something like this to show all 4 services running:</p> <pre><code>\u276f docker ps\nCONTAINER ID   IMAGE                              COMMAND                  CREATED         STATUS         PORTS                                                 NAMES\nd5e43f9ff0c2   neurobagel/federation_api:latest   \"/bin/sh -c 'uvicorn\u2026\"   8 seconds ago   Up 8 seconds   0.0.0.0:8080-&gt;8000/tcp, :::8080-&gt;8000/tcp             recipes-federation-1\nf0a26d0ea574   neurobagel/api:latest              \"/usr/src/api_entryp\u2026\"   8 seconds ago   Up 8 seconds   0.0.0.0:8000-&gt;8000/tcp, :::8000-&gt;8000/tcp             recipes-api-1\nd44d0b7359c8   ontotext/graphdb:10.8.12            \"/usr/src/neurobagel\u2026\"   8 seconds ago   Up 8 seconds   0.0.0.0:7200-&gt;7200/tcp, :::7200-&gt;7200/tcp, 7300/tcp   recipes-graph-1\n29a61a2d83de   neurobagel/query_tool:latest       \"/bin/sh -c 'npm run\u2026\"   8 seconds ago   Up 8 seconds   0.0.0.0:3000-&gt;5173/tcp, :::3000-&gt;5173/tcp             recipes-query_federation-1\n</code></pre>"},{"location":"user_guide/getting_started/#next-steps","title":"Next steps","text":"<p> You are now the proud owner of a running Neurobagel node. Here are some things you can do now:</p> <ul> <li>Try the Neurobagel node you just deployed by accessing:<ul> <li>your own query tool at http://localhost:3000, and reading the query tool usage guide</li> <li>the interactive docs for your node API at http://localhost:8000/docs, and reading the API usage guide</li> </ul> </li> <li>Prepare your own dataset for annotation with Neurobagel</li> <li>Add your own data to your Neurobagel graph to search</li> <li>Learn how to make a production deployment</li> <li>Hopefully all went well, but if you are experiencing issues, see how to get help</li> </ul>"},{"location":"user_guide/maintaining/","title":"Maintaining a node","text":"<p>Once your Neurobagel node is running and configured correctly, there are some recurring tasks you may have to do to keep it operating correctly.</p>"},{"location":"user_guide/maintaining/#updating-the-neurobagel-services","title":"Updating the Neurobagel services","text":""},{"location":"user_guide/maintaining/#updating-the-neurobagel-docker-images","title":"Updating the Neurobagel Docker images","text":"<p>We are continuously improving Neurobagel tools and services, so you may want to update your Neurobagel node to the latest version to benefit from new features and bug fixes. We always publish our tools as Docker images on Docker Hub.</p> <p>Each Docker image has a semantic version tag (vX.Y.Z), and also two rolling tags:</p> <ul> <li><code>latest</code> (the latest stable release). This is the default tag used in the Neurobagel <code>docker-compose.yml</code> file.</li> <li><code>nightly</code> (the latest build from the main branch). This tag is only used for compatibility testing and should not be used in production.</li> </ul> <p>You can pull the most recent docker images for Neurobagel tools by running:</p> <pre><code>docker compose pull\n</code></pre> Not sure what version you have? <p>Since <code>latest</code> is a rolling tag, each <code>latest</code> Docker image for a Neurobagel tool includes its corresponding semver number (vX.Y.X) as part of its Docker image labels.</p> <p>You can find the labels for an image you have pulled in the image metadata, e.g.: <pre><code>docker image inspect neurobagel/api:latest\n</code></pre> or, to view only the labels: <pre><code>docker image inspect --format='{{json .Config.Labels}}' neurobagel/api:latest\n</code></pre> In either case, you should see something like this in the output:</p> <p><pre><code>    \"Labels\": {\n        \"org.opencontainers.image.created\": \"https://github.com/neurobagel/api\",\n        \"org.opencontainers.image.revision\": \"01530f467e163f3dff595d3327bc60ba453de47d\",\n        \"org.opencontainers.image.version\": \"v0.3.1\"\n    }\n</code></pre> where <code>\"org.opencontainers.image.version\"</code> refers to the version number.</p>"},{"location":"user_guide/maintaining/#restarting-services-after-an-update","title":"Restarting services after an update","text":"<p>Whether you have updated the Docker images, the configuration, or the data of your Neurobagel node, you will need to restart the services to apply the changes.</p> <p>To shut down your Neurobagel services, navigate to the directory containing your deployment recipe and run:</p> <pre><code>docker compose down\n</code></pre> <p>Then, to start the services again:</p> <pre><code>docker compose up -d\n</code></pre> <p>For production deployments, you must specify the recipe filename</p> <p>To relaunch services for a <code>node</code> or <code>portal</code> deployment, you must provide the production Docker Compose recipe filename explicitly using the <code>-f</code> option:</p> <pre><code>docker compose -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"user_guide/maintaining/#updating-the-data-in-your-graph","title":"Updating the data in your graph","text":"<p>The Neurobagel deployment recipe launches a dedicated graph database that stores the datasets for a single node. The data in this graph database is loaded from the path specified in the <code>LOCAL_GRAPH_DATA</code> environment variable, and can be changed at any time.</p> <p>By default, the graph database will only contain an example dataset called <code>BIDS synthetic</code>.</p> <p>If you have followed the initial setup for deploying a Neurobagel node from our Docker Compose recipe, replacing the existing data in your graph database with your own data (or updated data) is a straightforward process.</p> <p>Once you have generated or updated the JSONLD files you want to upload, to update the data in your graph:</p> <ol> <li> <p>Shut down the Neurobagel node, if it is already running</p> <pre><code>docker compose down\n</code></pre> </li> <li> <p>Update the data files in the directory specified by the <code>LOCAL_GRAPH_DATA</code> variable in <code>.env</code>, or simply change the path to a directory containing your JSONLD files.</p> </li> <li> <p>(Re)start the Neurobagel node</p> <pre><code>docker compose up -d\n</code></pre> </li> </ol> <p>For production deployments, you must specify the recipe filename</p> <p>To relaunch services for a production <code>node</code> deployment, you must provide the production Docker Compose recipe filename explicitly using the <code>-f</code> option:</p> <pre><code>docker compose -f docker-compose.prod.yml up -d\n</code></pre> <p>Here are some other common scenarios where you might need to update the data in your graph:</p>"},{"location":"user_guide/maintaining/#following-a-change-in-my-dataset","title":"Following a change in my dataset","text":"<p>When using Neurobagel tools on a dataset that is still undergoing data collection, you may need to update the Neurobagel annotations and/or graph-ready data for the dataset when you want to add new subjects or measurements or to correct mistakes in prior data versions.</p> <p>For any of the below types of changes, you will need to regenerate a graph-ready <code>.jsonld</code> file for the dataset which reflects the change.</p>"},{"location":"user_guide/maintaining/#if-the-phenotypic-tabular-data-have-changed","title":"If the phenotypic (tabular) data have changed","text":"<p>If new variables have been added to the dataset such that there are new columns in the phenotypic TSV you previously annotated using Neurobagel's annotation tool, you will need to:</p> <ol> <li> <p>Generate an updated data dictionary by annotating the new variables in your TSV following the annotation workflow</p> </li> <li> <p>Generate a new graph-ready data file for the dataset by re-running the CLI on your updated TSV and data dictionary</p> </li> </ol>"},{"location":"user_guide/maintaining/#if-only-the-imaging-data-have-changed","title":"If only the imaging data have changed","text":"<p>If the BIDS data for a dataset have changed without changes in the corresponding phenotypic TSV (e.g., if new modalities or scans have been acquired for a subject), you have two options:</p> <ul> <li>If you still have access to the dataset's phenotypic JSONLD generated from the <code>pheno</code> command of the <code>bagel-cli</code> (step 1), you may choose to rerun only the <code>bids</code> CLI command on the updated BIDS directory. This will generate a new graph-ready data file with updated imaging metadata of subjects.</li> </ul> <p>OR</p> <ul> <li>Rerun the CLI entirely (<code>pheno</code> and <code>bids</code> steps) to generate a new graph-ready data file for the dataset.</li> </ul> <p>When in doubt, rerun both CLI commands.</p>"},{"location":"user_guide/maintaining/#if-only-the-subjects-have-changed","title":"If only the subjects have changed","text":"<p>If subjects have been added to or removed from the dataset but the phenotypic TSV is otherwise unchanged (i.e., only new or removed rows, without changes to the available variables), you will need to:</p> <ul> <li>Generate a new graph-ready data file for the dataset by re-running the CLI (<code>pheno</code> and <code>bids</code> steps) on your updated TSV and existing data dictionary</li> </ul>"},{"location":"user_guide/maintaining/#following-a-change-in-the-neurobagel-data-model","title":"Following a change in the Neurobagel data model","text":"<p>As Neurobagel continues developing the data model, new tool releases may introduce breaking changes to the data model for subject-level information in a <code>.jsonld</code> graph data file. Breaking changes will be highlighted in the release notes.</p> <p>If you have already created <code>.jsonld</code> files for a Neurobagel graph database but want to update your graph data to the latest Neurobagel data model following such a change, you can easily do so by rerunning the CLI on the existing data dictionaries and phenotypic TSVs for the dataset(s) in the graph. This will ensure that if you use the latest version of the Neurobagel CLI to process new datasets (i.e., generate new <code>.jsonld</code> files) for your database, the resulting data will not have conflicts with existing data in the graph.</p> <p>Note that if upgrading to a newer version of the data model, you should regenerate the <code>.jsonld</code> files for all datasets in your existing graph.</p>"},{"location":"user_guide/maintaining/#re-uploading-a-modified-dataset","title":"Re-uploading a modified dataset","text":"<p>To allow easy (re-)uploading of the updated <code>.jsonld</code> for your dataset(s) to a graph database, we recommend making a copy of it in a central directory on your research data fileserver for storing local Neurobagel <code>jsonld</code> datasets. Then, simply follow the steps for uploading/updating a dataset in the graph database.</p>"},{"location":"user_guide/maintaining/#updating-your-graph-backend-configuration","title":"Updating your graph backend configuration","text":""},{"location":"user_guide/maintaining/#updating-existing-database-user-permissions","title":"Updating existing database user permissions","text":"<p>If you want to change database access permissions (e.g., adding or removing access to a database) for an existing user in your GraphDB instance, you must do so manually.</p> <p>Of note, in GraphDB, there is no straightforward REST API call to update a user's database access permissions without replacing the list of their existing database permissions (<code>\"grantedAuthorities\"</code>) entirely.</p> <p>Tip</p> <p>You can verify a user's settings at any time with the following: <pre><code>curl -u \"admin:NewAdminPassword\" http://localhost:7200/rest/security/users/DBUSER\n</code></pre></p> <p>Example: if user <code>DBUSER</code> was granted read/write access to database <code>my_db1</code> with the following command (this command is run by default as part of <code>graphdb_setup.sh</code>):</p> <pre><code>curl -X PUT --header 'Content-Type: application/json' -d '\n{\"grantedAuthorities\": [\"WRITE_REPO_my_db\",\"READ_REPO_my_db\"]}' http://localhost:7200/rest/security/users/DBUSER -u \"admin:NewAdminPassword\"\n</code></pre> <p>To grant <code>DBUSER</code> read/write access to a second database <code>my_db2</code> (while keeping the existing access to <code>my_db1</code>), you would rerun the above <code>curl</code> command with all permissions (existing and new) specified since the existing permissions list will be overwritten:</p> <pre><code>curl -X PUT --header 'Content-Type: application/json' -d '\n{\"grantedAuthorities\": [\"WRITE_REPO_my_db1\",\"READ_REPO_my_db1\", \"WRITE_REPO_my_db2\",\"READ_REPO_my_db2\"]}' http://localhost:7200/rest/security/users/DBUSER -u \"admin:NewAdminPassword\"\n</code></pre> <p>Similarly, to revoke <code>my_db1</code> access so <code>DBUSER</code> only has access to <code>my_db2</code>:</p> <pre><code>curl -X PUT --header 'Content-Type: application/json' -d '\n{\"grantedAuthorities\": [\"WRITE_REPO_my_db2\",\"READ_REPO_my_db2\"]}' http://localhost:7200/rest/security/users/DBUSER -u \"admin:NewAdminPassword\"\n</code></pre> Managing user permissions using the GraphDB Workbench <p>If you are managing multiple GraphDB databases, the web-based administration interface for a GraphDB instance, the Workbench, might be an easier way to manage user permissions than the REST API. More information on using the GraphDB Workbench can be found here.</p>"},{"location":"user_guide/maintaining/#resetting-your-graphdb-instance","title":"Resetting your GraphDB instance","text":"<p>Each Neurobagel node has its own GraphDB instance, which is used to store the graph data for the node. If you want to reset your graph database and start again from scratch, follow these steps:</p> <ol> <li> <p>Ensure that your Neurobagel node is not running (i.e., shut down the Docker containers for the node).</p> <pre><code>docker compose down\n</code></pre> </li> <li> <p>Delete the Docker volume that contains the GraphDB data for your node.</p> <pre><code>docker volume rm neurobagel_node_graphdb_home\n</code></pre> <p>Replace <code>neurobagel_node_graphdb_home</code> with the name of the volume created for your node. It is usually named <code>&lt;project_name&gt;_graphdb_home</code> where <code>&lt;project_name&gt;</code> is the name of your Docker Compose stack as defined in <code>COMPOSE_PROJECT_NAME</code> in your <code>.env</code> file.</p> <code>docker volume ls</code> lists all volumes on your system <p>You can use the <code>docker volume ls</code> command to list all volumes on your system. This will help you identify the name of the volume that was created for your Neurobagel node.</p> </li> <li> <p>Launch your Neurobagel node again.</p> <pre><code>docker compose up -d\n</code></pre> <p>For production deployments, you must specify the recipe filename</p> <p>To relaunch services for a production <code>node</code> deployment, you must provide the production Docker Compose recipe filename explicitly using the <code>-f</code> option:</p> <pre><code>docker compose -f docker-compose.prod.yml up -d\n</code></pre> </li> </ol> <p>Some examples of when you might want to do this:</p> <ul> <li>You started but did not complete Neurobagel node setup previously and want to ensure you are using up-to-date instructions and recommended configuration options</li> <li>Your local node has stopped working after a configuration change to your graph database (e.g., your Neurobagel node API no longer starts or responds with an error, but you have confirmed all environment variables you have set should be correct)</li> <li>You need to modify credentials for your graph store</li> </ul> <p>Warning</p> <p>This action will wipe any graph databases and users you previously created!</p>"},{"location":"user_guide/maintaining/#environment-variables-reference","title":"Environment variables reference","text":"Ensure that shell variables do not clash with <code>.env</code> file <p>If the shell you run <code>docker compose</code> from already has any shell variable of the same name set, the shell variable will take precedence over the configuration of <code>.env</code>! In this case, make sure to <code>unset</code> the local variable first.</p> <p>For more information, see Docker's environment variable precedence.</p> <p>Tip</p> <p>Double check that any environment variables you have customized in <code>.env</code> are resolved with your expected values using the command <code>docker compose config</code>.</p> <p>Below are all the possible Neurobagel environment variables that can be set in <code>.env</code>.</p> Environment variable Default needs change? Description Default value if not set Used in these installation modes <code>COMPOSE_PROJECT_NAME</code> No (Used only by Docker Compose) Prefix for container names in the deployment; useful when running multiple deployments on the same machine. neurobagel_node Docker <code>NB_GRAPH_USERNAME</code> Yes Username to set for the graph database user. - Docker, Python <code>NB_GRAPH_SECRETS_PATH</code> Yes Path to files containing the secure passwords to set for the admin user (NB_GRAPH_ADMIN_PASSWORD.txt) and graph database user (NB_GRAPH_PASSWORD.txt). <code>./secrets</code> Docker <code>NB_GRAPH_DB</code> Yes Name to give your graph database (e.g., for a GraphDB database, use the format <code>repositories/{database_name}</code>) <code>repositories/my_db</code> Docker, Python <code>NB_GRAPH_MEMORY</code> No The maximum amount of memory that can be used by graph. Equivalent to setting the <code>-Xmx</code> parameter on the JVM. Value should be a number followed directly by a letter denoting the size. E.g. <code>264m</code> for 264 MB, <code>2g</code> for 2 GB. (For more info, see https://graphdb.ontotext.com/documentation/10.8/requirements.html#hardware-sizing.) <code>2g</code> Docker <code>LOCAL_GRAPH_DATA</code> Yes Path on your filesystem to the JSONLD files you want to upload to the graph database <code>./data</code> Docker <code>NB_GRAPH_PORT_HOST</code> No Port number on the host machine to map the graph server container port to <code>7200</code> Docker <code>NB_NAPI_ALLOWED_ORIGINS</code> No Origins allowed to make cross-origin resource sharing requests. Multiple origins must be separated with spaces in a single string enclosed in quotes. <code>\"\"</code> Docker, Python <code>NB_RETURN_AGG</code> No Whether to return only aggregate, dataset-level query results (excluding subject/session-level attributes). One of [true, false] <code>true</code> Docker, Python <code>NB_MIN_CELL_SIZE</code> No Minimum number of matching subjects required for a dataset to be returned as a query match. Datasets with matching subjects &lt;= this number will be excluded from query results. <code>0</code> Docker, Python <code>NB_NAPI_TAG</code> No Docker image tag for the Neurobagel node API <code>latest</code> Docker <code>NB_NAPI_PORT_HOST</code> No Port number on the host machine to map the Neurobagel node API container port to <code>8000</code> Docker <code>NB_NAPI_BASE_PATH</code> No (If using reverse proxy) The URL path where the node API is served from. Do not include a trailing slash. <code>\"\"</code> Docker <code>NB_NAPI_DOMAIN</code> Yes (Production only) The domain name where the n-API will be hosted from <code>\"\"</code> Docker <code>NB_FAPI_TAG</code> No Docker image tag for the Neurobagel federation API <code>latest</code> Docker <code>NB_FAPI_PORT_HOST</code> No Port number on the host machine to map the Neurobagel federation API container port to <code>8080</code> Docker <code>NB_FEDERATE_REMOTE_PUBLIC_NODES</code> No If \"True\", include public nodes in federation. If \"False\", only locally specified nodes in <code>local_nb_nodes.json</code> are queried. <code>true</code> Docker, Python <code>NB_FAPI_BASE_PATH</code> No (If using reverse proxy) The URL path where the federation API is served from. Do not include a trailing slash. <code>\"\"</code> Docker <code>NB_FAPI_DOMAIN</code> Yes (Production only) The domain name where the f-API will be hosted from <code>\"\"</code> Docker <code>NB_QUERY_TAG</code> No Docker image tag for the query tool <code>latest</code> Docker <code>NB_QUERY_PORT_HOST</code> No Port number used by the <code>query_tool</code> on the host machine <code>3000</code> Docker <code>NB_API_QUERY_URL</code> No (Testing only) URL of the Neurobagel f-API that the query tool will send its requests to. The query tool sends requests from a user's machine, so ensure the API URL is provided as a user would access it from their own machine. See also the query tool README. In a production deployment this variable is ignored. http://localhost:8080 Docker <code>NB_QUERY_APP_BASE_PATH</code> No (If using reverse proxy) The URL path for the query tool, determines the specific URL at which the app should be rendered for users to access it <code>/</code> Docker <code>NB_QUERY_DOMAIN</code> Yes (Production only) The domain name where the query tool will be hosted from <code>\"\"</code> Docker <code>NB_CONFIG</code> No (Experimental) Name of the Neurobagel community configuration to load, if the node uses a community's custom vocabularies. <code>Neurobagel</code> Docker, Python <code>NB_QUERY_HEADER_SCRIPT</code> No (Experimental, for development environments only) Custom script to add to the header section of the query tool site, such as for a GDPR-aware analytics tool. <code>\"\"</code> Docker <code>NB_ENABLE_AUTH</code> No (Experimental, for development environments only) Whether to enable authentication for cohort queries. One of [true, false] <code>false</code> Docker, Python <code>NB_QUERY_CLIENT_ID</code> No (Experimental, for development environments only) OAuth client ID for the query tool. Required if NB_ENABLE_AUTH is set to true. - Docker, Python <code>COMPOSE_PROFILES</code> No (Production only) Selects the production launch profile to use: \"node\" or \"portal\". See the documentation for details. <code>node</code> Docker"},{"location":"user_guide/preparing_imaging_data/","title":"Preparing the imaging metadata as a table","text":"<p>To be able to query information about available neuroimaging data in your dataset, you must provide Neurobagel with a <code>.tsv</code> file containing your dataset's neuroimaging metadata. We refer to this as a BIDS metadata table.</p> <p>If your dataset is already in BIDS, the Neurobagel CLI provides a <code>bids2tsv</code> command that will automatically generate this table for you.</p> <p>The BIDS metadata table lists each subject's available imaging files and modality information, using BIDS naming conventions, in the format shown below.</p>"},{"location":"user_guide/preparing_imaging_data/#example-bids-metadata-table","title":"Example BIDS metadata table","text":"sub ses suffix path sub-01 ses-01 T1w /data/bids-examples/synthetic/sub-01/ses-01/anat/sub-01_ses-01_T1w.nii sub-01 ses-01 bold /data/bids-examples/synthetic/sub-01/ses-01/func/sub-01_ses-01_task-rest_bold.nii sub-02 ses-01 T1w /data/bids-examples/synthetic/sub-02/ses-01/anat/sub-02_ses-01_T1w.nii sub-02 ses-01 bold /data/bids-examples/synthetic/sub-02/ses-01/func/sub-02_ses-01_task-rest_bold.nii ... ... ... ..."},{"location":"user_guide/preparing_imaging_data/#about-the-bids-metadata-table","title":"About the BIDS metadata table","text":"<p>The table must include at least four columns named exactly <code>sub</code>, <code>ses</code>, <code>suffix</code>, and <code>path</code> (adapted from BIDS entities). Additional columns are allowed but will be ignored by Neurobagel.</p> <p>Each row of the table indexes a single image file with the following metadata:</p> <ul> <li><code>sub</code> (required)<ul> <li>Subject ID</li> <li>Example: <code>sub-PD123</code></li> </ul> </li> <li><code>ses</code> (optional)<ul> <li>Session ID, if applicable</li> <li>Example: <code>ses-01</code></li> </ul> </li> <li><code>suffix</code> (required)<ul> <li>BIDS suffix corresponding to the modality (or sequence) of the image file</li> <li>See also Supported BIDS imaging modalities</li> <li>Example: <code>T1w</code></li> </ul> </li> <li><code>path</code> (required)<ul> <li>Path to the image file</li> <li>Example: <code>/data/pd/qpn/sub-PD123/ses-01/anat/sub-PD123_ses-01_T1w.nii.gz</code></li> </ul> </li> </ul>"},{"location":"user_guide/preparing_imaging_data/#supported-bids-imaging-modalities","title":"Supported BIDS imaging modalities","text":"<p>Neurobagel currently supports a subset of MRI modalities included in the BIDS specification, but we plan to expand this list for closer alignment with BIDS in the future.</p> <p>Supported modalities are listed below using their BIDS suffixes:</p> <ul> <li><code>dwi</code></li> <li><code>T1w</code></li> <li><code>T2w</code></li> <li><code>bold</code></li> <li><code>asl</code></li> <li><code>eeg</code></li> <li><code>meg</code></li> <li><code>pet</code></li> </ul> <p>Info</p> <p>For more information, see also the \"Modality specific files\" section of the BIDS specification or consult this master list of image suffixes supported by BIDS.</p>"},{"location":"user_guide/production_deployment/","title":"Production deployment","text":"<p>The getting started section is designed for quick local testing with minimal configuration. For a production deployment intended for external users, you should customize the settings to fit your specific needs.</p> <p>To help you with this, the Neurobagel <code>recipes</code> repository includes dedicated production deployment recipes that should cover the most common use cases.</p> <p>Start with a fresh recipe for each deployment</p> <p>Make a fresh clone of the <code>recipes</code> repository for each deployment you want to launch.</p> <p>For example, hosting multiple Neurobagel nodes, or a single node alongside a containerized proxy server, means creating a separate clone of the <code>recipes</code> repo for each node or proxy server. This will make it easier to update and maintain each deployment.</p>"},{"location":"user_guide/production_deployment/#deployable-services","title":"Deployable services","text":"<p>All Neurobagel services are containerized. The production deployment recipes use Docker Compose profiles to group related services and allow them to be launched together.</p> <p>Neurobagel services that are included in the Docker Compose deployment recipes:</p> <ul> <li>Neurobagel node API/n-API (<code>api</code>): The API that communicates with a single graph store and determines     how detailed the response to a query should be from that graph.</li> <li>Graph store (<code>graph</code>): A third-party RDF store that stores Neurobagel-harmonized data to be queried. At the moment our recipe uses the free tier     of GraphDB for this.</li> <li>Neurobagel federation/f-API (<code>federation</code>): A special API that can federate over one or more     Neurobagel nodes to provide a single point of access to multiple distributed databases.     By default it will federate over all public nodes and any local nodes you specify.</li> <li>Neurobagel query tool (<code>query_federation</code>): A web app that provides a graphical interface for users to query a     federation API and view the results from one or more nodes. Because the query tool is a static app and is run locally     in the user's browser, this service simply hosts the app.</li> </ul> <p>Two additional, third-party services are part of production deployment recipes:</p> <ul> <li>NGINX reverse proxy (<code>nginx-proxy</code>):     A containerized version     of the popular proxy server that lets you serve your Neurobagel services     under custom URLs, handling routes automatically.</li> <li>Automatic SSL certificate service (<code>acme-companion</code>):     A containerized companion tool     for nginx that automatically provisions SSL certificates for the routes     created by <code>nginx</code> so users can communicate with your services     via encrypted HTTPS connections.</li> </ul>"},{"location":"user_guide/production_deployment/#deployment-profiles","title":"Deployment profiles","text":"<p>Neurobagel offers different deployment profiles that allow you to launch specific combinations of services (listed below), depending on your use case.</p> <ul> <li> <p><code>node</code>: Deploys an individual Neurobagel node.     A Neurobagel node includes an internal graph database and a node API     that handles all incoming queries and talks to the graph database.     You can run several nodes on the same machine.</p> <ul> <li><code>api</code></li> <li><code>graph</code></li> </ul> </li> <li> <p><code>portal</code>: Deploys the federation engine and a connected web query interface.     Use this profile only if you need to host your own federated query tool,     e.g. to federate over nodes that are not in the     list of public Neurobagel nodes.</p> <ul> <li><code>federation</code></li> <li><code>query_federation</code></li> </ul> </li> <li> <p><code>proxy</code>: Deploys pre-configured, containerized     reverse-proxy services that will automatically set up routes     to your Neurobagel services under your desired URLs.</p> You can also use an existing proxy server <p>Our deployment instructions assume that there is no existing proxy server set up on the machine that will host your Neurobagel services. If you already have a proxy server setup, follow the slightly modified steps described in deploying with an existing proxy server.</p> <p>In this case, you can ignore the <code>proxy</code> deployment recipe.</p> </li> </ul>"},{"location":"user_guide/production_deployment/#setting-up-a-production-deployment","title":"Setting up a production deployment","text":""},{"location":"user_guide/production_deployment/#common-setup-for-all-deployment-profiles","title":"Common setup for all deployment profiles","text":"<p>Do these steps first for each deployment profile you set up</p> <p>Each production deployment profile requires a fresh deployment recipe and begins with the same initial steps. Complete these steps before following the profile-specific setup instructions.</p>"},{"location":"user_guide/production_deployment/#clone-the-recipe-repository","title":"Clone the recipe repository","text":"<p>Make a fresh clone of the <code>recipes</code> repository in a location of your choice.</p> <pre><code>git clone https://github.com/neurobagel/recipes.git recipes\n</code></pre> <p>Consider changing <code>recipes</code> to a name you will recognize in the future</p> <p>Then navigate into this directory for the remaining steps.</p> <pre><code>cd recipes\n</code></pre>"},{"location":"user_guide/production_deployment/#copy-the-template-configuration-files","title":"Copy the template configuration files","text":"<p>The recipe repository includes templates of files for configuring your deployment: <code>.env</code> and <code>local_nb_nodes.json</code>. Copy and rename these templates, but do not edit the templates themselves.</p> <pre><code>cp template.env .env\ncp local_nb_nodes.template.json local_nb_nodes.json\n</code></pre>"},{"location":"user_guide/production_deployment/#proxy-server","title":"Proxy server","text":"Skip if you already have a reverse proxy server <p>If you already have a reverse proxy server set up and want to continue using it, do not follow this section and instead continue with our guide on production deployment with an existing proxy server.</p> Always launch the proxy server first <p>Our reverse proxy recipe is set up to automatically configure routes to the Neurobagel services you launch. In order to do that, the proxy server must already be running when you launch a new Neurobagel service. If you have already launched Neurobagel services (e.g. node or portal), shut them down again, launch the proxy server, and then relaunch the services.</p> <p>Start from a fresh deployment recipe!</p> <p>To host your Neurobagel node services under a custom URL (e.g. <code>https://www.myfirstnode.org/query</code>) rather than a server IP address and port (e.g. <code>http://192.168.0.1:3000</code>), we provide a recipe for you to easily set up an NGINX reverse proxy alongside your Neurobagel services.</p> <p>Make sure that:</p> <ul> <li>you have access to the domain you want to host your services at, so that you can create a DNS entry that points at the machine that will host your Neurobagel services.</li> <li>your machine firewall allows incoming connections on ports 80 (HTTP) and 443 (HTTPS). Both are necessary to enable HTTPS connections.</li> </ul> <p>Launch the proxy server using the corresponding deployment recipe:</p> <pre><code>docker compose -f docker-compose.proxy.yml up -d\n</code></pre> <p>You should now see the <code>nginx-proxy</code> and <code>acme-companion</code> services running:</p> <pre><code>docker ps\n</code></pre>"},{"location":"user_guide/production_deployment/#node","title":"Node","text":"<p>Start from a fresh deployment recipe!</p> Make sure the proxy service is already running <p>The default <code>portal</code> deployment recipe requires that you have already deployed the proxy server.</p>"},{"location":"user_guide/production_deployment/#set-graph-store-credentials","title":"Set graph store credentials","text":"<p>This is a security relevant section!</p> <p>The graph store (GraphDB instance) in a Neurobagel node is secured with password-based access and includes two users: an <code>admin</code> superuser and a regular database user, both of which are automatically configured by the Neurobagel deployment recipe. Passwords for both users are defined via files in the <code>./secrets</code> directory of the recipes repository, while the regular database username is set through an environment variable in <code>.env</code> file.</p> Changing user credentials after the first launch requires a hard reset <p>If you've previously launched a Neurobagel deployment (Docker Compose stack), you'll need to reset your graph store for any changes you have made to user credentials to take effect. Any other configuration changes you've already made will be applied when you re-launch your node.</p> <ol> <li> <p>In your <code>.env</code>, set a custom username and database name for your graph store by editing the following variables:</p> <ul> <li><code>NB_GRAPH_USERNAME</code></li> <li><code>NB_GRAPH_DB</code></li> </ul> </li> <li> <p>In the (<code>./secrets</code> directory, change the default passwords by replacing the contents of the file <code>NB_GRAPH_ADMIN_PASSWORD.txt</code> for the <code>admin</code> superuser, and the file <code>NB_GRAPH_PASSWORD.txt</code> for the graph database user (corresponding to <code>NB_GRAPH_USERNAME</code>).</p> <ul> <li>To generate a random password in the terminal, you can use:</li> </ul> <pre><code>openssl rand -hex 16\n</code></pre> <ul> <li>(Optional) You can change the directory where your password files are stored by editing the <code>NB_GRAPH_SECRETS_PATH</code> variable in <code>.env</code>.</li> </ul> Graph store passwords are only for administrator use! <p>The <code>admin</code> user and graph database user credentials are intended solely for internal use by the deployment recipe scripts that automatically set up and update the graph store, or for a node administrator to interact directly with the graph store. These credentials also secure internal communication between your graph store and its node API, ensuring that node users cannot query your graph directly. GraphDB user credentials are not intended for use by a general node query user.</p> Passwords are handled as Docker secrets <p>The contents of <code>NB_GRAPH_ADMIN_PASSWORD.txt</code> and <code>NB_GRAPH_PASSWORD.txt</code> are passed to Neurobagel containers as Docker secrets. This ensures that your passwords are not exposed in the container logs or in the <code>docker-compose.yml</code> file.</p> <p>Do not share your password files with others.</p> </li> </ol>"},{"location":"user_guide/production_deployment/#add-data-to-the-node","title":"Add data to the node","text":"<p>By default, any JSONLD data in the <code>./data</code> subdirectory of your deployment recipe directory will be automatically uploaded to the graph store.</p> <p>To add the dataset JSONLD files for your node, you can either:</p> <ul> <li>place the JSONLD files inside <code>./data</code> (remember to delete the example JSONLD), OR</li> <li>change the path where the deployment recipe will look for JSONLD files by editing the variable <code>LOCAL_GRAPH_DATA</code> in your <code>.env</code> file</li> </ul>"},{"location":"user_guide/production_deployment/#set-node-response-granularity","title":"Set node response granularity","text":"<p>This is a security relevant section!</p> <p>Review and change as needed the following variables in <code>.env</code> based on your data sharing requirements:</p> <ul> <li><code>NB_RETURN_AGG</code>: whether to return aggregate counts only, instead of subject-level records</li> <li><code>NB_MIN_CELL_SIZE</code>: minimum matching subject threshold for dataset visibility in queries</li> </ul> <p>For more details on all available environment variables, check the Environment variable reference</p>"},{"location":"user_guide/production_deployment/#set-node-domain","title":"Set node domain","text":"<p>In your <code>.env</code> file, uncomment the variable <code>NB_NAPI_DOMAIN</code> and set it to the domain (including any subdomain) that you want to use for your node API (the web-accessible part of your node).</p> <pre><code>NB_NAPI_DOMAIN=node.mydomain.org\n</code></pre> <p>Do not include the protocol (<code>http://</code> or <code>https://</code>) in the domain name</p>"},{"location":"user_guide/production_deployment/#set-node-deployment-profile","title":"Set node deployment profile","text":"<p>In your <code>.env</code> file, ensure that <code>COMPOSE_PROFILES</code> is set to the <code>node</code> profile. This is the default value.</p> <pre><code>COMPOSE_PROFILES=node\n</code></pre>"},{"location":"user_guide/production_deployment/#set-node-subdirectory-path","title":"Set node subdirectory path","text":"<p>This is an optional step</p> <p>To host your node API on a subdirectory of your domain (e.g. <code>mydomain.org/node</code>), uncomment and set <code>NB_NAPI_BASE_PATH</code> to the desired subdirectory path in the <code>.env</code> file.</p> <pre><code>NB_NAPI_BASE_PATH=\"/node\"\n</code></pre> <p>This is useful if you want to serve several nodes (or services) on the same domain, because you can use a different subdirectory for each (e.g. <code>mydomain.org/node1</code>, <code>mydomain.org/node2</code>, ...).</p> <p>Custom paths must include a leading slash <code>/</code></p>"},{"location":"user_guide/production_deployment/#launch-node","title":"Launch node","text":"<p>Save the changes to your <code>.env</code> file and launch your node:</p> <pre><code>docker compose -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"user_guide/production_deployment/#portal","title":"Portal","text":"<p>Start from a fresh deployment recipe!</p> Make sure the proxy server is already running <p>The default <code>portal</code> deployment recipe requires that you have already deployed the proxy server.</p>"},{"location":"user_guide/production_deployment/#set-nodes-to-federate","title":"Set nodes to federate","text":"<p>To host your own query portal that federates over a set of nodes, use your <code>local_nb_nodes.json</code> to configure the nodes of interest.</p> <p>Each node to be federated over is defined using a dictionary with two required keys:</p> <ul> <li><code>NodeName</code>: Display name of the node, which will be shown in the query portal</li> <li><code>ApiURL</code>: URL of the node API for the node</li> </ul> <p>Example:</p> local_nb_nodes.json<pre><code>    [\n        {\n            \"NodeName\": \"Parkinson's Disease Data - Site 1\",\n            \"ApiURL\": \"https://mydomain.org/site1\"\n        },\n        {\n            \"NodeName\": \"Parkinson's Disease Data - Site 2\",\n            \"ApiURL\": \"https://mydomain.org/site2\"\n        }\n    ]\n</code></pre> <p><code>ApiURL</code> must include the protocol (<code>http://</code> or <code>https://</code>)</p> Public Neurobagel nodes do not need to be included <p>We maintain a list of publicly accessible Neurobagel nodes here. By default, every new f-API will look up this list on startup and include it in its internal list of nodes to federate over (this can be disabled using the environment variable <code>NB_FEDERATE_REMOTE_PUBLIC_NODES</code>). This means that you do not have to manually add these public nodes to your <code>local_nb_nodes.json</code> file.</p> <p>Avoid creating an infinite loop</p> <p>Make sure you do not include your own f-API in the list of nodes to federate over. This will cause an infinite request loop that will likely overload your service, as an f-API will be repeatedly making requests to itself.</p>"},{"location":"user_guide/production_deployment/#set-portal-domains","title":"Set portal domains","text":"<p>In your <code>.env</code> file, set the domain names for your web query tool and federation API by uncommenting and setting the following variables:</p> <ul> <li><code>NB_QUERY_DOMAIN</code> for the query tool</li> <li><code>NB_FAPI_DOMAIN</code> for the federation API</li> </ul> <p>Do not include the protocol (<code>http://</code> or <code>https://</code>) in the domain name</p>"},{"location":"user_guide/production_deployment/#set-portal-subdirectory-paths","title":"Set portal subdirectory paths","text":"<p>This is an optional step</p> <p>You may want to have one or several of your portal services respond on a subdirectory of your domain (e.g. <code>myinstitute.org/federate</code>).</p> <p>To host one or more of your portal services on a subdirectory of your domain (e.g. <code>mydomain.org/federate</code>), uncomment and set the following variable(s) to the desired subdirectory path(s) in the <code>.env</code> file:</p> <ul> <li><code>NB_QUERY_APP_BASE_PATH</code> for the query tool</li> <li><code>NB_FAPI_BASE_PATH</code> for the federation API</li> </ul> <p>Custom paths must include a leading slash <code>/</code></p> <p>This is useful if you want to serve the services on the same domain, because you can use a different subdirectory for each (e.g. <code>mydomain.org/federate</code>, <code>mydomain.org/query</code>).</p>"},{"location":"user_guide/production_deployment/#set-portal-deployment-profile","title":"Set <code>portal</code> deployment profile","text":"<p>In your .env file, set <code>COMPOSE_PROFILES</code> to the <code>portal</code> profile.</p> <pre><code>COMPOSE_PROFILES=portal\n</code></pre>"},{"location":"user_guide/production_deployment/#launch-portal","title":"Launch portal","text":"<pre><code>docker compose -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"user_guide/production_deployment/#making-your-node-publicly-discoverable","title":"Making your node publicly discoverable","text":"<p>The public Neurobagel query tool (https://query.neurobagel.org) provides query federation to all publicly accessible Neurobagel nodes.</p> <p>To make your node queryable at https://query.neurobagel.org, it simply needs to be added to our public federation index on GitHub.</p>"},{"location":"user_guide/production_deployment/#if-you-have-a-github-account","title":"If you have a GitHub Account","text":"<ol> <li> <p>Fork the neurobagel/menu repository.</p> </li> <li> <p>Add your node name and node API URL to the public federation index JSON file, using the following format:</p> <pre><code>{\n    \"NodeName\": \"NAME OF YOUR NODE\",\n    \"ApiURL\": \"https://URL-OF-YOUR-NODE-API\"\n}\n</code></pre> <p><code>NodeName</code> defines the display name of your node as it will appear the Neurobagel query tool.</p> </li> <li> <p>Open a pull request in the neurobagel/menu repository.</p> </li> </ol>"},{"location":"user_guide/production_deployment/#if-you-do-not-have-a-github-account","title":"If you do not have a GitHub Account","text":"<p>Join the Neurobagel Discord server and message <code>@neurobagel/dev</code> with your node information, so that a maintainer can add your node to the public federation index.</p>"},{"location":"user_guide/production_deployment_with_own_proxy/","title":"Production deployment with an existing proxy","text":"<p>If you already have a proxy server setup on your machine, you need to make a few adjustments to the standard deployment steps.</p> <p>Do not follow this section if you are using Neurobagel's containerized proxy server</p> <p>If you are unfamiliar with managing and maintaining a bare-metal reverse proxy, you should use the containerized proxy server provided with Neurobagel deployment recipes instead.</p> <p>We document how to run Neurobagel with an existing reverse proxy to help facilitate integration into established server setups. This type of deployment needs a good deal more manual configuration and maintenance.</p>"},{"location":"user_guide/production_deployment_with_own_proxy/#follow-the-default-setup","title":"Follow the default setup","text":"<p>Do not launch the <code>proxy</code> deployment profile</p> <p>If you want to use your existing reverse proxy setup, make sure to not launch the <code>proxy</code> deployment template provided by Neurobagel, or shut it down if you have already launched it.</p> <p>Deploying Neurobagel with an existing proxy server is very similar to using the default deployment recipes.</p> <p>Begin by following the default setup instructions for your desired deployment profile, but skip the final launch step (i.e., skip \"Launch node\" or \"Launch portal\"):</p> <ul> <li>For a <code>node</code> deployment follow the node setup</li> <li>For a <code>portal</code> deployment follow the portal setup</li> </ul> Deploying behind an existing proxy uses a different Docker Compose recipe <p>Attempting to launch a <code>node</code> or <code>portal</code> using the default deployment recipe without the proxy server deployment recipe running will most likely fail.</p>"},{"location":"user_guide/production_deployment_with_own_proxy/#default-ports-of-services","title":"Default ports of services","text":"Don't publicly expose service ports on a production server <p>We're providing the default ports as a reference for local deployment, testing, and for scenarios where you do not want to use the provided reverse proxy deployment recipes.</p> <p>Where possible, we strongly recommend that you avoid opening service ports to a public network.</p> <p>Neurobagel node services run inside Docker containers. Each service listens on an internal port within its container and exposes a host port that makes it accessible from the host machine. Below, we list the default host ports for each service along with the environment variables that can be used to configure them.</p> <ul> <li><code>api</code> (the node API)<ul> <li>environment variable: <code>NB_NAPI_PORT_HOST</code></li> <li>default host port: <code>8000</code></li> </ul> </li> <li><code>federation</code> (the federation API)<ul> <li>environment variable: <code>NB_FAPI_PORT_HOST</code></li> <li>default host port: <code>8080</code></li> </ul> </li> <li><code>query_tool</code> (the graphical query web interface)<ul> <li>environment variable: <code>NB_QUERY_PORT_HOST</code></li> <li>default host port: <code>3000</code></li> </ul> </li> <li><code>graph</code> (the internal graph database)<ul> <li>environment variable: <code>NB_GRAPH_PORT_HOST</code></li> <li>default host port: <code>7200</code></li> </ul> </li> </ul>"},{"location":"user_guide/production_deployment_with_own_proxy/#set-service-host-ports","title":"Set service host ports","text":"Differences from the default deployment recipe <p>Unlike the default production Docker Compose recipe, the deployment recipe for an existing proxy has the following characteristics:</p> <ul> <li>does not expect an existing proxy Docker network to connect with</li> <li>does expose the service ports to the host machine,     so you can configure your existing proxy server     to reach each service on <code>localhost</code> ports</li> </ul> <p>In our modified deployment recipe for an existing proxy server, Neurobagel services bind to default ports on the host.</p> <p>In your <code>.env</code> file, you can change these ports to avoid conflicts with existing services. Simply uncomment and set the relevant <code>NB_&lt;SERVICE&gt;_PORT_HOST</code> variables. See default ports for the list of port variables.</p>"},{"location":"user_guide/production_deployment_with_own_proxy/#configure-your-existing-reverse-proxy","title":"Configure your existing reverse proxy","text":"<p>You must manually configure the existing reverse proxy on your machine for Neurobagel services.</p> <p>For each service you deploy, this includes:</p> <ul> <li>configuring your reverse proxy routing rules so incoming   requests are directed to the correct service under the appropriate domain/path</li> <li>provisioning and keeping SSL certificates up to date for each domain used to host   the service</li> </ul> <p>Please refer to the documentation of your existing reverse proxy server on how to do this.</p>"},{"location":"user_guide/production_deployment_with_own_proxy/#launch-services-behind-your-existing-proxy","title":"Launch services behind your existing proxy","text":"<p>Ensure that you have correctly followed the setup instructions for your desired deployment profile.</p> <p>Then launch your services using the <code>docker-compose.noproxy.prod.yml</code> compose file:</p> <pre><code>docker compose -f docker-compose.noproxy.prod.yml up -d\n</code></pre>"},{"location":"user_guide/public_nodes/","title":"Search the public nodes","text":"<p>The public query tool at query.neurobagel.org queries the public Neurobagel federation API at federate.neurobagel.org which provides access to all publicly accessible Neurobagel nodes.</p> <p>To start running your own cohort queries, all you have to do is visit query.neurobagel.org, enter your cohort criteria into the web interface, and click the \"Submit\" button.</p>"},{"location":"user_guide/public_nodes/#public-neurobagel-nodes","title":"Public Neurobagel Nodes","text":"<p>At the moment, the following public Neurobagel nodes are available (you can query a specific node by selecting it from the dropdown under \"Neurobagel graph\" in the query tool):</p> <ul> <li>OpenNeuro. This node contains a (growing) subset of the datasets on OpenNeuro.   The datasets you can find in this node have been annotated by the community and live in the   OpenNeuroDatasets-JSONLD GitHub organization.</li> <li>International Neuroimaging Data-sharing Initiative (INDI): This node contains public datasets from the INDI project.   At the moment, the following datasets are queryable in the INDI node:<ul> <li>ABIDE 1</li> <li>ABIDE 2</li> <li>ADHD 200</li> <li>Consortium for Reliability and Reproducibility (CoRR) datasets.</li> </ul> </li> <li>Quebec Parkinson Network: This node contains the Quebec Parkinson Network datasets.   Unlike the other two public nodes, the Quebec Parkinson Network node will not return participant level details.</li> </ul> <p>All nodes except for the Quebec Parkinson Network node allow you to download both participant-level information and the corresponding imaging data (where available) for the cohorts you search. Downloading of imaging data is performed via Datalad.</p>"},{"location":"user_guide/public_nodes/#private-neurobagel-nodes","title":"Private Neurobagel nodes","text":"<p>Nodes that are not purposefully made public are not accessible outside of the institute or network where they are deployed. If you are interested in deploying a Neurobagel node for your institution, please refer to our deployment documentation for more information.</p>"},{"location":"user_guide/query_tool/","title":"The Neurobagel Query Tool","text":"<p>Neurobagel's query tool is a web interface for searching across a Neurobagel graph based on various subject clinical-demographic and imaging parameters.</p> <p>The query tool is a React application, developed in TypeScript using a variety of tools including Vite, Cypress, and MUI.</p>"},{"location":"user_guide/query_tool/#quickstart","title":"Quickstart","text":"<p>The query tool is hosted at https://query.neurobagel.org/ and interfaces with Neurobagel federation API.</p>"},{"location":"user_guide/query_tool/#local-installation","title":"Local Installation","text":"<p>To run the query tool locally, you have two options:</p> <ol> <li>Use our docker image</li> <li>Do a manual install from the cloned git repo.</li> </ol> <p>but before proceeding with either you need to set the environment variables.</p>"},{"location":"user_guide/query_tool/#mandatory-configuration","title":"Mandatory configuration","text":"Environment variable Type Required Default value if not set Example <code>NB_API_QUERY_URL</code> string Yes - <code>https://federate.neurobagel.org/</code> <code>NB_QUERY_APP_BASE_PATH</code> string No <code>/</code> <code>/query/</code> <code>NB_ENABLE_AUTH</code> boolean No <code>false</code> <code>false</code> <code>NB_QUERY_CLIENT_ID</code> string Yes (if <code>NB_ENABLE_AUTH</code> is set to true) - <code>\"\"</code> <code>NB_QUERY_HEADER_SCRIPT</code> string No <code>\"\"</code> <code>'&lt;script defer data-domain=\"mydomain\" src=\"plausible\"&gt;'</code>"},{"location":"user_guide/query_tool/#nb_api_query_url","title":"<code>NB_API_QUERY_URL</code>","text":"<p>You'll need to set the <code>NB_API_QUERY_URL</code> environment variable required to run the query tool. <code>NB_API_QUERY_URL</code> is the Neurobagel API URL that the query tool uses to send requests to for results.</p>"},{"location":"user_guide/query_tool/#nb_query_app_base_path","title":"<code>NB_QUERY_APP_BASE_PATH</code>","text":"<p>If you are using a custom configuration where the query tool is accessible via a path other than the root (<code>/</code>), you need to set the <code>NB_QUERY_APP_BASE_PATH</code> to your custom path. This ensures that the query tool is correctly rendered and accessible at the specified URL</p>"},{"location":"user_guide/query_tool/#nb_enable_auth","title":"<code>NB_ENABLE_AUTH</code>","text":"<p>If the API you'd like to send queries to requires authentication, you need to set <code>NB_ENABLE_AUTH</code> to <code>true</code> as it is <code>false</code> by default. This will enable authentication flow of the app.</p>"},{"location":"user_guide/query_tool/#nb_query_client_id","title":"<code>NB_QUERY_CLIENT_ID</code>","text":"<p>If the <code>NB_ENABLE_AUTH</code> is set to <code>true</code> (it is <code>false</code> by default), you need to provide a valid client ID for the authentication. At the moment, query tool uses Google for authentication, so you need to obtain a client ID from Google developer console. See documentation for more information.</p>"},{"location":"user_guide/query_tool/#nb_query_header_script","title":"<code>NB_QUERY_HEADER_SCRIPT</code>","text":"<p>If you want to add a custom script to the header of the query tool, you can set the <code>NB_QUERY_HEADER_SCRIPT</code> environment variable to the script you want to add. This script will be added to the header of the query tool. For example, in our production deployment we use the GDPR aware analytics tool Plausible, so we set the <code>NB_QUERY_HEADER_SCRIPT</code> to the script provided by Plausible. When you use this variable, make sure you use single quotes to include the script like so:</p> <pre><code>NB_QUERY_HEADER_SCRIPT='&lt;script defer data-domain=\"mydomain\" src=\"plausible\"&gt;'\n</code></pre>"},{"location":"user_guide/query_tool/#set-the-environment-variables","title":"Set the environment variables","text":"<p>To set environment variables, create a <code>.env</code> file in the root directory and add the environment variables there. If you're running a neurobagel node-API locally on your machine (following the instructions here), your <code>.env</code> file would look something like this:</p> <pre><code>NB_API_QUERY_URL=http://localhost:8000/\n</code></pre> <p>if you're using the remote api, your <code>.env</code> file would look something like this:</p> <pre><code>NB_API_QUERY_URL=https://federate.neurobagel.org/\n</code></pre> <p>if you're using a remote api with authentication, your <code>.env</code> file would look something like this:</p> <pre><code>NB_API_QUERY_URL=https://federate.neurobagel.org/\nNB_ENABLE_AUTH=true\nNB_QUERY_CLIENT_ID=46923719231972-dhsahgasl3123.apps.googleusercontent.com\n</code></pre> <p> The protocol matters here. If you wish to use the Neurobagel remote API, ensure your <code>NB_API_QUERY_URL</code> uses <code>https</code> instead of <code>http</code>.</p>"},{"location":"user_guide/query_tool/#docker-installation","title":"Docker installation","text":"<p>To obtain the query tool docker image, simply run the following command in your terminal:</p> <pre><code>docker pull neurobagel/query_tool:latest\n</code></pre> <p>This Docker image includes the latest release of the query tool and a minimal http server to serve the static tool.</p> <p>To launch the query tool Docker container and pass in the <code>.env</code> file you have created, simply run</p> <pre><code>docker run -p 5173:5173 --env-file=.env neurobagel/query_tool:latest\n</code></pre> <p>Then you can access the query tool at http://localhost:5173</p> <p>Note: the query tool is listening on port <code>5173</code> inside the docker container, replace port <code>5173</code> by the port you would like to expose to the host. For example if you'd like to run the tool on port <code>8000</code> of your machine you can run the following command:</p> <pre><code>docker run -p 8000:5173 --env-file=.env neurobagel/query_tool:latest\n</code></pre>"},{"location":"user_guide/query_tool/#manual-installation","title":"Manual installation","text":"<p>To install the query tool directly, you'll need node package manager (npm) and Node.js. You can find the instructions on installing npm and node in the official documentation.</p> <p>Once you have npm and node installed, you'll need to install the dependencies outlined in the package.json file. You can do so by running the following command:</p> <pre><code>npm install\n</code></pre> <p>To launch the tool in developer mode run the following command:</p> <pre><code>npm run dev\n</code></pre> <p>You can also build and then run the tool from (production) build of the application by running the following command:</p> <pre><code>npm run build &amp;&amp; npm run preview\n</code></pre> <p>You can verify the tool is running by watching for the` info messages from Vite regarding environment, rendering, and what port the tool is running on in your terminal.</p>"},{"location":"user_guide/query_tool/#developer-setup","title":"Developer setup","text":"<p>Having installed the dependencies, run the following command to enable husky <code>pre-commit</code> and <code>post-merge</code> hooks:</p> <pre><code>npx husky init\n</code></pre>"},{"location":"user_guide/query_tool/#docker-compose-testing-environment-for-development","title":"Docker compose testing environment for development","text":"<p>Since the query tool relies on other neurobagel tools to function, their presence is often required during development. To facilitate this, a docker compose containing a complete testing environment has been created. To use it follow the steps below:</p> <ol> <li>Install <code>recipes</code> and <code>neurobagel_examples</code> submodules:</li> </ol> <pre><code>git submodule init\ngit submodule update\n</code></pre> <ol> <li>Pull the latest images and bring up the stack using the <code>test</code> profile:</li> </ol> <pre><code>docker compose --profile test pull &amp;&amp; docker compose --profile test up -d\n</code></pre> <p>NOTE: Make sure your .env file in the root directory doesn't contain any of the environment variables used in the docker compose file as it will conflict with the configuration, since docker compose will try to use .env by default.</p>"},{"location":"user_guide/query_tool/#usage","title":"Usage","text":"<p>To define a cohort, set your inclusion criteria using the following:</p> <ul> <li>Age: Minimum and/or maximum age (in years) of participant that should be included in the results.</li> <li>Sex: Sex of participant that should be included in the results.</li> <li>Diagnosis: Diagnosis of participant that should be included in the results</li> <li>Minimum number of imaging sessions: Minimum number of imaging sessions that participant should have to be included in the results.</li> <li>Minimum number of phenotypic sessions: Minimum number of phenotypic sessions that participant should have to be included in the results.</li> <li>Assessment tool: Non-imaging assessment completed by participant that should be included in the results.</li> <li>Imaging modality: Imaging modality of participant scans that should be included in the results.</li> <li>Pipeline name: Name of the pipeline used to process subject scans.</li> <li>Pipeline version: Version of the pipeline used to process subject scans.</li> </ul> <p>Once you've defined your criteria, submit them as a query and the query tool will display the results.\\</p>"},{"location":"user_guide/query_tool/#downloading-query-results","title":"Downloading query results","text":"<p>For a given query, there are two formats of query results that users can download as a TSV file. At least one dataset matching the query must be selected in the results panel in order to download the query results.</p>"},{"location":"user_guide/query_tool/#harmonized-tsv-data-with-descriptive-labels","title":"Harmonized TSV data with descriptive labels","text":"<p>The default TSV available for download describes the available harmonized attributes and metadata for subjects matching the query, from the (selected) matching datasets. Harmonized data are provided as standardized vocabulary-derived labels for readability.</p> <p>Each row corresponds to a single matching subject session, except for datasets configured to only return aggregate results.</p> Example query result TSV DatasetName RepositoryURL NumMatchingSubjects SubjectID SessionID ImagingSessionPath SessionType NumMatchingPhenotypicSessions NumMatchingImagingSessions Age Sex Diagnosis Assessment SessionImagingModalities SessionCompletedPipelines DatasetImagingModalities DatasetPipelines AccessLink Balloon Analog Risk-taking Task https://github.com/OpenNeuroDatasets-JSONLD/ds000001.git 16 protected protected protected protected protected protected protected protected protected protected protected protected Blood-Oxygen-Level Dependent image,T1-weighted image,T2-weighted image nan nan Classification learning https://github.com/OpenNeuroDatasets-JSONLD/ds000002.git 17 protected protected protected protected protected protected protected protected protected protected protected protected Blood-Oxygen-Level Dependent image,T1-weighted image,T2-weighted image nan nan BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-01 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-01/ses-01 Imaging 2 2 nan nan nan nan T1-weighted image,Blood-Oxygen-Level Dependent image fmriprep 23.1.3,freesurfer 7.3.2 Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-01 ses-01 nan Phenotypic 2 2 34.1 female Healthy Control Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-01 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-01/ses-02 Imaging 2 2 nan nan nan nan T1-weighted image,Blood-Oxygen-Level Dependent image nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-01 ses-02 nan Phenotypic 2 2 35.3 female Healthy Control Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-02 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-02/ses-01 Imaging 2 2 nan nan nan nan T1-weighted image,Blood-Oxygen-Level Dependent image fmriprep 23.1.3,freesurfer 7.3.2 Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-02 ses-01 nan Phenotypic 2 2 nan male Attention deficit hyperactivity disorder Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-02 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-02/ses-02 Imaging 2 2 nan nan nan nan T1-weighted image,Blood-Oxygen-Level Dependent image freesurfer 7.3.2 Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-02 ses-02 nan Phenotypic 2 2 39 male Attention deficit hyperactivity disorder Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-03 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-03/ses-01 Imaging 2 2 nan nan nan nan T1-weighted image,Blood-Oxygen-Level Dependent image nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-03 ses-01 nan Phenotypic 2 2 22.1 nan nan Montreal cognitive assessment nan nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-03 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-03/ses-02 Imaging 2 2 nan nan nan nan T1-weighted image,Blood-Oxygen-Level Dependent image nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-03 ses-02 nan Phenotypic 2 2 23.2 nan Attention deficit hyperactivity disorder Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-04 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-04/ses-01 Imaging 2 2 nan nan nan nan T1-weighted image,Blood-Oxygen-Level Dependent image nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-04 ses-01 nan Phenotypic 2 2 21.1 female Healthy Control Unified Parkinsons disease rating scale score nan nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-04 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-04/ses-02 Imaging 2 2 nan nan nan nan T1-weighted image,Blood-Oxygen-Level Dependent image nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-04 ses-02 nan Phenotypic 2 2 22.3 female Healthy Control Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-05 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-05/ses-01 Imaging 2 2 nan nan nan nan T1-weighted image,Blood-Oxygen-Level Dependent image nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-05 ses-01 nan Phenotypic 2 2 42.5 male Attention deficit hyperactivity disorder Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-05 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-05/ses-02 Imaging 2 2 nan nan nan nan T1-weighted image,Blood-Oxygen-Level Dependent image nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-05 ses-02 nan Phenotypic 2 2 43.2 male Attention deficit hyperactivity disorder Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Blood-Oxygen-Level Dependent image,T1-weighted image fmriprep 23.1.3,freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git <p>Columns in the TSV are described below: <code>*</code> = required values</p> Column name Description DatasetName * name of the dataset RepositoryURL URL to a repository where the dataset can be downloaded or retrieved from (e.g., DataLad, Zenodo, GitHub) NumMatchingSubjects * (dataset-level) total number of subjects matching the query in the dataset SubjectID * subject label SessionID * session label ImagingSessionPath (imaging sessions only) path to the session directory, or subject directory if only one session exists. Either an absolute path from the filesystem root where the dataset is stored, or a relative path from the dataset root for DataLad datasets. SessionType * type of data acquired in the session, either <code>ImagingSession</code> or <code>PhenotypicSession</code>. Represents the nature of data being described, without denoting specific time or visits. e.g., A session in which both imaging and non-imaging data were acquired would be represented by separate rows, one per type. Age subject age Sex subject sex Diagnosis list of diagnoses for subject Assessment list of assessments completed by subject NumMatchingPhenotypicSessions * (subject-level) total number of phenotypic sessions for the subject which match the query NumMatchingImagingSessions * (subject-level) total number of imaging sessions for the subject which match the query SessionImagingModalities (imaging sessions only) imaging modalities acquired in the session SessionCompletedPipelines (imaging sessions only) processing pipelines completed for the session DatasetImagingModalities (dataset-level) imaging modalities acquired in at least one session in the dataset DatasetPipelines (dataset-level) processing pipelines completed for at least one session in the dataset AccessLink Primary link for access requests or information"},{"location":"user_guide/query_tool/#harmonized-tsv-data-with-uris","title":"Harmonized TSV data with URIs","text":"<p>A machine-optimized version of the query results, containing URIs instead of descriptive labels for harmonized attributes and metadata of matching subjects, is also available for download as a TSV.</p> <p>Each row corresponds to a single matching subject session, except for datasets configured to only return aggregate results.</p> Example query result TSV DatasetName RepositoryURL NumMatchingSubjects SubjectID SessionID ImagingSessionPath SessionType NumMatchingPhenotypicSessions NumMatchingImagingSessions Age Sex Diagnosis Assessment SessionImagingModalities SessionCompletedPipelines DatasetImagingModalities DatasetPipelines AccessLink Balloon Analog Risk-taking Task https://github.com/OpenNeuroDatasets-JSONLD/ds000001.git 16 protected protected protected protected protected protected protected protected protected protected protected protected http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#T2Weighted nan nan Classification learning https://github.com/OpenNeuroDatasets-JSONLD/ds000002.git 17 protected protected protected protected protected protected protected protected protected protected protected protected http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#T2Weighted nan nan BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-01 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-01/ses-01 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-01 ses-01 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 34.1 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C94342 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-01 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-01/ses-02 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-01 ses-02 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 35.3 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C94342 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-02 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-02/ses-01 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-02 ses-01 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 nan http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/406506008 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-02 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-02/ses-02 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-02 ses-02 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 39 http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/406506008 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-03 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-03/ses-01 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-03 ses-01 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 22.1 nan nan http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-03 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-03/ses-02 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-03 ses-02 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 23.2 nan http://purl.bioontology.org/ontology/SNOMEDCT/406506008 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-04 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-04/ses-01 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-04 ses-01 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 21.1 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C94342 http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-04 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-04/ses-02 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-04 ses-02 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 22.3 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C94342 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-05 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-05/ses-01 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-05 ses-01 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 42.5 http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/406506008 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-05 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-05/ses-02 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git BIDS synthetic https://github.com/bids-standard/bids-examples.git 5 sub-05 ses-02 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 43.2 http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/406506008 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 https://github.com/bids-standard/bids-examples.git <p>This file contains the same columns and data as the descriptive query results TSV. However, the harmonized terms in the following columns are provided in their raw URI form instead of as descriptive labels:</p> Column name SessionType Sex Diagnosis Assessment SessionImagingModalities SessionCompletedPipelines DatasetImagingModalities DatasetPipelines"},{"location":"user_guide/query_tool/#protected-subject-level-results-for-aggregate-datasets","title":"<code>protected</code> subject-level results for aggregate datasets","text":"<p>Example</p> <p>For examples of aggregated matching dataset results, see the last rows of the example query result TSV in the previous two sections.</p> <p>A row in a query result TSV may show <code>protected</code> for all columns except for <code>DatasetName</code>, <code>RepositoryURL</code>, <code>AccessLink</code> and other dataset-level columns. This means the source graph database (node) has been configured (via its corresponding Neurobagel node API) to return only aggregate information about matching subjects e.g., for data privacy reasons.</p> <p>More information on this configuration setting, called <code>NB_RETURN_AGG</code>, and how to change it for a node can be found here.</p>"},{"location":"user_guide/query_tool/#testing","title":"Testing","text":"<p>The query tool utilizes Cypress framework for testing.</p> <p>To run the tests execute the following command:</p> <pre><code>npx cypress open\n</code></pre>"},{"location":"user_guide/query_tool/#license","title":"License","text":"<p>The query tool is released under the terms of the MIT License</p>"}]}