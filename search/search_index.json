{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"cite/","title":"Citing Neurobagel","text":"<p>If you use Neurobagel in your research, we recommend citing the Zenodo DOI associated with the Neurobagel tool and version you used.</p> Tool Zenodo reference Annotation tool CLI API Query tool <p>Note</p> <p>If you used the query tool, we recommend also citing the API.</p>"},{"location":"getting_help/","title":"Getting Help","text":"<p>There are several ways you can interact with the Neurobagel team:</p> <ul> <li>Create an issue in our GitHub repositories</li> <li>Our tag on the Neurostars forum</li> <li>Join our Discord server to chat with us</li> </ul>"},{"location":"getting_help/#general-usage-questions","title":"General usage questions","text":"<p>If you have a usage question about Neurobagel (feedback on a specific use case), or want to discuss the Neurobagel project in general with the development team or other users, please either:</p> <ul> <li>search for or start a new thread on Neurostars with the <code>neurobagel</code> tag: https://neurostars.org/tag/neurobagel or</li> <li>send us a message in the Neurobagel Discord server</li> </ul>"},{"location":"getting_help/#problem-with-deploying-a-neurobagel-node","title":"Problem with deploying a Neurobagel node","text":"<p>If you encounter a problem while deploying a Neurobagel node, first make sure you are following our getting started guide correctly. If your problem persists, please open a node deployment issue on GitHub.</p>"},{"location":"getting_help/#report-a-bug-or-request-a-feature","title":"Report a bug or request a feature","text":"<p>For bug reports or feature requests, we prefer if you open an issue in the repository for the relevant tool in the Neurobagel GitHub organization.</p> <p>If you are using one of our web tools (https://annotate.neurobagel.org or https://query.neurobagel.org), you may also submit bug reports and feature requests using the feedback widget directly on the app.</p>"},{"location":"getting_help/#documentation-feedback","title":"Documentation feedback","text":"<p>We are always looking to improve our user documentation. To request a change or addition to existing documentation, please open an issue in the <code>neurobagel/documentation</code> repo.</p>"},{"location":"getting_help/#propose-a-new-variable-to-be-added-to-the-neurobagel-data-model","title":"Propose a new variable to be added to the Neurobagel data model","text":"<p>If there is a variable you'd like to be able to harmonize and query using Neurobagel that is not currently supported, please suggest it using a discussion in our dedicated GitHub Discussion category.</p>"},{"location":"glossary/","title":"Glossary","text":"<p>This glossary compiles some key terms used in the Neurobagel documentation and defines them in the context of the Neurobagel ecosystem.</p>"},{"location":"glossary/#data-dictionary","title":"Data dictionary","text":"A JSON file that describes the information contained in columns from a tabular data file,  along with the meaning and properties (format of numerical data, unique \u201clevels\u201d  of categorical data, etc.) of values in each column. In the context of Neurobagel,  the meanings of columns and column values are encoded using terms from standardised vocabularies."},{"location":"glossary/#data-model","title":"Data model","text":"Used interchangeably with: data schema <p>A structure that has been designed with the  purpose to represent a specific kind of information.  A data model is made up of generic types or classes that are relevant to the data model designers (for Neurobagel, examples include \"Research Participant\" and \"Neuroimaging Dataset\"), the properties these types can have (e.g., \"Age in years\", \"Dataset name\"), and the  relationships that can exist between them (e.g., \"is part of\"). The goal of a data model is to give information a structure so that we can write programs that can consume the information.</p> <p>The Neurobagel data model is designed to represent the kind of information that is important to support the most relevant cohort definition queries, and thus models types, properties, and relationships that are important for this purpose. It is not a static thing, and we constantly add new things to  the data model as we support new use cases that rely on this information.</p>"},{"location":"glossary/#controlled-term","title":"Controlled term","text":"<p>A unique identifier or code for a concept that is described in a controlled vocabulary.</p> <p>A controlled term has a </p> <ul> <li>a clear definition</li> <li>a unique and persistent identifier</li> <li>from a specific curated list of terms like a vocabulary, taxonomy or ontology</li> </ul> <p>An example is the controlled term for  \"Parkinson's disease\" from the ICD-11 taxonomy with the unique code <code>8A00.0</code>.</p>"},{"location":"glossary/#controlled-vocabulary","title":"Controlled vocabulary","text":"Used interchangeably with: taxonomy, and ontology <p>A controlled vocabulary is a collection of controlled terms that are often all about one specific topic. The main benefit of a  controlled vocabulary is that it provides unambiguous terms with clear definitions that people have agreed to use to describe their information - removing the need to align variable names and value formats between datasets and enabling interoperability. </p> <p>For example, most websites use the schema.org vocabulary to describe things like  products to purchase,  events to book,  recipes to cook etc. in a consistent way that can be understood by  the search spiders of big search engines.</p> Reusing controlled vocabularies <p>Creating a controlled vocabulary is a laborious task  that involves deep subject matter expertise, often from many experts,  and needs to be maintained to remain relevant. You should therefore almost always reuse an existing vocabulary rather than creating your own. </p> <p>A taxonomy is a more specific form of a controlled vocabulary  that organizes terms into hierarchical relationships. For example, a \"Recipe\" in schema.org is a subtype of a \"HowTo\" which itself is a subtype of a \"CreativeWork\". This hierarchy  let's you do things like search for \"CreativeWork\" and also find \"Recipe\", even if you have never made this link directly.</p> <p>An ontology is an even more specific form of a taxonomy  where terms can have very complex relationships with each other that include logical constraints. In an ontology, you could for example express that for someone to be a \"sister\" to someone else,  both the subject and the object of the relationship have to be \"human\", only the subject of the relation has to be \"female\", and both have to  have at least one parent in common. These complex expressions are very labour intensive to create but can provide also very  rich ways of validating and even inferring information.</p>"},{"location":"glossary/#graph-database","title":"Graph database","text":"Used interchangeably with: knowledge graph store, graph store, graph <p>A type of database, in the same way that a relational databases is a type of database. The main distinguishing feature of graph databases is that they  represent entities as nodes in a graph,  and relationships between entities as edges between these nodes. This data model makes it easy to easily add new information by drawing a new edge between two nodes.</p> Note <p>A single Neurobagel graph database can contain harmonised information about multiple datasets and their respective subjects. Each subject is represented by a node, and their harmonised phenotypic and imaging data characteristics are described using controlled terms connected to the subject node via a series of edges that individually encode the type of attribute described by the controlled term.</p> <p>Neurobagel uses the RDF graph data model, see also https://en.wikipedia.org/wiki/Graph_database.</p>"},{"location":"glossary/#annotation","title":"Annotation","text":"In the context of Neurobagel, annotation refers to the process of describing tabular demographic, cognitive, and/or clinical (phenotypic) data for a dataset with terms from controlled vocabularies to create machine  understandable data dictionaries for the data. You can learn more about this process in our documentation."},{"location":"glossary/#aggregated-results","title":"Aggregated results","text":"If the owner of a Neurobagel node decides that query responses should not include information at the level of individual  participants, they can configure their node to only return aggregated results. In this mode, the node will aggregate all participants that match a query at the dataset level and only respond with counts of matching participants."},{"location":"glossary/#data-owner","title":"Data owner","text":"A person or an institute who is responsible in the data governance sense  for one or many datasets. In the context of Neurobagel, one data owner can have one or more Neurobagel nodes, but every Neurobagel node can only have one data owner who is responsible for all of the data stored inside the node."},{"location":"glossary/#federation-api","title":"Federation API","text":"Used interchangeably with: f-API <p>A standalone service that allows query users to send a single query and have it automatically sent to many Neurobagel node APIs (n-API) without having to know where these node APIs are located. The f-API takes care of keeping an up to date list of available  n-APIs, federating queries, retrieving and combining results,  and returning them to the user.</p> <p>Designed to very closely resemble the behaviour and the endpoints of a n-API so that services can be built that are able to work either directly with a single n-API or with an f-API.</p>"},{"location":"glossary/#node-api","title":"Node API","text":"Used interchangeably with: n-API <p>A Neurobagel \"node\" is a locally deployed service that holds information about data for one data owner who controls and manages the node. A node has two core components:</p> <ul> <li>a graph backend to store the harmonised data for querying</li> <li>a RESTful node API that exposes query endpoints for users or programs to send queries and retrieve results</li> </ul> <p>One important purpose of the n-API is to act as a barrier between the user and the graph backend so that the user cannot execute arbitrary queries on the graph, and the data owner can control how detailed the query responses should be.</p>"},{"location":"glossary/#tabular-data","title":"Tabular data","text":"Used interchangeably with: phenotypic data <p>Tabular text files (e.g., .tsv or .csv) that contain information about participants such as their demographic information or data from cognitive or clinical assessments they have completed.  We often refer to this information as phenotypic data because they describe observable characteristics of the participant.</p>"},{"location":"glossary/#tsv","title":"TSV","text":"<p>A Tab-Separated Values (TSV) file has the <code>.tsv</code> extension and is a plain text file structured as a table, where values belonging to different columns are separated by a single tab character (<code>\\t</code>).  Each column represents a field of interest, and each row represents a single datapoint.  The first line of the file (the header) contains the names of each column. </p> <p>A valid TSV should also follow some common formatting guidelines:</p> <ul> <li>Each line must contain the same number of tab-separated fields (columns), even if some are empty</li> <li>Column names must be unique (no duplicates)</li> <li>Do not include completely blank rows or columns</li> <li>Avoid leading or trailing spaces in column names and values</li> </ul> <p>Most spreadsheet software (e.g., Microsoft Excel, Google Sheets) will allow you to save a file as TSV (this webpage has guides on creating TSVs in common applications),  but it is your responsibility to ensure that the file is well-structured according to TSV formatting conventions.</p>"},{"location":"contributing/CONTRIBUTING/","title":"Contributing Guidelines","text":"<p>We're so excited that you're interested in contributing to Neurobagel! </p> <p>We appreciate all contributions, and hope that the below guidelines will make it as easy as possible for you to contribute to the Neurobagel codebase and to ensure your contribution can be easily integrated.</p>"},{"location":"contributing/CONTRIBUTING/#contributing-through-github","title":"Contributing through GitHub","text":"<p>In order to contribute to Neurobagel, you'll need to set up a GitHub account and sign in. Here are some instructions to help you get started.</p>"},{"location":"contributing/CONTRIBUTING/#identifying-an-issue-to-contribute-to","title":"Identifying an issue to contribute to","text":"<p>The best way to get started contributing is to explore the list of open issues in one of our GitHub repositories.  (For example, see the open issues for the Neurobagel API.)</p> <p>When you are ready to contribute, we welcome you to join the conversation through one of these issues,  or open a new issue referencing a change you would like to see or contribute. Ensuring that a relevant issue is open before you start contributing code is important because it allows others in the project to discuss your idea and tell you where your contribution would be the most helpful.</p> <ul> <li> <p>If the issue you want to work on already exists:  Comment on the open issue to indicate you would like to work on it, along with any clarification/implementation questions you have</p> <ul> <li>If someone is already assigned to the issue, the task is actively being worked on and a solution will soon be proposed. Feel free to share some helpful resources or pointers that may be interesting to the person who is working the issue, and/or check back in a couple of days.</li> </ul> </li> <li> <p>If the issue you want to work on does not exist:  Open a new issue describing your proposed change and why it is necessary/beneficial.  The more detail here, the better!</p> </li> </ul> <p>This allows members of the Neurobagel developer team to confirm that you will not be overlapping with currently active work and that everyone is on the same page about the task to be accomplished.</p> <p>If you would like to contribute but are not sure where to start, we recommend looking for open issues with the following labels:</p> <p> Issue that is good for a new or beginner contributor, as it does not involve a steep learning curve or advanced understanding of the codebase. (Please note: if you're a seasoned contributor, we would appreciate if you could select a different issue to work on to keep these available for less experienced folks!)</p> <p> Issue that is not an internal priority, but external pull requests to address it are welcome.</p> <p>:  Issue that should involve minimal planning or implementation work, given an understanding of the relevant code.</p>"},{"location":"contributing/CONTRIBUTING/#making-a-change","title":"Making a change","text":"<p>All Neurobagel issues are expected to be addressed through pull requests.</p> <p>As an external contributor, the process you would follow to make your proposed changes should look something like this:</p>"},{"location":"contributing/CONTRIBUTING/#1-fork-the-relevant-neurobagel-repository-to-your-profile","title":"1. Fork the relevant Neurobagel repository to your profile.","text":""},{"location":"contributing/CONTRIBUTING/#2-clone-your-fork-of-the-neurobagel-repository-to-your-local-machine","title":"2. Clone your fork of the Neurobagel repository to your local machine.","text":"<p>To keep up with changes in the Neurobagel repository while you work and avoid merge conflicts later on, make sure to:</p> <ul> <li>Add the \"upstream\" Neurobagel repository as a remote to your locally cloned repository</li> <li>Keep your fork up to date with the upstream repository</li> </ul>"},{"location":"contributing/CONTRIBUTING/#3-set-up-a-development-environment","title":"3. Set up a development environment.","text":"<p>Refer to the README of the Neurobagel repository you are contributing to for instructions on setting up a development environment so you can test any local code changes you make. Note that the steps to set up a development environment (usually under a \"Local installation\", \"Manual installation\", or \"Development environment\" section) are generally different than those used to install the tool purely as a user, which usually appears at the top of the README.</p> <p>For example, for the Neurobagel CLI:</p> <ul> <li>\"Development\" mode installation steps</li> <li>Normal (non-development mode) installation steps</li> </ul>"},{"location":"contributing/CONTRIBUTING/#follow-repository-code-style","title":"Follow repository code style","text":"<p>Most Neurobagel repositories use tools to apply automatic code formatting and linting according to the project's code style,  which are typically configured (but not automatically enabled) in the development environment for these repositories.  At this point, please follow the repository instructions to set up these tools before you begin your work to ensure your contribution matches the existing code.</p> <p>For example, our repositories written in Python have pre-commit configured for this purpose.</p> <p>To tell pre-commit to run on any local changes you make, run the following from the repository root of your local clone: <pre><code>pre-commit install\n</code></pre></p> <p>Now, a number of code linters and formatters will run automatically when you attempt to make a commit, which will keep your changes consistent with the rest of the codebase.</p>"},{"location":"contributing/CONTRIBUTING/#4-create-a-new-branch-to-make-the-proposed-changes","title":"4. Create a new branch to make the proposed changes","text":"<p>Please consider using descriptive branch names. Some examples:</p> <ul> <li><code>&lt;username&gt;/&lt;issue-identifier&gt;</code> (<code>jsmith/fix-1234</code>)</li> <li><code>&lt;username&gt;/&lt;brief-change-title&gt;</code> (<code>jsmith/add-logging</code>, <code>jsmith/enh/add-logging</code>)</li> </ul> <p>Once you are satisfied with your local changes, add, commit, and push them to your branch on your forked repository on GitHub. </p>"},{"location":"contributing/CONTRIBUTING/#5-open-a-pull-request","title":"5. Open a pull request","text":"<p>See the below sections for information on how to submit your pull request and what to expect in a pull request review.</p>"},{"location":"contributing/CONTRIBUTING/#pull-request-guidelines","title":"Pull request guidelines","text":"<p>When you first open a pull request, you should automatically see a template in the pull request body that looks something like this that you can fill out. The template is designed to make it easier for maintainers to review your pull request, but feel free to add any additional information that you feel is useful or necessary.</p> <p>Pull request titles should begin with a descriptive prefix (e.g., \"[ENH] Implement check for presence of a session ID column\"):</p> <ul> <li><code>[ENH]:</code> Feature improvements or additions</li> <li><code>[REF]:</code> Refactoring existing code</li> <li><code>[TST]</code>: Updating or adding a test</li> <li><code>[CI]</code>: Automation-related changes</li> <li><code>[MNT]</code>: General maintenance not covered by <code>[REF]</code>, <code>[TST]</code>, or <code>[CI]</code></li> <li><code>[INF]</code>: Software or graph infrastructure-related changes</li> <li><code>[FIX]</code>: Bug fixes</li> <li><code>[MODEL]</code>: Updates or changes related to the Neurobagel data model</li> <li><code>[DOC]</code>: Documentation-only changes to a code repo (READMEs, within-code documentation, etc.) <ul> <li>Exception: changes to the <code>documentation</code> repo should use one of the below PR prefixes instead of <code>[DOC]</code></li> </ul> </li> </ul>"},{"location":"contributing/CONTRIBUTING/#pull-requests-to-the-documentation-repo","title":"Pull requests to the <code>documentation</code> repo","text":"<p>In PRs to the Neurobagel documentation, using the <code>[DOC]</code> title prefix is discouraged as it is too broad.  Instead, for documentation content changes, the following prefixes can be used to specify the nature of the change:</p> <ul> <li><code>[ENH]</code>: Updating or adding new documentation</li> <li><code>[REF]</code>: Simplifying or restructuring documentation (i.e., pages, sections, paragraphs)</li> <li><code>[FIX]</code>: Fixing errors in the documentation</li> </ul>"},{"location":"contributing/CONTRIBUTING/#pull-request-reviews","title":"Pull request reviews","text":"<p>A maintainer will review each PR and may provide comments or suggestions for you to address.</p> <p>Neurobagel PR reviews may use the following emoji signifiers:</p> <p>: Ready to merge or approved without suggestions</p> <p>: Some optional/suggested changes that could be nice to have but are not required to merge</p> <p>If (required) changes are requested, please re-request a review from the reviewer once the comments have been addressed.</p>"},{"location":"contributing/CONTRIBUTING/#when-your-pull-request-is-approved","title":"When your pull request is approved","text":"<p>If you do not have write access to the repository: the reviewing Neurobagel maintainer is responsible for merging the PR.</p> <p>If you have write access to the repository: the PR author is responsible for merging the PR.</p>"},{"location":"contributing/CONTRIBUTING/#have-a-question-about-contributing","title":"Have a question about contributing?","text":"<p>At any point during a contribution,  please do not hesitate to mention one of the core maintainers if you have a question or need further guidance,  in either the issue or pull request.</p> <p>If you have ideas for improving this page, please help us improve it by opening an issue .</p>"},{"location":"contributing/team/","title":"Our team","text":"<p>Neurobagel is a project originating from the ORIGAMI Lab at the Montreal Neurological Institute in collaboration with the Douglas Research Centre.</p>"},{"location":"contributing/team/#developers","title":"Developers","text":"<p>\ud83e\uddd1\u200d\ud83c\udf73 Core maintainer \ud83e\uddd1\u200d\ud83d\udd2c Principal Investigator</p>"},{"location":"contributing/team/#current","title":"Current","text":"<sub>Sebastian Urchs</sub>\ud83e\uddd1\u200d\ud83c\udf73 <sub>Arman Jahanpour</sub>\ud83e\uddd1\u200d\ud83c\udf73 <sub>Alyssa Dai</sub>\ud83e\uddd1\u200d\ud83c\udf73 <sub>Nikhil Bhagwat</sub> <sub>Michelle Wang</sub> <sub>Brent McPherson</sub> <sub>Remi Gau</sub> <sub>Jean-Baptise Poline</sub>\ud83e\uddd1\u200d\ud83d\udd2c"},{"location":"contributing/team/#past","title":"Past","text":"<sub>Jonathan Armoza</sub>\ud83e\uddd1\u200d\ud83c\udf73"},{"location":"data_models/dictionaries/","title":"Neurobagel data dictionaries","text":""},{"location":"data_models/dictionaries/#overview","title":"Overview","text":"<p>When you annotate a phenotypic TSV using the Neurobagel annotation tool  (see also the section on the annotation tool), your annotations are automatically stored in a JSON data dictionary. A Neurobagel data dictionary essentially describes the meaning and properties of columns and column values using standardized vocabularies.</p> <p>Example</p> <p>A comprehensive example data dictionary containing all currently supported phenotypic attributes and annotations can be found here (corresponding phenotypic .tsv).</p> <p>Importantly, Neurobagel uses a structure for these data dictionaries that is compatible  with and expands on  BIDS <code>participant.json</code> data dictionaries. </p> <p>Info</p> <p>The specification for how a Neurobagel data dictionary is structured is also called a schema.  Because Neurobagel data dictionaries are stored as <code>.json</code> files, we use the <code>jsonschema</code> schema language  to write the specification.</p> <p>Neurobagel data dictionaries uniquely include an <code>Annotations</code> attribute  for each column entry to store user-provided semantic annotations.</p> <p>Here is an example BIDS data dictionary (<code>participants.json</code>):</p> <pre><code>{\n  \"age\": {\n    \"Description\": \"age of the participant\",\n    \"Units\": \"years\"\n  },\n  \"sex\": {\n    \"Description\": \"sex of the participant as reported by the participant\",\n    \"Levels\": {\n      \"M\": \"male\",\n      \"F\": \"female\"\n    }\n  }\n}\n</code></pre> <p>And here is the same data dictionary augmented with Neurobagel annotations:</p> <pre><code>{\n  \"age\": {\n    \"Description\": \"age of the participant\",\n    \"Units\": \"years\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Age\",\n        \"Label\": \"Age\"\n      },\n      \"Format\": {\n        \"TermURL\": \"nb:FromFloat\",\n        \"Label\": \"Float\"\n      },\n      \"VariableType\": \"Continuous\"\n    }\n  },\n  \"sex\": {\n    \"Description\": \"sex of the participant as reported by the participant\",\n    \"Levels\": {\n      \"M\": \"male\",\n      \"F\": \"female\"\n    },\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Sex\",\n        \"Label\": \"Sex\"\n      },\n      \"Levels\": {\n        \"M\": {\n          \"TermURL\": \"snomed:248153007\",\n          \"Label\": \"Male\"\n        },\n        \"F\": {\n          \"TermURL\": \"snomed:248152002\",\n          \"Label\": \"Female\"\n        }\n      },\n      \"MissingValues\": [\n        \"\",\n        \" \"\n      ],\n      \"VariableType\": \"Categorical\"\n    }\n  }\n}\n</code></pre> <p>Info</p> <p><code>TermURL</code> values in Neurobagel data dictionaries are compact URIs.</p> <p>A custom Neurobagel namespace, defined by the prefix <code>nb</code> (full URI: <code>http://neurobagel.org/vocab/</code>), is used for controlled terms that represent attribute classes modelled by Neurobagel, such as <code>\"Age\"</code> and <code>\"Sex\"</code>, even though these terms may have equivalents in other vocabularies used for annotation. </p> <p>For example, the following terms from the Neurobagel annotations above are conceptually equivalent to terms from the SNOMED CT namespace:</p> Neurobagel namespace term Equivalent external controlled vocabulary term http://neurobagel.org/vocab/Age http://purl.bioontology.org/ontology/SNOMEDCT/397669002 http://neurobagel.org/vocab/Sex http://purl.bioontology.org/ontology/SNOMEDCT/184100006"},{"location":"data_models/dictionaries/#phenotypic-attributes","title":"Phenotypic attributes","text":"<p>The Neurobagel annotation tool generates a data dictionary entry for a given column  by augmenting the information recommended by BIDS with unambiguous semantic tags.</p> <p>Below we'll outline several example annotations using the following example <code>participants.tsv</code> file:</p> participant_id session_id group age sex updrs_1 updrs_2 sub-01 ses-01 PAT 25 M 2 sub-01 ses-02 PAT 26 M 3 5 sub-02 ses-01 CTL 28 F 1 1 sub-02 ses-02 CTL 29 F 1 1 <p>Controlled terms in the below examples are shortened using the RDF prefix/context syntax for json-ld:</p> <pre><code>{\n  \"@context\": {\n    \"nb\": \"http://neurobagel.org/vocab/\",\n    \"ncit\": \"http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#\",\n    \"nidm\": \"http://purl.org/nidash/nidm#\",\n    \"snomed\": \"http://purl.bioontology.org/ontology/SNOMEDCT/\"\n  }\n}\n</code></pre>"},{"location":"data_models/dictionaries/#participant-identifier","title":"Participant identifier","text":"<p>Term from the Neurobagel vocabulary.</p> <pre><code>{\n  \"participant_id\": {\n    \"Description\": \"A participant ID\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:ParticipantID\",\n        \"Label\": \"Subject Unique Identifier\"\n      },\n      \"VariableType\": \"Identifier\"\n    }\n  }\n}\n</code></pre> <p>Info</p> <p><code>participant_id</code> is a reserved name in BIDS and BIDS data dictionaries therefore typically don't annotate this column.  Neurobagel supports tables containing multiple subject ID columns for studies that employ more than one ID scheme.</p>"},{"location":"data_models/dictionaries/#session-identifier","title":"Session identifier","text":"<p>Term from the Neurobagel vocabulary.</p> <pre><code>{\n  \"session_id\": {\n    \"Description\": \"A session ID\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:SessionID\",\n        \"Label\": \"Run Identifier\"\n      },\n      \"VariableType\": \"Identifier\"\n    }\n  }\n}\n</code></pre> <p>Info</p> <p>Unlike the BIDS specification, Neurobagel supports a <code>participants.tsv</code> file with a <code>session_id</code> field.</p>"},{"location":"data_models/dictionaries/#diagnosis","title":"Diagnosis","text":"<p>Terms for clinical diagnosis are from the SNOMED-CT ontology. Terms for healthy control status are from the National Cancer Institute Thesaurus.</p> <pre><code>{\n  \"group\": {\n    \"Description\": \"Group variable\",\n    \"Levels\": {\n      \"PD\": \"Parkinson's patient\",\n      \"CTRL\": \"Control subject\",\n    },\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Diagnosis\",\n        \"Label\": \"Diagnosis\"\n      },\n      \"Levels\": {\n        \"PD\": {\n          \"TermURL\": \"snomed:49049000\",\n          \"Label\": \"Parkinson's disease\"\n        },\n        \"CTRL\": {\n          \"TermURL\": \"ncit:C94342\",\n          \"Label\": \"Healthy Control\"\n        }\n      },\n      \"VariableType\": \"Categorical\"\n    }\n  }\n}\n</code></pre> <p>The <code>IsAbout</code> relation uses a term from the Neurobagel namespace because <code>\"Diagnosis\"</code> is a standardized term.</p> <p>Info</p> <p>Columns with categorical values (e.g., study groups, diagnoses, sex) require a <code>Levels</code> key in their Neurobagel annotation.  The Neurobagel \"Levels\" key is modeled after the BIDS \"Levels\" key for human readable descriptions.</p>"},{"location":"data_models/dictionaries/#sex","title":"Sex","text":"<p>Terms are from the SNOMED-CT ontology, which has controlled terms aligning with BIDS <code>participants.tsv</code> descriptions for sex.  Below are the SNOMED terms for the sex values allowed by BIDS: </p> Sex Controlled term Male http://purl.bioontology.org/ontology/SNOMEDCT/248153007 Female http://purl.bioontology.org/ontology/SNOMEDCT/248152002 Other http://purl.bioontology.org/ontology/SNOMEDCT/32570681000036106 <p>Here is what a sex annotation looks like in practice:</p> <pre><code>{\n  \"sex\": {\n    \"Description\": \"Sex variable\",\n    \"Levels\": {\n      \"M\": \"Male\",\n      \"F\": \"Female\"\n    },\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Sex\",\n        \"Label\": \"Sex\"\n      },\n      \"Levels\": {\n        \"M\": {\n          \"TermURL\": \"snomed:248153007\",\n          \"Label\": \"Male\"\n        },\n        \"F\": {\n          \"TermURL\": \"snomed:248152002\",\n          \"Label\": \"Female\"\n        }\n      },\n      \"VariableType\": \"Categorical\"\n    }\n  }\n}\n</code></pre> <p>The <code>IsAbout</code> relation uses a Neurobagel scoped term for <code>\"Sex\"</code> because  this is a Neurobagel common data element.</p>"},{"location":"data_models/dictionaries/#age","title":"Age","text":"<p>Neurobagel has a common data element for <code>\"Age\"</code> describing a continuous column.  To ensure age values are represented as floats in Neurobagel graphs,  Neurobagel encodes the format of the raw numerical values in a given age column.  This is stored in the <code>Format</code> annotation (required for continuous columns) and maps internally to a specific transformation that is then used to convert the raw values to floats.</p> <p>Possible formats: </p> TermURL Label Examples <code>nb:FromFloat</code> float value <code>31.5</code>, <code>31</code> <code>nb:FromEuro</code> european decimal value <code>31,5</code> <code>nb:FromBounded</code> bounded value <code>30+</code> <code>nb:FromRange</code> a range between a minimum and maximum value <code>30-35</code> <code>nb:FromISO8061</code> period of time defined according to the ISO8601 standard <code>31Y6M</code> <pre><code>{\n  \"age\": {\n    \"Description\": \"Participant age\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Age\",\n        \"Label\": \"Chronological age\"\n      },\n      \"Format\": {\n        \"TermURL\": \"nb:FromEuro\",\n        \"Label\": \"European value decimals\"\n      },\n      \"VariableType\": \"Continuous\"\n    }\n  }\n}\n</code></pre>"},{"location":"data_models/dictionaries/#assessment-tool","title":"Assessment tool","text":"<p>Terms are from the SNOMED-CT ontology.</p> <p>For assessment tools like cognitive tests or rating scales,  Neurobagel encodes whether a subject has a value/score for at least one item or subscale of the assessment. Because assessment tools often have several subscales or items  that can be stored as separate columns in the tabular <code>participant.tsv</code> file, each assessment tool column receives a minimum of two annotations:</p> <ul> <li>one to classify that the column <code>IsAbout</code> the generic category of assessment tools</li> <li>one to classify that the column <code>IsPartOf</code> the specific assessment tool</li> </ul> <p>An optional additional annotation <code>MissingValues</code> can be used to specify value(s)  in an assessment tool column which represent that the participant is missing a value/response for that subscale, when instances of missing values are present (see also section Missing values).</p> <pre><code>{\n  \"updrs_1\": {\n    \"Description\": \"item 1 scores for UPDRS\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Assessment\",\n        \"Label\": \"Assessment tool\"\n      },\n      \"IsPartOf\": {\n        \"TermURL\": \"snomed:342061000000106\",\n        \"Label\": \"Unified Parkinsons disease rating scale score\"\n      },\n      \"VariableType\": \"Collection\"\n    }\n  },\n  \"updrs_2\": {\n    \"Description\": \"item 2 scores for UPDRS\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Assessment\",\n        \"Label\": \"Assessment tool\"\n      },\n      \"IsPartOf\": {\n        \"TermURL\": \"snomed:342061000000106\",\n        \"Label\": \"Unified Parkinsons disease rating scale score\"\n      },\n      \"MissingValues\": [\"\"],\n      \"VariableType\": \"Collection\"\n    }\n  }\n}\n</code></pre> <p>To determine whether a specific assessment tool is available for a given participant, we then consider all of the columns that were classified as <code>IsPartOf</code> that specific tool and then apply a simple <code>any()</code> heuristic to check that at least one column does not contain any <code>MissingValues</code>.</p> <p>For the above example, this would be:</p> particpant_id updrs_1 updrs_2 sub-01 2 sub-02 1 1 sub-03 <p>Therefore: </p> particpant_id updrs_available sub-01 True sub-02 True sub-03 False"},{"location":"data_models/dictionaries/#missing-values","title":"Missing values","text":"<p>Missing values are allowed for any phenotypic variable (column) that does not describe a participant or session identifier (e.g., columns like <code>participant_id</code> or <code>session_id</code>).  In a Neurobagel data dictionary, missing values for a given column are listed under the <code>\"MissingValues\"</code> annotation for the column (see the Assessment tool section or the comprehensive example data dictionary for examples).</p>"},{"location":"data_models/graph_data/","title":"Neurobagel graph data files","text":""},{"location":"data_models/graph_data/#overview","title":"Overview","text":"<p>Using the Neurobagel CLI (see also the section on the CLI),  a Neurobagel data dictionary (<code>.json</code>) for a dataset can be processed together with the corresponding tabular data (<code>.tsv</code>) and BIDS dataset (if available) to generate subject-level linked data that can be encoded in a knowledge graph.  The Neurobagel graph-ready data are stored in JSON-LD format (<code>.jsonld</code>),  and include a representation of each subject's harmonized phenotypic properties and imaging metadata.</p> <p>Another way to think about the difference between a Neurobagel data dictionary and a graph-ready <code>.jsonld</code> data file is this:  more than one dataset can theoretically have the same data dictionary (if the tabular data include the same columns and unique column values),  but the <code>.jsonld</code> file for each dataset is unique as long as the actual data of subjects differs across datasets.</p>"},{"location":"data_models/graph_data/#example-jsonld-files","title":"Example <code>.jsonld</code> files","text":"<p>Depending on whether a dataset annotated using Neurobagel includes BIDS imaging data,  the <code>.jsonld</code> data for the dataset may or may not include imaging metadata of subjects (extracted automatically with the CLI).</p> <ul> <li>Example <code>.jsonld</code> containing only phenotypic data</li> <li>Example <code>.jsonld</code> containing phenotypic and raw imaging (BIDS) data</li> <li>Example <code>.jsonld</code> containing phenotypic and imaging derivative data</li> <li>Example <code>.jsonld</code> containing phenotypic, raw imaging (BIDS), and imaging derivative data</li> </ul> More info on example dataset <p>The above <code>.jsonld</code> files represent an example dataset used for testing which includes the following:</p> Data Link Phenotypic TSV Neurobagel data dictionary BIDS dataset"},{"location":"data_models/term_naming_standards/","title":"Neurobagel standards for controlled term naming","text":""},{"location":"data_models/term_naming_standards/#naming-conventions","title":"Naming conventions","text":""},{"location":"data_models/term_naming_standards/#namespace-prefixes","title":"Namespace prefixes","text":"<ul> <li>Names should be all lowercase (e.g., <code>nidm</code>, <code>snomed</code>)</li> </ul>"},{"location":"data_models/term_naming_standards/#properties-graph-edges","title":"Properties (graph \"edges\")","text":"<ul> <li>Names should adhere to camelCase (uses capitalized words except for the first word/letter)</li> <li>Should be a compound of:<ul> <li>a verb relevant to the property (e.g., hasAge, isSubjectGroup)</li> <li>the range of the property, (e.g.,hasDiagnosis points to a Diagnosis object)</li> </ul> </li> </ul> <p>What this might look like in semantic triples: <pre><code>&lt;Subject&gt; &lt;nb:hasDiagnosis&gt; &lt;snomed:1234&gt;\n&lt;snomed:1234&gt; &lt;rdf:type&gt; &lt;nb:Diagnosis&gt;\n</code></pre></p>"},{"location":"data_models/term_naming_standards/#classes-or-resources-graph-nodes","title":"Classes or resources (graph \"nodes\")","text":"<ul> <li>Names should adhere to PascalCase (each word capitalized)</li> <li>Where possible, simplify to a single word (e.g., <code>Diagnosis</code>, <code>Dataset</code>, <code>Sex</code>)</li> </ul> <p>Note</p> <p>Generally, we own the terms for properties and classes (e.g., Diagnosis, Assessment) but not the resources representing instances of classes such as specific diagnosis, sex, or assessment values (these are reused from existing vocabularies).</p> <p>In cases where we reuse a term for a class that comes from an existing controlled vocabulary, and that vocabulary follows a different naming convention (e.g., all lowercase), we should follow the existing naming convention.</p>"},{"location":"data_models/term_naming_standards/#currently-used-namespaces","title":"Currently used namespaces","text":"Prefix IRI Types of terms <code>nb</code> http://neurobagel.org/vocab/ Neurobagel-\"owned\" properties and classes <code>snomed</code> http://purl.bioontology.org/ontology/SNOMEDCT/ diagnoses, sex, and assessments or instruments <code>ncit</code> http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl# group designation (e.g., healthy control) <code>nidm</code> http://purl.org/nidash/nidm# imaging modalities <code>np</code> https://github.com/nipoppy/pipeline-catalog/tree/main/processing/ processing pipeline and derivative metadata"},{"location":"data_models/term_naming_standards/#what-if-an-nb-term-already-exists-in-another-controlled-vocabulary","title":"What if an <code>nb</code> term already exists in another controlled vocabulary?","text":"<p>If there is an equivalent controlled term to one we are defining in a different namespace,  we document this and express their equivalence using <code>owl:sameAs</code>.</p> <p>Example: If our term is <code>nb:Subject</code> and <code>nidm:Subject</code> is conceptually equivalent: <pre><code>&lt;nb:12345&gt; a &lt;nb:Subject&gt;\n&lt;nb:Subject&gt; a &lt;rdfs:Resource&gt;;\n    &lt;owl:sameAs&gt; &lt;nidm:Subject&gt;\n</code></pre></p>"},{"location":"data_models/term_naming_standards/#other-general-guidelines","title":"Other general guidelines","text":"<ul> <li>Each property (edge) should use a single namespace for the resources it corresponds to</li> <li>Where possible, hardcode or refer to identifiers and not human-readable labels</li> </ul>"},{"location":"data_models/variables/","title":"Variables harmonized by Neurobagel","text":"<p>Neurobagel semantically harmonizes a number of variables used to describe different data and metadata for a study participant.</p> <p>The following list summarizes the variables that can currently be modeled and queried using Neurobagel, at the level of a single subject session.</p> <p>We are actively working to expand our subject data model, and welcome feedback on any other variables you would like to be able to query.</p>"},{"location":"data_models/variables/#phenotypic","title":"Phenotypic","text":"<p>For details on how phenotypic attributes are modeled, see the page on Neurobagel data dictionaries.</p> <ul> <li>Age</li> <li>Sex</li> <li>Diagnosis</li> <li>Availability of scores on a particular assessment</li> </ul>"},{"location":"data_models/variables/#imaging","title":"Imaging","text":""},{"location":"data_models/variables/#raw-data","title":"Raw data","text":"<ul> <li>Available MRI acquisition sequences, following the BIDS specification</li> </ul>"},{"location":"data_models/variables/#derived-data","title":"Derived data","text":"<ul> <li>Completed processing pipelines, following the Nipoppy specification</li> </ul>"},{"location":"user_guide/","title":"Ecosystem","text":"<p>The Neurobagel ecosystem comprises four primary tools:</p> <ul> <li>The annotation tool  (annotate.neurobagel.org)<ul> <li>to create harmonized annotations of phenotypic variables</li> <li>intended for use by researchers and domain experts</li> <li>static site, deployed on GitHub Pages</li> </ul> </li> <li>The command-line interface <ul> <li>to extract subject-specific metadata from annotated phenotypic and BIDS data</li> <li>intended for data managers to create graph-ready harmonized data</li> </ul> </li> <li>The knowledge graph store and Neurobagel API  (e.g., api.neurobagel.org)<ul> <li>to store and query extracted metadata using RDF and the SPARQL query language</li> <li>intended for research/data platform owners and for isolated deployments</li> </ul> </li> <li>The query tool  (query.neurobagel.org)<ul> <li>to search across datasets and obtain metadata for subjects based on harmonized subject-level attributes</li> <li>intended to help researchers and scientific data users find cohorts</li> <li>static site, deployed on GitHub Pages</li> </ul> </li> </ul> <p>You can also find official Docker images for our containerized tools on the Neurobagel Docker Hub profile.</p>"},{"location":"user_guide/#what-to-do-next","title":"What to do next","text":"<ul> <li>Learn how to run a cohort query on publicly accessible Neurobagel nodes</li> <li>Deploy your own Neurobagel node using our official Docker Compose recipe</li> <li>Prepare your own dataset for annotation and harmonization with Neurobagel</li> </ul>"},{"location":"user_guide/annotation_tool/","title":"The Neurobagel Annotation Tool","text":"<p>The Neurobagel annotation tool creates standardized, machine-readable data dictionaries for tabular data using curated FAIR vocabularies. The tool helps to harmonize tabular research data and is compatible with BIDS datasets.</p> <p>Workflow summary: </p> <ol> <li>Upload tabular data</li> <li>Column annotation</li> <li>Value annotation</li> <li>Download data dictionary</li> </ol>"},{"location":"user_guide/annotation_tool/#1-upload-tabular-data","title":"1. Upload tabular data","text":"<ul> <li>Upload your data table (.tsv file)<ul> <li>Can be <code>participants.tsv</code> from a BIDS dataset</li> </ul> </li> <li>Optional: Upload an existing data dictionary (.json file) for extra context<ul> <li>Can use <code>participants.json</code> from a BIDS dataset</li> <li>Or continue previous Neurobagel annotation work</li> </ul> </li> </ul> <p>In the following steps, you will annotate your table by first describing the columns and then the values within the columns.</p>"},{"location":"user_guide/annotation_tool/#2-column-annotation","title":"2. Column Annotation","text":"<p>Each column in your uploaded table is represented as a card on this page. For each column, you can:</p> <ul> <li>Add a description</li> <li>Select the standardized variable that best describes the column from the dropdown (if a suitable match exists)</li> <li>Select the data type <ul> <li>Choose \"Categorical\" if the column contains discrete values, \"Continuous\" if it contains numerical measurements, or leave it empty if neither applies</li> <li>Columns mapped to standardized variables will have their data type inferred automatically</li> </ul> </li> </ul> <p>When to manually select data type</p> <p>We recommend manually selecting the data type in two cases:</p> <ol> <li>When your column doesn't match any standardized variable</li> <li>When your column matches the \"Assessment tool\" standardized variable (which does not have a predefined data type since it can represent multi-column measures)</li> </ol>"},{"location":"user_guide/annotation_tool/#21-multi-column-measure-annotation","title":"2.1 Multi-column measure annotation","text":"<p>Info</p> <p>This step is only available if you have mapped columns in your data table to the \"Assessment tool\" standardized variable.</p> <p></p> <p>The card on the right lists all columns from your data table that you have mapped to the \"Assessment tool\" standardized variable.</p> <ol> <li>Create a card for each assessment or instrument represented in your data by clicking  and then selecting the name of the assessment from the dropdown list.<ul> <li>If no suitable match exists, the available standardized vocabulary likely cannot currently represent your assessment. </li> <li>To avoid incomplete annotations, un-map any column(s) corresponding to missing assessments from the \"Assessment tool\" standardized variable using the  button in the overview card.</li> </ul> </li> <li>Select the column(s) that describe each assessment, grouping together related columns as needed, using the dropdown on the respective assessment card.<ul> <li>You can check remaining, ungrouped columns in the overview on the right.</li> </ul> </li> </ol>"},{"location":"user_guide/annotation_tool/#3-value-annotation","title":"3. Value Annotation","text":"<p>The left sidebar displays the standardized variables that are represented in your tabular data, along with the column names that have been mapped to those variables.</p> <p>Click on a standardized variable (or data type, for unannotated columns) subheading in the sidebar to display the columns corresponding to that variable (or data type).  Then, in the column-level view on the right, navigate between the column tabs to annotate the values within each column.</p> Understanding sidebar sections <p>The sidebar organizes your columns by their annotation status: </p> <ul> <li>Annotated contains columns you have mapped to standardized variables</li> <li>Unannotated contains columns you have not mapped to a standardized variable<ul> <li>Within this section, unannotated columns are organized based on whether you have assigned them a data type</li> </ul> </li> </ul>"},{"location":"user_guide/annotation_tool/#columns-with-continuous-data","title":"Columns with continuous data","text":"<p>For a column containing continuous data, you can:</p> <ul> <li>Add a description of the units of measurement</li> <li>Select the format of the numerical values<sup>1</sup> </li> <li>Select \"Mark as missing\" for any values that represent missing, unavailable, or invalid data<sup>1</sup><ul> <li>Note: the column-level view will only display unique values in the column</li> </ul> </li> </ul> Units vs. Format <p>Format refers to how the numbers in your data are structured (e.g., <code>float</code> for decimal numbers like 25.5, <code>int</code> for whole numbers like 25) whereas Units describe what the numbers represent (e.g., \"years\" for age, \"points\" for test scores, \"mg/dL\" for measurements).</p>"},{"location":"user_guide/annotation_tool/#columns-with-categorical-data","title":"Columns with categorical data","text":"<p>For a column containing categorical data, you will be prompted to annotate the unique values detected in the column. This includes any values that are blank (empty strings) or contain only whitespace.</p> <p>For each unique column value, you can:</p> <ul> <li>Add a free-form description of the value</li> <li>Select a standardized term that best captures the meaning of the value<sup>1</sup></li> <li>Select \"Mark as missing\" if the value:<sup>1</sup><ul> <li>indicates missing, unavailable, or invalid data</li> <li>OR, does not have a suitable match among the standardized term options</li> </ul> </li> </ul> <p>Warning</p> <p>For the value annotation to be considered complete by Neurobagel, all unique values must either be mapped to a standardized term or marked as missing.</p>"},{"location":"user_guide/annotation_tool/#4-download-data-dictionary","title":"4. Download data dictionary","text":"<ul> <li>Preview your annotated data dictionary</li> <li>Download the data dictionary <code>.json</code> file</li> <li>Annotate a new dataset if desired</li> </ul> <p>Tip</p> <p>If you see a warning about \"Incomplete Annotations\", you will need to return to the Value Annotation page to complete any missing annotations before your data dictionary is valid for downstream Neurobagel tools.</p> <p>Your downloaded data dictionary is BIDS-compatible and, if you see the confirmation that you have successfully created a Neurobagel data dictionary, it is ready to be used to generate data for a Neurobagel graph database.</p> <ol> <li> <p>Attribute can only be annotated if the column has been mapped to a standardized variable.\u00a0\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"user_guide/api/","title":"The Neurobagel API","text":""},{"location":"user_guide/api/#introduction","title":"Introduction","text":"<p>Neurobagel has two flavours of APIs that can be deployed: node API and federation API. </p> <ul> <li>A Neurobagel node API (n-API) formulates SPARQL queries based on a set of user-defined parameters to a single connected graph database, and processes returned query results into a user-friendly format.</li> <li>A Neurobagel federation API (f-API) lets the user sends a single query to each node API it is aware of, and collects and combines the decentralized responses into a single set of query results.</li> </ul> <p>Neurobagel's query tool provides a GUI for querying one or more Neurobagel graphs by sending requests to a Neurobagel federation API instance.  However, HTTP requests can also be sent directly to any publicly accessible Neurobagel API (node or federation).</p>"},{"location":"user_guide/api/#public-neurobagel-apis","title":"Public Neurobagel APIs","text":"<p>In addition to supporting independent local/institutional deployments (i.e., instances) of the Neurobagel API, which can interface with a local or restricted graph, Neurobagel also hosts its own public instances of a node API and a federation API.</p> <p>https://api.neurobagel.org/ is a public, Neurobagel-hosted node API that interfaces with Neurobagel's own running graph instance containing harmonized datasets from the OpenNeuro platform.</p>"},{"location":"user_guide/api/#sending-a-request-to-a-neurobagel-api-directly","title":"Sending a request to a Neurobagel API directly","text":"<p>Cohort queries of a specific Neurobagel graph database can be submitted via direct requests to the corresponding node API using the <code>/query</code> endpoint, e.g. <code>https://api.neurobagel.org/query</code>. Specific query parameters are defined using key-value pairs in the URL following <code>/query</code>.</p> <p>Example: \"I want to query for only female participants in the OpenNeuro graph.\"</p> <p>The URL for such a query would be <code>https://api.neurobagel.org/query?sex=snomed:248152002</code>, where <code>snomed:248152002</code> is a controlled term from the SNOMED CT vocabulary corresponding to female sex.</p>"},{"location":"user_guide/api/#example-using-a-curl-request","title":"Example using a curl request","text":"<pre><code># To query for female participants in the graph\n\ncurl -X 'GET' \\\n  'https://api.neurobagel.org/query?sex=snomed:248152002' \\\n  -H 'accept: application/json'\n\n# or\ncurl -L https://api.neurobagel.org/query?sex=snomed:248152002\n</code></pre> <p>Avoid trailing slashes in API endpoint URLs</p> <p>Neurobagel APIs have strict requirements regarding trailing slashes. When sending <code>curl</code> requests to an instance of a Neurobagel API, ensure that you do not include trailing slashes in endpoint URLs.  For example, requests to <code>https://api.neurobagel.org/query</code> will work, but <code>https://api.neurobagel.org/query/</code> will not.</p>"},{"location":"user_guide/api/#using-the-interactive-neurobagel-api-docs","title":"Using the interactive Neurobagel API docs","text":"<p>Interactive documentation for a Neurobagel API (provided by Swagger UI) is available at the <code>/docs</code> endpoint (e.g., https://api.neurobagel.org/docs) and can also be used to run queries against the graph.</p> <p>Note</p> <p>For convenience, navigating to https://api.neurobagel.org/ in the browser will automatically redirect you to the docs.</p> <p>To send a request to the API from the docs interface, expand the <code>query</code> endpoint tab with the  icon to view the parameters that can be set,  and click \"Try it out\" and then \"Execute\" to execute a query.</p> <p>Note</p> <p>Due to limitations of Swagger UI in displaying very large HTTP response bodies,  queries with very few parameters sent using the interactive docs UI may be very slow or time out.  If this is the case, try using a <code>curl</code> request or the query tool instead.</p>"},{"location":"user_guide/cli/","title":"The Neurobagel CLI","text":"<p>The Neurobagel CLI is a command-line tool that processes a Neurobagel-annotated dataset and produces harmonized subject-level phenotypic and imaging attributes.  The resulting harmonized data can be directly integrated into a Neurobagel graph store.</p>"},{"location":"user_guide/cli/#installation","title":"Installation","text":"PythonDockerSingularity <p>The Neurobagel CLI can be installed from PyPI using <code>pip</code>.</p> <ol> <li> <p>Before installing the Python package, we recommend first creating and activating a Python virtual environment (using a tool such as venv).</p> </li> <li> <p>Install the <code>bagel</code> package into your virtual environment: <pre><code>pip install bagel\n</code></pre></p> </li> </ol> <p>Pull the Docker image for the Neurobagel CLI from Docker Hub: <pre><code>docker pull neurobagel/bagelcli\n</code></pre></p> <p>Build a Singularity image for the Neurobagel CLI using the Docker Hub image: <pre><code>singularity pull bagel.sif docker://neurobagel/bagelcli\n</code></pre></p>"},{"location":"user_guide/cli/#input-files","title":"Input files","text":"<p>The Neurobagel CLI creates a single harmonized view of each subject's data in a dataset, and can integrate information from several data sources (phenotypic, raw neuroimaging, processed neuroimaging).</p> <p>To run the CLI on a dataset, you will need the following files:</p> <ul> <li> A phenotypic TSV</li> <li> A Neurobagel JSON data dictionary for the TSV</li> <li> (Optional) A valid BIDS metadata table, if subjects have neuroimaging data available (1)</li> <li> (Optional) A TSV of subject statuses for any image processing pipelines that have been run, following the Nipoppy processing status file schema (2)</li> </ul> <ol> <li>This table can be generated automatically using the CLI's <code>bids2tsv</code> command, and will be used to generate harmonized subject imaging data availability.</li> <li>This file is adapted from the Nipoppy workflow and can be automatically generated using Nipoppy pipeline trackers. It will be used to generate harmonized subject processed imaging data availability.</li> </ol>"},{"location":"user_guide/cli/#running-the-cli","title":"Running the CLI","text":"<p>To view the general CLI help and information about the available commands:</p> PythonDockerSingularity <pre><code>bagel -h\n</code></pre> <pre><code># This is a shorthand for: docker run --rm neurobagel/bagelcli --help\ndocker run --rm neurobagel/bagelcli\n</code></pre> <pre><code># This is a shorthand for: singularity run bagel.sif --help\nsingularity run bagel.sif\n</code></pre>"},{"location":"user_guide/cli/#generate-a-bids-metadata-table","title":"Generate a BIDS metadata table","text":"<p>Info</p> <ul> <li>If your dataset does not have imaging data, skip this step. </li> <li>If your dataset's imaging data are not in BIDS format, you must manually create a BIDS metadata table.</li> </ul> <p>To include BIDS imaging data as part of the harmonized subject data, you must first convert the BIDS metadata into a table. </p> <p>You can do this automatically using the CLI's <code>bids2tsv</code> command.<sup>1</sup></p> <p>Example:</p> <p>If your BIDS directory is located at <code>/data/public/Dataset1_bids</code> and you want the table output to be saved to <code>/home/Neurobagel</code>:</p> PythonDockerSingularity <pre><code>bagel bids2tsv \\\n    --bids-dir \"/data/public/Dataset1_bids\"\n    --output \"/home/Neurobagel/Dataset1_bids.tsv\"\n</code></pre> <pre><code>docker run --rm \\\n    -v \"/data/public:/data/public\" \\\n    -v \"/home/Neurobagel:/home/Neurobagel\" \\ \n    neurobagel/bagelcli bids2tsv \\\n    --bids-dir \"/data/public/Dataset1_bids\" \\\n    --output \"/home/Neurobagel/Dataset1_bids.tsv\"\n</code></pre> Mounting input paths using <code>-v</code>/<code>--volume</code> <p>When running the CLI in a container, you must mount any input or output directories to directory paths within the container so that the app can access them. In your CLI options, always refer to the container paths.  In the example above, container paths are set to match the host paths for simplicity.</p> <pre><code>singularity run --no-home \\\n    -B \"/data/public,/home/Neurobagel\" \\\n    bagel.sif bids2tsv \\\n    --bids-dir \"/data/public/Dataset1_bids\" \\\n    --output \"/home/Neurobagel/Dataset1_bids.tsv\"\n</code></pre> Mounting input paths using <code>-B</code>/<code>--bind</code> <p>When running the CLI in a container, you must mount any input or output directories to directory paths within the container so that the app can access them. In your CLI options, always refer to the container paths.  In the example above, the container paths are set to match the host paths for simplicity.</p> This command may be slow on large datasets <p>On datasets with more than a few hundred subjects, <code>bids2tsv</code> can take upwards of several minutes  due to the time needed for <code>PyBIDS</code> to read the dataset structure.</p> <p>This will produce a BIDS metadata table named <code>Dataset1_bids.tsv</code>, which can then be provided as input to the <code>bids</code> command below.</p>"},{"location":"user_guide/cli/#generate-graph-ready-data-jsonld-files","title":"Generate graph-ready data (JSONLD files)","text":"<p>The Neurobagel CLI provides different commands for generating different types of harmonized subject (meta)data:</p> <ul> <li> <p><code>pheno</code></p> <p>Must be run first</p> <p>Each subject in a Neurobagel graph requires at least phenotypic data.  The other metadata are optional and can be added afterward via the <code>bids</code> and/or <code>derivatives</code> commands in any order.</p> </li> <li> <p><code>bids</code></p> </li> <li><code>derivatives</code></li> </ul> <p>If you are using Docker or Singularity, we strongly recommend placing all the input files for your dataset into a single directory. This avoids the need to mount multiple paths into the container when running CLI commands.</p>"},{"location":"user_guide/cli/#viewing-help-for-a-command","title":"Viewing help for a command","text":"<p>To view the command-line options for a specific command, such as <code>pheno</code>:</p> PythonDockerSingularity <pre><code>bagel pheno -h\n</code></pre> <pre><code>docker run --rm neurobagel/bagelcli pheno -h\n</code></pre> <pre><code>singularity run bagel.sif pheno -h\n</code></pre> <p>Example:</p> <p>The following example assumes that the input files for your dataset are located in <code>/home/Dataset1/Neurobagel</code>:</p> <pre><code>home/\n\u2514\u2500\u2500 Dataset1/\n    \u251c\u2500\u2500 Neurobagel/\n    \u2502   \u251c\u2500\u2500 Dataset1_pheno.tsv # (1)!\n    \u2502   \u251c\u2500\u2500 Dataset1_pheno.json # (2)!\n    \u2502   \u251c\u2500\u2500 Dataset1_bids.tsv # (3)!\n    \u2502   \u251c\u2500\u2500 Dataset1_proc_status.tsv # (4)!\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 ...\n</code></pre> <ol> <li>The phenotypic TSV</li> <li>The phenotypic data dictionary</li> <li>The BIDS metadata table</li> <li>The processing status file</li> </ol> <p>Navigate to the directory containing your input files, e.g.:</p> <pre><code>cd /home/Dataset1/Neurobagel\n</code></pre> <p>Info</p> <p>In the example commands below, replace the Dataset1 files with the actual input files for your dataset.</p>"},{"location":"user_guide/cli/#1-process-phenotypic-data-using-the-pheno-command-required","title":"1. Process phenotypic data using the <code>pheno</code> command (required)","text":"<p>Run the command below to generate harmonized subject-level phenotypic data for your dataset as a JSONLD file:</p> PythonDockerSingularity <pre><code>bagel pheno \\\n    --pheno \"Dataset1_pheno.tsv\" \\\n    --dictionary \"Dataset1_pheno.json\" \\\n    --name \"Dataset 1\" \\\n    --portal \"https://www.mydatasetportal.org/dataset1\" \\ # (1)!\n    --output \"Dataset1.jsonld\"\n</code></pre> <ol> <li>The website/URL you enter here will be shown as a clickable link when this  dataset is discovered in the query tool</li> </ol> <pre><code>docker run --rm -v $PWD:$PWD neurobagel/bagelcli pheno \\\n    --pheno \"$PWD/Dataset1_pheno.tsv\" \\\n    --dictionary \"$PWD/Dataset1_pheno.json\" \\\n    --name \"Dataset 1\" \\\n    --portal \"https://www.mydatasetportal.org/dataset1\" \\ # (1)!\n    --output \"$PWD/Dataset1.jsonld\"\n</code></pre> <ol> <li>The website/URL you enter here will be shown as a clickable link when this  dataset is discovered in the query tool</li> </ol> <pre><code>singularity run --no-home -B $PWD bagel.sif pheno \\\n    --pheno \"$PWD/Dataset1_pheno.tsv\" \\\n    --dictionary \"$PWD/Dataset1_pheno.json\" \\\n    --name \"Dataset 1\" \\\n    --portal \"https://www.mydatasetportal.org/dataset1\" \\ # (1)!\n    --output \"$PWD/Dataset1.jsonld\"\n</code></pre> <ol> <li>The website/URL you enter here will be shown as a clickable link when this  dataset is discovered in the query tool</li> </ol>"},{"location":"user_guide/cli/#2-process-raw-imaging-metadata-using-the-bids-command-optional","title":"2. Process raw imaging metadata using the <code>bids</code> command (optional)","text":"<p>If you have a BIDS metadata table, run this command to include subjects' imaging data availability to your dataset JSONLD file:</p> PythonDockerSingularity <pre><code>bagel bids \\\n    --jsonld-path \"Dataset1.jsonld\" \\\n    --bids-table \"Dataset1_bids.tsv\" \\\n    --output \"Dataset1.jsonld\" \\\n    --overwrite\n</code></pre> <pre><code>docker run --rm -v $PWD:$PWD neurobagel/bagelcli bids \\\n    --jsonld-path \"$PWD/Dataset1.jsonld\" \\\n    --bids-table \"$PWD/Dataset1_bids.tsv\" \\\n    --output \"$PWD/Dataset1.jsonld\" \\\n    --overwrite\n</code></pre> <pre><code>singularity run --no-home -B $PWD bagel.sif bids \\\n    --jsonld-path \"$PWD/Dataset1.jsonld\" \\\n    --bids-table \"$PWD/Dataset1_bids.tsv\" \\\n    --output \"$PWD/Dataset1.jsonld\" \\\n    --overwrite\n</code></pre>"},{"location":"user_guide/cli/#3-process-derived-imaging-metadata-using-the-derivatives-command-optional","title":"3. Process derived imaging metadata using the <code>derivatives</code> command (optional)","text":"<p>If you have a processing status file from Nipoppy, run this command to add subjects' processing pipeline data availability to the dataset JSONLD:</p> PythonDockerSingularity <pre><code>bagel derivatives \\\n    --jsonld-path \"Dataset1.jsonld\" \\\n    --tabular \"Dataset1_proc_status.tsv\" \\\n    --output \"Dataset1.jsonld\" \\\n    --overwrite\n</code></pre> <pre><code>docker run --rm --v $PWD:$PWD neurobagel/bagelcli derivatives \\\n    --jsonld-path \"$PWD/Dataset1.jsonld\" \\\n    --tabular \"$PWD/Dataset1_proc_status.tsv\" \\\n    --output \"$PWD/Dataset1.jsonld\" \\\n    --overwrite\n</code></pre> <pre><code>singularity run --no-home -B $PWD bagel.sif derivatives \\\n    --jsonld-path \"$PWD/Dataset1.jsonld\" \\\n    --tabular \"$PWD/Dataset1_proc_status.tsv\" \\\n    --output \"$PWD/Dataset1.jsonld\" \\\n    --overwrite\n</code></pre> <p>Tip</p> <p>To see all options for a CLI command, including short forms and optional parameters, refer to the command's help.</p> When to use <code>-f</code>/<code>--overwrite</code> <p>If you're only interested in the final JSONLD with all metadata added (i.e., after all relevant commands have been run), you can safely overwrite intermediate output files by specifying the same output file path each time.</p> <p>These steps have generated a graph-ready JSONLD file for Dataset1 (<code>Dataset1.jsonld</code>) that incorporates all the available subject data sources.  The resulting JSONLD is ready to upload to a Neurobagel graph database.</p>"},{"location":"user_guide/cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide/cli/#file-or-directory-does-not-exist-error-when-using-dockersingularity","title":"<code>File or directory does not exist</code> error when using Docker/Singularity","text":"<p>This error usually means the container cannot access your input files because the directories were not mounted correctly.</p> <p>The examples assume you are running the CLI from inside the directory containing your inputs. Thus, they mount the current working directory <code>$PWD</code> to the same path inside the container for convenience using the syntax:</p> DockerSingularity <pre><code>docker run --rm -v $PWD:$PWD neurobagel/bagelcli ...\n</code></pre> <p>However, if your inputs are located in a different directory or spread across multiple directories, you must mount each directory explicitly using the Docker option <code>-v /path/on/host:/path/in/container</code>.</p> <p>When passing file paths to the CLI, always use the absolute path inside the container to avoid confusion.</p> <pre><code>singularity run --no-home -B $PWD bagel.sif ...\n</code></pre> <p>However, if your inputs are located in a different directory or spread across multiple directories, you must mount each directory explicitly using the Singularity option <code>-B /path/on/host:/path/in/container</code>.</p> <p>When passing file paths to the CLI, always use the absolute path inside the container to avoid confusion.</p>"},{"location":"user_guide/cli/#upgrading-data-to-a-newer-version-of-the-cli","title":"Upgrading data to a newer version of the CLI","text":"<p>Neurobagel is under active development and future CLI releases may introduce breaking changes to the data model used in subject-level <code>.jsonld</code> graph files.  Breaking changes are highlighted in the release notes.</p> <p>To upgrade to the latest version of the data model:</p> <ol> <li> <p>Upgrade to the latest CLI version:</p> PythonDockerSingularity <pre><code>pip install --upgrade bagel\n</code></pre> <pre><code>docker pull neurobagel/bagelcli\n</code></pre> <pre><code>singularity pull bagel.sif docker://neurobagel/bagelcli\n</code></pre> </li> <li> <p>If you have an existing Neurobagel graph database, we recommend regenerating and reuploading all existing <code>.jsonld</code> files in your database using the latest CLI version. This keeps the database internally consistent and avoids conflicts with dataset <code>.jsonld</code> files generated using older CLI versions.</p> </li> </ol> <ol> <li> <p><code>bids2tsv</code> internally uses bids2table.\u00a0\u21a9</p> </li> </ol>"},{"location":"user_guide/config/","title":"Configuring a node","text":"<p>Neurobagel is designed to be easily deployed with a single command without deep configuration. In many cases however, you will want to customize your deployment to fit your needs.</p> <p>If you already have a running Neurobagel node,  after making any configuration changes  (including changing the data you want to be available in the graph database),  follow the instructions to restart your services  for the changes to take effect.</p>"},{"location":"user_guide/config/#deployment","title":"Deployment","text":""},{"location":"user_guide/config/#available-services","title":"Available services","text":"<p>The Neurobagel Docker Compose recipe includes several services and coordinates them to work together:</p> <p>(In parentheses are the names of services within the Docker Compose stack)</p> <ul> <li>Neurobagel node API/n-API (<code>api</code>): The API that communicates with a single graph store and determines      how detailed the response to a query should be from that graph.</li> <li>Graph store (<code>graph</code>): A third-party RDF store that stores Neurobagel-harmonized data to be queried. At the moment our recipe uses the free tier     of GraphDB for this.</li> <li>Neurobagel federation/f-API (<code>federation</code>): A special API that can federate over one or more     Neurobagel nodes to provide a single point of access to multiple distributed databases.     By default it will federate over all public nodes and any local nodes you specify. </li> <li>Neurobagel query tool (<code>query_federation</code>): A web app that provides a graphical interface for users to query a      federation API and view the results from one or more nodes. Because the query tool is a static app and is run locally     in the user's browser, this service simply hosts the app.</li> </ul>"},{"location":"user_guide/config/#available-profiles","title":"Available profiles","text":"<p>Neurobagel offers different deployment profiles that allow you to spin up specific combinations of services (listed below), depending on your use case.</p> <ol> <li> <p><code>full_stack</code>: Best profile to get started with Neurobagel.      It includes all services you need to run a local Neurobagel node and have the ability to query public nodes, along with a graphical query tool.</p> <ul> <li><code>api</code></li> <li><code>graph</code></li> <li><code>federation</code></li> <li><code>query_tool</code></li> </ul> <p>Info</p> <p>This is the default profile if you don't specify one.</p> <p>By default, this profile will also federate over all publicly accessible Neurobagel nodes, although this behaviour can be disabled in the f-API using the environment variable <code>NB_FEDERATE_REMOTE_PUBLIC_NODES</code>.</p> </li> <li> <p><code>local_node</code>: Best profile if you want to run a standalone Neurobagel node     but rely on a separate deployment for providing federation and a graphical query tool (such as Neurobagel's own hosted public instances).</p> <ul> <li><code>api</code></li> <li><code>graph</code></li> </ul> </li> <li> <p><code>local_federation</code>: Best profile if you already have multiple standalone (local or non-publicly-accessible) Neurobagel node     deployments running and you now want to provide federation over them.  </p> <ul> <li><code>federation</code></li> <li><code>query_tool</code></li> </ul> <p>Info</p> <p>If you only want to federate over a single local node and all public Neurobagel nodes, we recommend using the <code>full_stack</code> profile to set up your node and federation in one step. If you choose to use the <code>local_federation</code> profile,  you will have to manually configure your <code>local_nb_nodes.json</code> file.</p> </li> </ol>"},{"location":"user_guide/config/#launching-a-profile","title":"Launching a profile","text":"<p>You can then launch a specific profile using the <code>--profile</code> or  <code>-p</code> flag with <code>docker compose</code>, e.g.: <pre><code>docker compose --profile full_stack up -d\n</code></pre> If no profile is specified, <code>docker compose up -d</code> will start the services for the default profile, <code>full_stack</code>.</p> <p>Take a look at the getting started guide for more information setting up for a first launch.</p>"},{"location":"user_guide/config/#default-host-ports-for-services","title":"Default host ports for services","text":"Don't publicly expose service ports on a production server <p>We're providing the default ports as a reference for local deployment, testing, and for scenarios where you do not want to use the provided reverse proxy deployment recipes.</p> <p>Where possible, we strongly recommend that you avoid opening service ports to a public network and instead use our reverse proxy deployment recipe.</p> <p>Neurobagel node services run inside Docker containers. Each service listens on an internal port within its container and  exposes a host port that makes it accessible from the host machine. Below, we list the default host ports for each service when running in a fresh deployment following our getting started guide,  along with the environment variables that can be used to configure them.</p> <ul> <li><code>api</code> (the node API)<ul> <li>environment variable: <code>NB_NAPI_PORT_HOST</code></li> <li>default host port: <code>8000</code></li> </ul> </li> <li><code>federation</code> (the federation API)<ul> <li>environment variable: <code>NB_FAPI_PORT_HOST</code></li> <li>default host port: <code>8080</code></li> </ul> </li> <li><code>query_tool</code> (the graphical query web interface)<ul> <li>environment variable: <code>NB_QUERY_PORT_HOST</code></li> <li>default host port: <code>3000</code></li> </ul> </li> <li><code>graph</code> (the internal graph database)<ul> <li>environment variable: <code>NB_GRAPH_PORT_HOST</code></li> <li>default host port: <code>7200</code></li> </ul> </li> </ul>"},{"location":"user_guide/config/#environment-variables","title":"Environment variables","text":"<p>Below are all the possible Neurobagel environment variables that can be set in <code>.env</code>.</p> Environment variable Default needs change? Description Default value if not set Used in these installation modes <code>NB_GRAPH_USERNAME</code> Yes Username to set for the graph database user. - Docker, Python <code>NB_GRAPH_SECRETS_PATH</code> Yes Path to files containing the secure passwords to set for the admin user (NB_GRAPH_ADMIN_PASSWORD.txt) and graph database user (NB_GRAPH_PASSWORD.txt). <code>./secrets</code> Docker <code>NB_GRAPH_DB</code> Yes Name to give your graph database (e.g., for a GraphDB database, use the format <code>repositories/{database_name}</code>) <code>repositories/my_db</code> Docker, Python <code>NB_GRAPH_MEMORY</code> No The maximum amount of memory that can be used by graph. Equivalent to setting the <code>-Xmx</code> parameter on the JVM. Value should be a number followed directly by a letter denoting the size. E.g. <code>264m</code> for 264 MB, <code>2g</code> for 2 GB. (For more info, see https://graphdb.ontotext.com/documentation/10.8/requirements.html#hardware-sizing.) <code>2g</code> Docker <code>LOCAL_GRAPH_DATA</code> Yes Path on your filesystem to the JSONLD files you want to upload to the graph database <code>./data</code> Docker <code>NB_GRAPH_PORT_HOST</code> No Port number on the host machine to map the graph server container port to <code>7200</code> Docker <code>NB_NAPI_ALLOWED_ORIGINS</code> No Origins allowed to make cross-origin resource sharing requests. Multiple origins must be separated with spaces in a single string enclosed in quotes. <code>\"\"</code> Docker, Python <code>NB_RETURN_AGG</code> No Whether to return only aggregate, dataset-level query results (excluding subject/session-level attributes). One of [true, false] <code>true</code> Docker, Python <code>NB_MIN_CELL_SIZE</code> No Minimum number of matching subjects required for a dataset to be returned as a query match. Datasets with matching subjects &lt;= this number will be excluded from query results. <code>0</code> Docker, Python <code>NB_NAPI_TAG</code> No Docker image tag for the Neurobagel node API <code>latest</code> Docker <code>NB_NAPI_PORT_HOST</code> No Port number on the host machine to map the Neurobagel node API container port to <code>8000</code> Docker <code>NB_NAPI_BASE_PATH</code> No (If using reverse proxy) The URL path where the node API is served from. Do not include a trailing slash. <code>\"\"</code> Docker <code>NB_FAPI_TAG</code> No Docker image tag for the Neurobagel federation API <code>latest</code> Docker <code>NB_FAPI_PORT_HOST</code> No Port number on the host machine to map the Neurobagel federation API container port to <code>8080</code> Docker <code>NB_FEDERATE_REMOTE_PUBLIC_NODES</code> No If \"True\", include public nodes in federation. If \"False\", only locally specified nodes in <code>local_nb_nodes.json</code> are queried. <code>true</code> Docker, Python <code>NB_FAPI_BASE_PATH</code> No (If using reverse proxy) The URL path where the federation API is served from. Do not include a trailing slash. <code>\"\"</code> Docker <code>NB_QUERY_TAG</code> No Docker image tag for the query tool <code>latest</code> Docker <code>NB_QUERY_PORT_HOST</code> No Port number used by the <code>query_tool</code> on the host machine <code>3000</code> Docker <code>NB_API_QUERY_URL</code> Yes URL (and port number, if needed) of the Neurobagel API that the query tool will send its requests to. The query tool sends requests from a user's machine, so ensure the API URL is provided as a user would access it from their own machine. See also the query tool README. - Docker <code>NB_QUERY_APP_BASE_PATH</code> No (If using reverse proxy) The URL path for the query tool, determines the specific URL at which the app should be rendered for users to access it <code>/</code> Docker <code>NB_QUERY_HEADER_SCRIPT</code> No (Experimental, for development environments only) Custom script to add to the header section of the query tool site, such as for a GDPR-aware analytics tool. <code>\"\"</code> Docker <code>NB_ENABLE_AUTH</code> No (Experimental, for development environments only) Whether to enable authentication for cohort queries. One of [true, false] <code>false</code> Docker, Python <code>NB_QUERY_CLIENT_ID</code> No (Experimental, for development environments only) OAuth client ID for the query tool. Required if NB_ENABLE_AUTH is set to true. - Docker, Python Ensure that shell variables do not clash with <code>.env</code> file <p>If the shell you run <code>docker compose</code> from already has any  shell variable of the same name set,  the shell variable will take precedence over the configuration of <code>.env</code>! In this case, make sure to <code>unset</code> the local variable first.</p> <p>For more information, see Docker's environment variable precedence.</p> <p>Tip</p> <p>Double check that any environment variables you have customized in <code>.env</code> are resolved with your expected values using the command <code>docker compose config</code>.</p>"},{"location":"user_guide/config/#change-security-relevant-variables","title":"Change security relevant variables","text":"<p>The graph store (GraphDB instance) in a Neurobagel node is secured with password-based access and includes two users: an <code>admin</code> superuser and a regular database user, both of which are automatically configured by the Neurobagel deployment recipe. Passwords for both users are defined via files in the <code>./secrets</code> directory of the recipes repository, while the regular database username is set through an environment variable in <code>.env</code> file.</p> <p>For security and best practice purposes, we recommend changing the following values from their defaults if you are using a deployment profile that includes a graph store:</p> <ol> <li> <p>In your <code>.env</code>, set a custom username and database name for your graph store by editing the following variables:</p> <ul> <li><code>NB_GRAPH_USERNAME</code></li> <li><code>NB_GRAPH_DB</code></li> </ul> </li> <li> <p>In the (<code>./secrets</code> directory, change the default passwords by replacing the contents of the file <code>NB_GRAPH_ADMIN_PASSWORD.txt</code> for the <code>admin</code> superuser, and the file <code>NB_GRAPH_PASSWORD.txt</code> for the graph database user (corresponding to <code>NB_GRAPH_USERNAME</code>).</p> <ul> <li>To generate a random password in the terminal, you can use:   <pre><code>openssl rand -hex 16\n</code></pre></li> <li>(Optional) You can change the directory where your password files are stored by editing the <code>NB_GRAPH_SECRETS_PATH</code> variable in <code>.env</code>.</li> </ul> Graph store passwords are not meant for node users! <p>The <code>admin</code> user and graph database user credentials are intended solely for internal use by the deployment recipe scripts that automatically set up and update the graph store,  or for a node administrator to interact directly with the graph store. These credentials also secure internal communication between your graph store and its node API, ensuring that node users cannot query your graph directly. GraphDB user credentials are not intended for use by a general node query user.</p> Passwords are handled as Docker secrets <p>The contents of <code>NB_GRAPH_ADMIN_PASSWORD.txt</code> and <code>NB_GRAPH_PASSWORD.txt</code> are passed to Neurobagel containers as Docker secrets. This ensures that your passwords are not exposed in the container logs or in the <code>docker-compose.yml</code> file.</p> <p>Do not share your password files with others.</p> </li> <li> <p>Review and change as needed the following variables in <code>.env</code> based on your data sharing requirements:</p> <ul> <li><code>NB_RETURN_AGG</code></li> <li><code>NB_MIN_CELL_SIZE</code></li> </ul> <p>Info</p> <p>These variables are modifiable after node initialization; you can change their values at any time.</p> </li> <li> <p>If you've previously launched a Neurobagel Docker Compose stack following the Getting started instructions,     you'll need to reset your graph store for any changes you have made to user credentials to take effect (steps 1-2 above).      Don't worry, any other configuration changes you've already made will be applied when you re-launch your node.</p> </li> </ol>"},{"location":"user_guide/config/#configuring-local-node-names-and-urls-for-federation","title":"Configuring local node names and URLs for federation","text":"<p>When using a deployment profile that provides federation (i.e., includes the federation API),  you can configure the URLs and display names of the node APIs of any local nodes you wish to federate over in the file <code>local_nb_nodes.json</code>.  This file is read by the f-API.</p> <p>Each node to be federated over is defined using a dictionary with two key-value pairs: <pre><code>{\n  \"NodeName\": \"&lt;DISPLAY NAME OF NODE&gt;\",\n  \"ApiURL\": \"&lt;URL OF NODE API&gt;\"\n}\n</code></pre></p> <p><code>ApiURL</code> must include the protocol (<code>http://</code> or <code>https://</code>)</p> <p>If you are only running a single node, or only want to federate across your local node and the public Neurobagel nodes,  you do not need to modify the default <code>ApiURL</code> in <code>local_nb_nodes.json</code>. However, you may want to customize your node's <code>NodeName</code>.</p> <p>Notes:</p> <ul> <li><code>NodeName</code> can be any string, and determines how the node appears in the node selection dropdown in the query tool</li> <li>To add more local nodes to federate over, simply add more dictionaries to <code>local_nb_nodes.json</code>, and ensure the dictionaries are wrapped in a list <code>[]</code> (see example below)</li> </ul> <p>Nodes that do not need to be manually configured</p> <p>We maintain a list of publicly accessible Neurobagel nodes  here. By default, every new f-API will look up this list on startup and include it in its internal list of nodes to federate over (this can be disabled using the environment variable <code>NB_FEDERATE_REMOTE_PUBLIC_NODES</code>). This means that you do not have to manually add these public nodes to your <code>local_nb_nodes.json</code> file.</p> <p>Example: Assume there are two local nodes already running on different servers of your institutional network:</p> <ul> <li>a node named <code>\"My Institute\"</code> running on your local computer (<code>localhost</code>), on port <code>8000</code> </li> <li>a node named <code>\"Node Recruitment\"</code> running on a different computer with the local IP <code>192.168.0.1</code>, listening on the default HTTP port <code>80</code> </li> </ul> <p>To set up federation across both nodes, you would configure <code>local_nb_nodes.json</code> as follows: local_nb_nodes.json<pre><code>[\n  {\n    \"NodeName\": \"My Institute\",\n    \"ApiURL\": \"https://neurobagel.myinstitute.edu\",\n  },\n  {\n    \"NodeName\": \"Node Recruitment\",\n    \"ApiURL\": \"http://192.168.0.1\"\n  }\n]\n</code></pre></p> Do not use <code>localhost</code>/<code>127.0.0.1</code> in <code>local_nb_nodes.json</code> <p>Even if the local node API(s) you are federating over are running  on the same host machine as your federation API,  you cannot use <code>localhost</code> for the <code>ApiURL</code> and must instead provide a network-accessible URL, IP address, or container name. For an example, see the configuration for the node called <code>\"My Institute\"</code> above.</p> Be careful to not use your federation API's own address for <code>ApiURL</code>! <p>This will cause an infinite request loop that will likely overload your service, as an f-API will be repeatedly making requests to itself.</p>"},{"location":"user_guide/config/#behind-a-reverse-proxy","title":"Behind a reverse proxy","text":"<p>These steps are for advanced users and production deployments</p> <p>To make your Neurobagel node services (node API, query tool, etc.) accessible via custom URLs (e.g. <code>https://www.myfirstnode.org/query</code>) rather than a server IP address and port (e.g. <code>http://192.168.0.1:3000</code>) as shown in in the getting started guide,  you will need to set up a reverse proxy such as NGINX or Caddy.  This will route incoming requests for custom URLs to the Neurobagel services deployed on your server. </p> <p>The Neurobagel <code>recipes</code> repository includes pre-configured Docker Compose files for both NGINX and Caddy, each of which can be used to launch a reverse proxy server alongside the services in your Neurobagel node. The reverse proxy setup will then automatically handle routing  and also manage and renew SSL certificates (providing secure HTTPS connections) for node services.</p> <ol> <li> <p>If you haven't already, follow the steps to clone and minimally configure the services in the Neurobagel deployment recipe.</p> </li> <li> <p>Ensure you have already registered your desired domain(s) with a DNS provider and configured the DNS settings to resolve correctly to your host machine.</p> </li> <li> <p>Ensure ports 80 and 443 are open on the host machine where your Docker Compose stack is running. These are the ports your reverse proxy will listen on for incoming HTTP and HTTPS traffic.</p> </li> </ol> NGINXCaddy <ol> <li> <p>In your local <code>docker-compose-nginx.yml</code> file, for each service (i.e. <code>api</code>, <code>federation</code>, and <code>query_federation</code>),</p> <ol> <li>Locate the <code>environment</code> section for that service </li> <li> <p>Update the value of the following variables to the custom domain that specific service will use:</p> <ul> <li><code>VIRTUAL_HOST</code></li> <li><code>LETSENCRYPT_HOST</code></li> </ul> <p>Both variables must have the same value for a given service, and must not include a protocol (<code>http://</code> or <code>https://</code>).</p> </li> <li> <p>(Optional) To host services on different subpaths instead of different subdomains (e.g., <code>myinstitute.org/service1</code> instead of <code>service1.myinstitute.org</code>),  do not include the subpath in the <code>VIRTUAL_HOST</code> or <code>LETSENCRYPT_HOST</code> values.  Instead, update the corresponding <code>XXX_BASE_PATH</code> variables for the services in the <code>.env</code> file. The full list of service-specific base path variables can be found here.</p> <p>For example, to host your node API at <code>myinstitute.org/node</code>:</p> docker-compose-nginx.yml<pre><code>...\napi:\n  environment: \n    VIRTUAL_HOST: myinstitute.org\n    LETSENCRYPT_HOST: myinstitute.org\n...\n</code></pre> .env<pre><code>...\nNB_NAPI_BASE_PATH=\"/node\"\n...\n</code></pre> </li> </ol> Do not change the <code>VIRTUAL_PATH</code> and <code>VIRTUAL_PORT</code> variables <p>You can look at the NGINX-Proxy documentation to learn more about how these variables work.</p> </li> <li> <p>In your <code>.env</code> file, set <code>NB_API_QUERY_URL</code> to your custom URL for the federation API, including any subpath if used.  This URL must always begin with <code>https://</code>.</p> <p>Example: .env<pre><code>...\nNB_API_QUERY_URL=\"https://myinstitute.org/federate\"\n...\n</code></pre></p> </li> <li> <p>Finally, launch your node by explicitly referencing the custom Docker Compose file:</p> <pre><code>docker compose -f docker-compose-nginx.yml up -d\n</code></pre> </li> </ol> <p>You do not need to edit the <code>docker-compose-caddy.yml</code> file directly.</p> <ol> <li>In your local <code>recipes/config/caddy/Caddyfile</code>, change the default URL for each service to the URL you want to use for that service. Follow the comments in the file for guidance.</li> </ol> For more complex reverse proxy setups, refer to the Caddy documentation <p>The Caddy documentation has more detailed information on subdirectory routing and other configuration options.</p> <ol> <li> <p>Finally, launch your node by explicitly referencing the custom Docker Compose file:</p> <pre><code>docker compose -f docker-compose-caddy.yml up -d\n</code></pre> </li> </ol>"},{"location":"user_guide/data_prep/","title":"Preparing the phenotypic data","text":"Looking for more general guidelines on how to organize your dataset? <p>We recommend also checking out Nipoppy, a protocol for standardized organization and processing of clinical-neuroimaging datasets that extends BIDS.  Neurobagel tools are designed to be compatible with data organized according to the Nipoppy specification, although you do not need to use Nipoppy in order to use Neurobagel.</p> <p>To use the Neurobagel annotation tool, you must prepare the tabular data for your dataset as a single tab-separated values (<code>.tsv</code>) file.</p> <p>Within Neurobagel, tabular (or phenotypic) data  refers to any demographic, clinical/behavioural, cognitive, or other non-imaging-derived data of participants which are typically stored in a tabular format.</p>"},{"location":"user_guide/data_prep/#requirements-for-the-phenotypic-tsv","title":"Requirements for the phenotypic TSV","text":"<p>If you're unfamiliar with TSV files or unsure how to format them correctly</p> <p>Please first consult our TSV glossary section for information on creating valid TSV files.</p> <p>In a valid phenotypic TSV file for Neurobagel, rows identify each participant (or participant-session in a longitudinal dataset), and columns describe properties of participants (age, sex, diagnosis, etc.). </p> <p>Each row MUST describe only one participant/session, and each participant/session MUST be described by only one row.</p> <p>The TSV MUST contain:</p> <ul> <li>A minimum of 2 columns</li> <li> <p>At least 1 column containing subject identifiers (IDs). Subject IDs must be unique per row.</p> <ul> <li>If both subject and session ID columns are present, then the combinations of IDs must be unique per row.</li> </ul> <p>Only one subject ID column can be annotated</p> <p>Neurobagel currently does not support linking multiple IDs to a single subject.  If your TSV file includes multiple subject ID columns (e.g., study ID, hospital ID), you will choose one column to serve as the primary subject ID during annotation.</p> </li> <li> <p>At least 1 column that describes demographic or other phenotypic information</p> </li> </ul> <p>The TSV MAY contain:</p> <ul> <li> <p>At least 1 column containing session identifiers (e.g., if the dataset is longitudinal)</p> <p>Only one session ID column can be annotated</p> <p>Neurobagel currently does not support linking multiple IDs to a single session. If your TSV file includes multiple session ID columns (e.g., visit number, scan ID), you will choose one column to serve as the primary session ID during annotation.</p> </li> </ul> <p>The TSV MUST NOT contain:</p> <ul> <li>Missing values in the columns you intend to annotate as the primary subject ID or session ID (if available)</li> </ul> <p>For all phenotypic variables currently modeled by Neurobagel, see here.</p>"},{"location":"user_guide/data_prep/#if-your-dataset-has-imaging-bids-data","title":"If your dataset has imaging (BIDS) data","text":"<p>In addition to phenotypic characteristics of subjects, Neurobagel can also harmonize information about subjects' imaging data from a corresponding BIDS dataset (see also Preparing imaging data).</p> <p>To include subjects' BIDS imaging data as part of their representation in Neurobagel, your phenotypic TSV MUST meet the following requirements in addition to the ones listed above:</p> <ul> <li> <p>At least 1 column in the TSV contains subject IDs that    match the BIDS subject IDs (in the form <code>sub-&lt;label&gt;</code>),    AND this must be the column you annotate as the primary subject ID</p> <p>Subject IDs are case-sensitive</p> <p>Subject IDs in the phenotypic TSV must match corresponding BIDS subject IDs exactly  for Neurobagel to correctly link phenotypic and BIDS information.  For example, a BIDS ID of <code>sub-MNI001</code> would not be matched to subject IDs <code>sub-mni001</code> or <code>mni001</code> in a phenotypic TSV.</p> </li> <li> <p>All BIDS subject IDs must appear in the phenotypic TSV,    even if the subject only has imaging data (columns can be left empty for missing phenotypic values) </p> <ul> <li>Datasets that include subjects in the BIDS dataset but not in the phenotypic TSV are not supported.  However, subjects with phenotypic data only (no BIDS data) are allowed.</li> </ul> </li> </ul> <p>If your dataset is longitudinal, the session IDs in the phenotypic TSV MAY match the BIDS session IDs, but this is not required.</p>"},{"location":"user_guide/data_prep/#examples-of-valid-phenotypic-tsvs","title":"Examples of valid phenotypic TSVs","text":"<p>Depending on your dataset, your tabular data may look like one of the following:</p>"},{"location":"user_guide/data_prep/#a-bids-participantstsv-file","title":"A BIDS <code>participants.tsv</code> file","text":"<p>If you have a BIDS compliant <code>participants.tsv</code> that contains  all the demographic and clinical/behavioural information for participants,  you can annotate this file with Neurobagel's annotation tool to create a data dictionary for the file.</p> <p>Example TSV:</p> participant_id age sex tools sub-01 22 female WASI-2 sub-02 28 male Stroop ... ... ... ..."},{"location":"user_guide/data_prep/#a-longitudinal-data-file","title":"A longitudinal data file","text":"<p>If you have longitudinal tabular data (e.g. age collected at multiple sessions/visits),  then the information for all sessions should be combined into a single TSV.  Each row must describe a unique combination of subject and session.</p> <p>Example TSV:</p> participant_id session_id age tools sub-01 ses-01 22 WASI-2 sub-01 ses-02 23 sub-02 ses-01 28 Stroop ... ... ... ... <p>Tip</p> <p>A <code>participants.tsv</code> file with multiple sessions is not BIDS compliant.  If you want to store multi-session phenotypic data in a BIDS dataset,  you could do so in the <code>phenotype/</code> subdirectory  (see also the BIDS specification section on Longitudinal and multi-site studies).</p>"},{"location":"user_guide/data_prep/#multiple-participant-or-session-id-columns","title":"Multiple participant or session ID columns","text":"<p>In some cases, there may be a need for more than one set of IDs for participants and/or sessions.</p> <p>For example, if a participant was first enrolled in a behavioural study with one type of ID, and then later joined an imaging study under a different ID,  both types of participant IDs may be recorded in the tabular file.</p> <p>For Neurobagel, the only requirement is that the combination of all ID values for a row is unique.</p> <p>Only one subject ID and session ID column can be annotated for Neurobagel</p> <p>If your TSV file contains multiple subject or session ID columns, you will select one to use as the primary ID during annotation. The remaining subject/session ID columns will be ignored by Neurobagel.</p> <p>Example invalid TSV:</p> participant_id alternative_participant_id ... sub-01 SID-1234 ... sub-01 SID-2222 ... sub-02 SID-1234 ... <p>The same rules apply when multiple session IDs are present.</p> <p>Example valid TSV:</p> participant_id alt_participant_id session_id alt_session_id age ... sub-01 SID-1234 ses-01 visit-1 22 ... sub-01 SID-1234 ses-02 visit-2 23 ... sub-02 SID-2222 ses-01 visit-1 28 ... ... ... ... ... ... ..."},{"location":"user_guide/getting_started/","title":"Getting started","text":"<p>The following sections will get you started with deploying your own Neurobagel node, a graphical query tool,  and a local federation API (everything in blue in the picture below) that lets you search across the data in your node and in public Neurobagel nodes.</p> <p></p> <p>To prepare your Neurobagel node for production use (i.e., for local or other users), and to configure your deployment according to your specific needs, refer to the detailed Configuration documentation.</p>"},{"location":"user_guide/getting_started/#requirements","title":"Requirements","text":"<p>Neurobagel tools are provided as Docker containers  and are launched with Docker Compose. </p> <p>Don't install Neurobagel tools directly on your machine</p> <p>Please only use the Docker images provided by Neurobagel  (or the third party providers Neurobagel relies on) and only launch them with our provided <code>docker-compose.yml</code> recipe.</p> <p>Do not install GraphDB locally on your computer,  as doing so can interfere with the deployment of the Neurobagel tools.</p>"},{"location":"user_guide/getting_started/#docker-and-docker-compose","title":"<code>docker</code> and <code>docker compose</code>","text":"<p>If you haven't yet, please install both <code>docker</code> and <code>docker compose</code> for your operating system:</p> LinuxWindowsMacOS <ol> <li> <p>Install the Docker engine and follow the post-setup instructions</p> </li> <li> <p>Install Docker Compose using the repository option</p> </li> </ol> <ol> <li> <p>Install Docker Desktop on Windows.  This will install both <code>docker</code> and <code>docker compose</code>.</p> </li> <li> <p>We strongly recommend also installing Windows Subsystem for Linux to get a Windows-supported Linux installation for a more seamless Neurobagel deployment experience.  Simply follow these instructions to make your existing Docker Desktop installation (including Docker and Docker Compose) available when running WSL.</p> </li> </ol> <p>Install Docker Desktop on Mac. This will install both <code>docker</code> and <code>docker compose</code> automatically.</p> Linux is the only supported OS <p>We test and deploy on Linux and ensure that our deployment instructions work on Linux systems.</p> <p>We also try to provide docs and help for different architectures, but as a small team with limited resources we won't be able to  help you debug Operating System specific problems. </p> <p>Because we rely on some modern features of these tools, please make sure you have at least the following versions on your machine:</p> <ul> <li><code>docker</code> engine: v20.10.24 or greater <pre><code>docker --version\n</code></pre></li> <li><code>docker compose</code>: v2.7.0 or above <pre><code>docker compose version\n</code></pre></li> </ul>"},{"location":"user_guide/getting_started/#the-neurobagel-node-deployment-recipe","title":"The Neurobagel node deployment recipe","text":"<p>The <code>neurobagel/recipes</code> repository  on GitHub contains our official Docker Compose recipe and template configuration files for setting up a local Neurobagel node.</p> <ol> <li>Clone the GitHub repository to your machine and navigate to it <pre><code>git clone https://github.com/neurobagel/recipes.git\ncd recipes\n</code></pre></li> <li> <p>Make copies of the template configuration files to edit for your deployment (do not edit the <code>template</code> files themselves) <pre><code>cp template.env .env\ncp local_nb_nodes.template.json local_nb_nodes.json\n</code></pre></p> <p>Configuring local nodes to federate over</p> <p>To customize the name for your node as it will appear in the Neurobagel query tool (default: \"Local graph 1\"), or to federate over more than one local node, see the section on configuring local nodes for federation.</p> </li> <li> <p>Change the placeholder value of <code>NB_API_QUERY_URL</code> in the <code>.env</code> file</p> <p><pre><code>NB_API_QUERY_URL=http://XX.XX.XX.XX\n</code></pre>    Replace <code>http://XX.XX.XX.XX</code> with the full URL where the Neurobagel federation API will be accessed, including the protocol (<code>http://</code> or <code>https://</code>):</p> <ul> <li>For local use or testing of the services on your machine only:     you can use <code>NB_API_QUERY_URL=http://localhost:8080</code> (<code>8080</code> is the default host port for the federation API)</li> <li>For deployment on a server for other users:  you must use the IP address with port (e.g., <code>http://123.45.67.89:8080</code>) or a domain-based address (e.g., <code>https://mysite.com/federation</code>) of the federation API that is reachable from other users' computers</li> </ul> </li> </ol> On a machine with an ARM-based processor? <p>The default Docker Compose recipe assumes that you are launching Neurobagel on a machine with x86_64 (AMD/Intel) architecture (most Linux or Windows machines).  If your machine instead uses ARM-based architecture (e.g., certain Macs), additionally change the following line in your <code>docker-compose.yml</code> file: <pre><code>    graph:\n        image: \"ontotext/graphdb:10.3.1\"     \n</code></pre> to <pre><code>    graph:\n        image: \"ontotext/graphdb:10.3.1-arm64\"        \n</code></pre> You can double check the architecture of your machine in the system settings or using the command <code>lscpu</code>.</p>"},{"location":"user_guide/getting_started/#if-you-have-already-have-graph-ready-data","title":"If you have already have graph-ready data","text":"<p>At this point, if you have already generated Neurobagel JSONLD data files, you can proceed with the below additional steps before launching Neurobagel:</p> <ol> <li> <p>Update <code>LOCAL_GRAPH_DATA</code> in <code>.env</code> to the path containing the data files you wish to add to the graph database.</p> <p>You can update these data in the graph at any time. For more information, see this section.</p> </li> <li> <p>Change the default credentials for your graph database following these instructions.</p> </li> </ol> <p>Info</p> <p>This section provide a minimal configuration for launching Neurobagel. In most cases, particularly when deploying Neurobagel for other users, additional configurations may be necessary to ensure optimal usability and security of your node.</p> <p>Please refer to our detailed documentation for a complete overview of  configuration options.</p>"},{"location":"user_guide/getting_started/#launch-neurobagel","title":"Launch Neurobagel","text":"<p>Once you have completed at least steps 1 to 3 above,  you can launch your own Neurobagel node using Docker Compose:</p> <pre><code>docker compose up -d\n</code></pre> Explanation <p>This is a shorthand for: <code>docker compose --profile full_stack up -d</code></p> <p>This will:</p> <ul> <li>pull the required Docker images (if you haven't pulled them before)</li> <li>launch the containers for the Neurobagel services using the default <code>full_stack</code> service profile</li> <li>automatically set up and configure the services based on your configuration files</li> <li>automatically upload data to the Neurobagel graph (by default, it will upload an example dataset we have provided for testing)</li> </ul> <p>You can check that your docker containers have launched correctly by running:</p> <p><pre><code>docker ps\n</code></pre> and you will want to see something like this to show all 4 services running: <pre><code>\u276f docker ps\nCONTAINER ID   IMAGE                              COMMAND                  CREATED         STATUS         PORTS                                                 NAMES\nd5e43f9ff0c2   neurobagel/federation_api:latest   \"/bin/sh -c 'uvicorn\u2026\"   8 seconds ago   Up 8 seconds   0.0.0.0:8080-&gt;8000/tcp, :::8080-&gt;8000/tcp             recipes-federation-1\nf0a26d0ea574   neurobagel/api:latest              \"/usr/src/api_entryp\u2026\"   8 seconds ago   Up 8 seconds   0.0.0.0:8000-&gt;8000/tcp, :::8000-&gt;8000/tcp             recipes-api-1\nd44d0b7359c8   ontotext/graphdb:10.3.1            \"/usr/src/neurobagel\u2026\"   8 seconds ago   Up 8 seconds   0.0.0.0:7200-&gt;7200/tcp, :::7200-&gt;7200/tcp, 7300/tcp   recipes-graph-1\n29a61a2d83de   neurobagel/query_tool:latest       \"/bin/sh -c 'npm run\u2026\"   8 seconds ago   Up 8 seconds   0.0.0.0:3000-&gt;5173/tcp, :::3000-&gt;5173/tcp             recipes-query_federation-1\n</code></pre></p> <p>The <code>docker-compose.yml</code> recipe provides additional service profiles for different deployment use cases (e.g., if you do not need to set up local query federation). Please refer to our service profile documentation for details.</p>"},{"location":"user_guide/getting_started/#next-steps","title":"Next steps","text":"<p> You are now the proud owner of a running Neurobagel node. Here are some things you can do now:</p> <ul> <li>Try the Neurobagel node you just deployed by accessing:<ul> <li>your own query tool at  http://localhost:3000, and reading the query tool usage guide</li> <li>the interactive docs for your node API at http://localhost:8000/docs, and reading the API usage guide</li> </ul> </li> <li>Change the default ports of services</li> <li>Prepare your own dataset for annotation with Neurobagel</li> <li>Add your own data to your Neurobagel graph to search</li> <li>Learn about the different configuration options for your Neurobagel node</li> <li>Hopefully all went well, but if you are experiencing issues, see how to get help</li> </ul>"},{"location":"user_guide/maintaining/","title":"Maintaining a node","text":"<p>Once your Neurobagel node is running and configured correctly, there are some recurring tasks you may have to do to keep it operating correctly.</p>"},{"location":"user_guide/maintaining/#updating-the-neurobagel-services","title":"Updating the Neurobagel services","text":""},{"location":"user_guide/maintaining/#updating-the-neurobagel-docker-images","title":"Updating the Neurobagel Docker images","text":"<p>We are continuously improving Neurobagel tools and services, so you may want to update your Neurobagel node to the latest version to benefit from new features and bug fixes. We always publish our tools as Docker images on Docker Hub.</p> <p>Each Docker image has a semantic version tag (vX.Y.Z), and also two rolling tags:</p> <ul> <li><code>latest</code> (the latest stable release). This is the default tag used in the Neurobagel <code>docker-compose.yml</code> file.</li> <li><code>nightly</code> (the latest build from the main branch). This tag is only used for compatibility testing and should not be used in production.</li> </ul> <p>You can pull the most recent docker images for Neurobagel tools by running:</p> <pre><code>docker compose --profile full_stack pull\n</code></pre> Not sure what version you have? <p>Since <code>latest</code> is a rolling tag, each <code>latest</code> Docker image for a Neurobagel tool includes its corresponding semver number (vX.Y.X) as part of its Docker image labels.</p> <p>You can find the labels for an image you have pulled in the image metadata, e.g.: <pre><code>docker image inspect neurobagel/api:latest\n</code></pre> or, to view only the labels: <pre><code>docker image inspect --format='{{json .Config.Labels}}' neurobagel/api:latest\n</code></pre> In either case, you should see something like this in the output:</p> <p><pre><code>    \"Labels\": {\n        \"org.opencontainers.image.created\": \"https://github.com/neurobagel/api\",\n        \"org.opencontainers.image.revision\": \"01530f467e163f3dff595d3327bc60ba453de47d\",\n        \"org.opencontainers.image.version\": \"v0.3.1\"\n    }\n</code></pre> where <code>\"org.opencontainers.image.version\"</code> refers to the version number.</p> <p><code>docker compose</code> will only pull the images used by the current deployment profile</p> <p>If you don't specify a deployment profile, the default profile (<code>full_stack</code>) will be used, which pulls the images for all Neurobagel services including the node API, federation API, graph store, and graphical query tool.</p> <p>See the deployment profiles  section for more information on the available profiles.</p>"},{"location":"user_guide/maintaining/#restarting-services-after-an-update","title":"Restarting services after an update","text":"<p>Whether you have updated the Docker images, the configuration, or the data of your Neurobagel node, you will need to restart the services to apply the changes.</p> <p>To shut down a running Neurobagel node, navigate to the path on your file system where you have stored the <code>docker-compose.yml</code> file from the initial setup and run:</p> <pre><code>docker compose --profile full_stack down\n</code></pre> <p>Then, to start the services again:</p> <pre><code>docker compose --profile full_stack up -d\n</code></pre> <p>Explicitly specify the deployment profile</p> <p>To avoid unexpected behaviour when running <code>docker compose</code> commands, we recommend always explicitly specifying the deployment profile you want to use with the <code>-p</code> or <code>--profile</code> flag. Otherwise, <code>docker compose</code> will only manage (start, stop, or update) the services in the default profile  (for more info, see Launching a profile).</p>"},{"location":"user_guide/maintaining/#updating-the-data-in-your-graph","title":"Updating the data in your graph","text":"<p>The Neurobagel deployment recipe launches a dedicated graph database that stores the datasets for a single node. The data in this graph database is loaded from the location specified in the  <code>LOCAL_GRAPH_DATA</code> environment variable,  and can be changed at any time.</p> <p>By default, the graph database will only contain an example dataset called <code>BIDS synthetic</code>. </p> <p>If you have followed the initial setup for deploying a Neurobagel node from our Docker Compose recipe, replacing the existing data in your graph database with your own data (or updated data) is a straightforward process.</p> <p>Once you have generated or updated the JSONLD files you want to upload, to update the data in your graph:</p> <ol> <li> <p>Shut down the Neurobagel node, if it is already running</p> <p><pre><code>docker compose --profile full_stack down\n</code></pre>    (or, replace <code>full_stack</code> with the profile you are using)</p> </li> <li> <p>Update the data files in the directory specified by the <code>LOCAL_GRAPH_DATA</code> variable in <code>.env</code>, or simply change the path to a directory containing your JSONLD files.</p> </li> <li> <p>(Re)start the Neurobagel node</p> <pre><code>docker compose --profile full_stack up -d\n</code></pre> </li> </ol> <p>Here are some other common scenarios where you might need to update the data in your graph:</p>"},{"location":"user_guide/maintaining/#following-a-change-in-my-dataset","title":"Following a change in my dataset","text":"<p>When using Neurobagel tools on a dataset that is still undergoing data collection, you may need to update the Neurobagel annotations and/or graph-ready data for the dataset when you want to add new subjects or measurements or to correct mistakes in prior data versions.</p> <p>For any of the below types of changes, you will need to regenerate a graph-ready <code>.jsonld</code> file for the dataset which reflects the change.</p>"},{"location":"user_guide/maintaining/#if-the-phenotypic-tabular-data-have-changed","title":"If the phenotypic (tabular) data have changed","text":"<p>If new variables have been added to the dataset such that there are new columns in the phenotypic TSV you previously annotated using Neurobagel's annotation tool, you will need to:  </p> <ol> <li> <p>Generate an updated data dictionary by annotating the new variables in your TSV following the annotation workflow</p> </li> <li> <p>Generate a new graph-ready data file for the dataset by re-running the CLI on your updated TSV and data dictionary</p> </li> </ol>"},{"location":"user_guide/maintaining/#if-only-the-imaging-data-have-changed","title":"If only the imaging data have changed","text":"<p>If the BIDS data for a dataset have changed without changes in the corresponding phenotypic TSV (e.g., if new modalities or scans have been acquired for a subject), you have two options:</p> <ul> <li>If you still have access to the dataset's phenotypic JSONLD generated from the <code>pheno</code> command of the <code>bagel-cli</code> (step 1), you may choose to rerun only the <code>bids</code> CLI command on the updated BIDS directory. This will generate a new graph-ready data file with updated imaging metadata of subjects.</li> </ul> <p>OR</p> <ul> <li>Rerun the CLI entirely (<code>pheno</code> and <code>bids</code> steps) to generate a new graph-ready data file for the dataset.</li> </ul> <p>When in doubt, rerun both CLI commands.</p>"},{"location":"user_guide/maintaining/#if-only-the-subjects-have-changed","title":"If only the subjects have changed","text":"<p>If subjects have been added to or removed from the dataset but the phenotypic TSV is otherwise unchanged (i.e., only new or removed rows, without changes to the available variables), you will need to:</p> <ul> <li>Generate a new graph-ready data file for the dataset by re-running the CLI (<code>pheno</code> and <code>bids</code> steps) on your updated TSV and existing data dictionary</li> </ul>"},{"location":"user_guide/maintaining/#following-a-change-in-the-neurobagel-data-model","title":"Following a change in the Neurobagel data model","text":"<p>As Neurobagel continues developing the data model, new tool releases may introduce breaking changes to the data model for subject-level information in a <code>.jsonld</code> graph data file. Breaking changes will be highlighted in the release notes.</p> <p>If you have already created <code>.jsonld</code> files for a Neurobagel graph database but want to update your graph data to the latest Neurobagel data model following such a change, you can easily do so by rerunning the CLI on the existing data dictionaries and phenotypic TSVs for the dataset(s) in the graph. This will ensure that if you use the latest version of the Neurobagel CLI to process new datasets (i.e., generate new <code>.jsonld</code> files) for your database, the resulting data will not have conflicts with existing data in the graph.</p> <p>Note that if upgrading to a newer version of the data model, you should regenerate the <code>.jsonld</code> files for all datasets in your existing graph.</p>"},{"location":"user_guide/maintaining/#re-uploading-a-modified-dataset","title":"Re-uploading a modified dataset","text":"<p>To allow easy (re-)uploading of the updated <code>.jsonld</code> for your dataset(s) to a graph database, we recommend making a copy of it in a central directory on your research data fileserver for storing local Neurobagel <code>jsonld</code> datasets. Then, simply follow the steps for uploading/updating a dataset in the graph database.</p>"},{"location":"user_guide/maintaining/#updating-your-graph-backend-configuration","title":"Updating your graph backend configuration","text":""},{"location":"user_guide/maintaining/#updating-existing-database-user-permissions","title":"Updating existing database user permissions","text":"<p>If you want to change database access permissions (e.g., adding or removing access to a database) for an existing user in your GraphDB instance, you must do so manually.</p> <p>Of note, in GraphDB, there is no straightforward REST API call to update a user's database access permissions without replacing the list of their existing database permissions (<code>\"grantedAuthorities\"</code>) entirely.</p> <p>Tip</p> <p>You can verify a user's settings at any time with the following: <pre><code>curl -u \"admin:NewAdminPassword\" http://localhost:7200/rest/security/users/DBUSER\n</code></pre></p> <p>Example: if user <code>DBUSER</code> was granted read/write access to database <code>my_db1</code> with the following command (this command is run by default as part of <code>graphdb_setup.sh</code>):</p> <pre><code>curl -X PUT --header 'Content-Type: application/json' -d '\n{\"grantedAuthorities\": [\"WRITE_REPO_my_db\",\"READ_REPO_my_db\"]}' http://localhost:7200/rest/security/users/DBUSER -u \"admin:NewAdminPassword\"\n</code></pre> <p>To grant <code>DBUSER</code> read/write access to a second database <code>my_db2</code> (while keeping the existing access to <code>my_db1</code>), you would rerun the above <code>curl</code> command with all permissions (existing and new) specified since the existing permissions list will be overwritten:</p> <pre><code>curl -X PUT --header 'Content-Type: application/json' -d '\n{\"grantedAuthorities\": [\"WRITE_REPO_my_db1\",\"READ_REPO_my_db1\", \"WRITE_REPO_my_db2\",\"READ_REPO_my_db2\"]}' http://localhost:7200/rest/security/users/DBUSER -u \"admin:NewAdminPassword\"\n</code></pre> <p>Similarly, to revoke <code>my_db1</code> access so <code>DBUSER</code> only has access to <code>my_db2</code>:</p> <pre><code>curl -X PUT --header 'Content-Type: application/json' -d '\n{\"grantedAuthorities\": [\"WRITE_REPO_my_db2\",\"READ_REPO_my_db2\"]}' http://localhost:7200/rest/security/users/DBUSER -u \"admin:NewAdminPassword\"\n</code></pre> Managing user permissions using the GraphDB Workbench <p>If you are managing multiple GraphDB databases, the web-based administration interface for a GraphDB instance, the Workbench,  might be an easier way to manage user permissions than the REST API. More information on using the GraphDB Workbench can be found here.</p>"},{"location":"user_guide/maintaining/#resetting-your-graphdb-instance","title":"Resetting your GraphDB instance","text":"<p>Each Neurobagel node has its own GraphDB instance, which is used to store the graph data for the node. If you want to reset your graph database and start again from scratch, follow these steps:</p> <ol> <li> <p>Ensure that your Neurobagel node is not running (i.e., shut down the Docker containers for the node).</p> <pre><code>docker compose --profile full_stack down\n</code></pre> <p>If you are not using the <code>full_stack</code> profile, replace <code>full_stack</code> with the name of the profile you are using.</p> </li> <li> <p>Delete the Docker volume that contains the GraphDB data for your node.</p> <pre><code>docker volume rm neurobagel_node_graphdb_home\n</code></pre> <p>Replace <code>neurobagel_node_graphdb_home</code> with the name of the volume created for your node. It is usually named <code>&lt;project_name&gt;_graphdb_home</code> where <code>&lt;project_name&gt;</code> is the name of your Docker Compose stack as defined in <code>COMPOSE_PROJECT_NAME</code> in your <code>.env</code> file.</p> <code>docker volume ls</code> lists all volumes on your system <p>You can use the <code>docker volume ls</code> command to list all volumes on your system. This will help you identify the name of the volume that was created for your Neurobagel node.</p> </li> <li> <p>Launch your Neurobagel node again.</p> <pre><code>docker compose --profile full_stack up -d\n</code></pre> <p>If you are not using the <code>full_stack</code> profile, replace <code>full_stack</code> with the name of the profile you are using.</p> </li> </ol> <p>Some examples of when you might want to do this:</p> <ul> <li>You started but did not complete Neurobagel node setup previously and want to ensure you are using up-to-date instructions and recommended configuration options</li> <li>Your local node has stopped working after a configuration change to your graph database (e.g., your Neurobagel node API no longer starts or responds with an error, but you have confirmed all environment variables you have set should be correct)</li> <li>You need to modify credentials for your graph store</li> </ul> <p>Warning</p> <p>This action will wipe any graph databases and users you previously created!</p>"},{"location":"user_guide/preparing_imaging_data/","title":"Preparing the imaging metadata as a table","text":"<p>To be able to query information about available neuroimaging data in your dataset, you must provide Neurobagel with a <code>.tsv</code> file containing your dataset's neuroimaging metadata. We refer to this as a BIDS metadata table.</p> <p>If your dataset is already in BIDS, the Neurobagel CLI provides a <code>bids2tsv</code> command that will automatically generate this table for you.</p> <p>The BIDS metadata table lists each subject's available imaging files and modality information, using BIDS naming conventions, in the format shown below.</p>"},{"location":"user_guide/preparing_imaging_data/#example-bids-metadata-table","title":"Example BIDS metadata table","text":"sub ses suffix path sub-01 ses-01 T1w /data/bids-examples/synthetic/sub-01/ses-01/anat/sub-01_ses-01_T1w.nii sub-01 ses-01 bold /data/bids-examples/synthetic/sub-01/ses-01/func/sub-01_ses-01_task-rest_bold.nii sub-02 ses-01 T1w /data/bids-examples/synthetic/sub-02/ses-01/anat/sub-02_ses-01_T1w.nii sub-02 ses-01 bold /data/bids-examples/synthetic/sub-02/ses-01/func/sub-02_ses-01_task-rest_bold.nii ... ... ... ..."},{"location":"user_guide/preparing_imaging_data/#about-the-bids-metadata-table","title":"About the BIDS metadata table","text":"<p>The table must include at least four columns named exactly <code>sub</code>, <code>ses</code>, <code>suffix</code>, and <code>path</code> (adapted from BIDS entities).  Additional columns are allowed but will be ignored by Neurobagel.</p> <p>Each row of the table indexes a single image file with the following metadata:</p> <ul> <li><code>sub</code> (required)<ul> <li>Subject ID in the format <code>sub-&lt;label&gt;</code></li> <li>Example: <code>sub-PD123</code></li> </ul> </li> <li><code>ses</code> (optional)<ul> <li>Session ID in the format <code>ses-&lt;label&gt;</code>, if applicable</li> <li>Example: <code>ses-01</code></li> </ul> </li> <li><code>suffix</code> (required)<ul> <li>BIDS suffix corresponding to the modality (or sequence) of the image file </li> <li>See also Supported BIDS imaging modalities</li> <li>Example: <code>T1w</code></li> </ul> </li> <li><code>path</code> (required)<ul> <li>Path to the image file</li> <li>Example: <code>/data/pd/qpn/sub-PD123/ses-01/anat/sub-PD123_ses-01_T1w.nii.gz</code></li> </ul> </li> </ul>"},{"location":"user_guide/preparing_imaging_data/#supported-bids-imaging-modalities","title":"Supported BIDS imaging modalities","text":"<p>Neurobagel currently supports a subset of MRI modalities included in the BIDS specification, but we plan to expand this list for closer alignment with BIDS in the future.</p> <p>Supported modalities are listed below using their BIDS suffixes:</p> <ul> <li><code>dwi</code></li> <li><code>T1w</code></li> <li><code>T2w</code></li> <li><code>bold</code></li> <li><code>asl</code></li> <li><code>eeg</code></li> <li><code>meg</code></li> <li><code>pet</code></li> </ul> <p>Info</p> <p>For more information, see also the \"Modality specific files\" section of the BIDS specification  or consult this master list of image suffixes supported by BIDS.</p>"},{"location":"user_guide/public_nodes/","title":"Search the public nodes","text":"<p>The public query tool at query.neurobagel.org queries the public Neurobagel federation API at federate.neurobagel.org which provides access to all publicly accessible Neurobagel nodes.</p> <p>To start running your own cohort queries,  all you have to do is visit query.neurobagel.org, enter your cohort criteria into the web interface, and click the \"Submit\" button.</p>"},{"location":"user_guide/public_nodes/#public-neurobagel-nodes","title":"Public Neurobagel Nodes","text":"<p>At the moment, the following public Neurobagel nodes are available  (you can query a specific node by selecting it from the dropdown under \"Neurobagel graph\" in the query tool):</p> <ul> <li>OpenNeuro. This node contains a (growing) subset of the datasets on OpenNeuro.   The datasets you can find in this node have been annotated by the community and live in the   OpenNeuroDatasets-JSONLD GitHub organization.</li> <li>International Neuroimaging Data-sharing Initiative (INDI): This node contains public datasets from the INDI project.   At the moment, the following datasets are queryable in the INDI node:<ul> <li>ABIDE 1</li> <li>ABIDE 2</li> <li>ADHD 200</li> <li>Consortium for Reliability and Reproducibility (CoRR) datasets.</li> </ul> </li> <li>Quebec Parkinson Network: This node contains the Quebec Parkinson Network datasets.   Unlike the other two public nodes, the Quebec Parkinson Network node will not return participant level details.</li> </ul> <p>All nodes except for the Quebec Parkinson Network node allow you to download both participant-level information and the corresponding imaging data (where available) for the cohorts you search.  Downloading of imaging data is performed via Datalad.</p>"},{"location":"user_guide/public_nodes/#private-neurobagel-nodes","title":"Private Neurobagel nodes","text":"<p>Nodes that are not purposefully made public are not accessible  outside of the institute or network where they are deployed. If you are interested in deploying a Neurobagel node for your institution, please refer to our deployment documentation for more information.</p>"},{"location":"user_guide/query_tool/","title":"The Neurobagel Query Tool","text":"<p>Neurobagel's query tool is a web interface for searching across a Neurobagel graph based on various subject clinical-demographic and imaging parameters.</p> <p>The query tool is a React application, developed in TypeScript using a variety of tools including Vite, Cypress, and MUI.</p>"},{"location":"user_guide/query_tool/#quickstart","title":"Quickstart","text":"<p>The query tool is hosted at https://query.neurobagel.org/ and interfaces with Neurobagel federation API.</p>"},{"location":"user_guide/query_tool/#local-installation","title":"Local Installation","text":"<p>To run the query tool locally, you have two options:</p> <ol> <li>Use our docker image</li> <li>Do a manual install from the cloned git repo.</li> </ol> <p>but before proceeding with either you need to set the environment variables.</p>"},{"location":"user_guide/query_tool/#mandatory-configuration","title":"Mandatory configuration","text":"Environment variable Type Required Default value if not set Example <code>NB_API_QUERY_URL</code> string Yes - <code>https://federate.neurobagel.org/</code> <code>NB_QUERY_APP_BASE_PATH</code> string No <code>/</code> <code>/query/</code> <code>NB_ENABLE_AUTH</code> boolean No <code>false</code> <code>false</code> <code>NB_QUERY_CLIENT_ID</code> string Yes (if <code>NB_ENABLE_AUTH</code> is set to true) - <code>\"\"</code> <code>NB_QUERY_HEADER_SCRIPT</code> string No <code>\"\"</code> <code>'&lt;script defer data-domain=\"mydomain\" src=\"plausible\"&gt;'</code>"},{"location":"user_guide/query_tool/#nb_api_query_url","title":"<code>NB_API_QUERY_URL</code>","text":"<p>You'll need to set the <code>NB_API_QUERY_URL</code> environment variable required to run the query tool. <code>NB_API_QUERY_URL</code> is the Neurobagel API URL that the query tool uses to send requests to for results.</p>"},{"location":"user_guide/query_tool/#nb_query_app_base_path","title":"<code>NB_QUERY_APP_BASE_PATH</code>","text":"<p>If you are using a custom configuration where the query tool is accessible via a path other than the root (<code>/</code>), you need to set the <code>NB_QUERY_APP_BASE_PATH</code> to your custom path. This ensures that the query tool is correctly rendered and accessible at the specified URL</p>"},{"location":"user_guide/query_tool/#nb_enable_auth","title":"<code>NB_ENABLE_AUTH</code>","text":"<p>If the API you'd like to send queries to requires authentication, you need to set <code>NB_ENABLE_AUTH</code> to <code>true</code> as it is <code>false</code> by default. This will enable authentication flow of the app.</p>"},{"location":"user_guide/query_tool/#nb_query_client_id","title":"<code>NB_QUERY_CLIENT_ID</code>","text":"<p>If the <code>NB_ENABLE_AUTH</code> is set to <code>true</code> (it is <code>false</code> by default), you need to provide a valid client ID for the authentication. At the moment, query tool uses Google for authentication, so you need to obtain a client ID from Google developer console. See documentation for more information.</p>"},{"location":"user_guide/query_tool/#nb_query_header_script","title":"<code>NB_QUERY_HEADER_SCRIPT</code>","text":"<p>If you want to add a custom script to the header of the query tool, you can set the <code>NB_QUERY_HEADER_SCRIPT</code> environment variable to the script you want to add. This script will be added to the header of the query tool. For example, in our production deployment we use the GDPR aware analytics tool Plausible, so we set the <code>NB_QUERY_HEADER_SCRIPT</code> to the script provided by Plausible. When you use this variable, make sure you use single quotes to include the script like so:</p> <pre><code>NB_QUERY_HEADER_SCRIPT='&lt;script defer data-domain=\"mydomain\" src=\"plausible\"&gt;'\n</code></pre>"},{"location":"user_guide/query_tool/#set-the-environment-variables","title":"Set the environment variables","text":"<p>To set environment variables, create a <code>.env</code> file in the root directory and add the environment variables there. If you're running a neurobagel node-API locally on your machine (following the instructions here), your <code>.env</code> file would look something like this:</p> <pre><code>NB_API_QUERY_URL=http://localhost:8000/\n</code></pre> <p>if you're using the remote api, your <code>.env</code> file would look something like this:</p> <pre><code>NB_API_QUERY_URL=https://federate.neurobagel.org/\n</code></pre> <p>if you're using a remote api with authentication, your <code>.env</code> file would look something like this:</p> <pre><code>NB_API_QUERY_URL=https://federate.neurobagel.org/\nNB_ENABLE_AUTH=true\nNB_QUERY_CLIENT_ID=46923719231972-dhsahgasl3123.apps.googleusercontent.com\n</code></pre> <p> The protocol matters here. If you wish to use the Neurobagel remote API, ensure your <code>NB_API_QUERY_URL</code> uses <code>https</code> instead of <code>http</code>.</p>"},{"location":"user_guide/query_tool/#docker-installation","title":"Docker installation","text":"<p>To obtain the query tool docker image, simply run the following command in your terminal:</p> <pre><code>docker pull neurobagel/query_tool:latest\n</code></pre> <p>This Docker image includes the latest release of the query tool and a minimal http server to serve the static tool.</p> <p>To launch the query tool Docker container and pass in the <code>.env</code> file you have created, simply run</p> <pre><code>docker run -p 5173:5173 --env-file=.env neurobagel/query_tool:latest\n</code></pre> <p>Then you can access the query tool at http://localhost:5173</p> <p>Note: the query tool is listening on port <code>5173</code> inside the docker container, replace port <code>5173</code> by the port you would like to expose to the host. For example if you'd like to run the tool on port <code>8000</code> of your machine you can run the following command:</p> <pre><code>docker run -p 8000:5173 --env-file=.env neurobagel/query_tool:latest\n</code></pre>"},{"location":"user_guide/query_tool/#manual-installation","title":"Manual installation","text":"<p>To install the query tool directly, you'll need node package manager (npm) and Node.js. You can find the instructions on installing npm and node in the official documentation.</p> <p>Once you have npm and node installed, you'll need to install the dependencies outlined in the package.json file. You can do so by running the following command:</p> <pre><code>npm install\n</code></pre> <p>To launch the tool in developer mode run the following command:</p> <pre><code>npm run dev\n</code></pre> <p>You can also build and then run the tool from (production) build of the application by running the following command:</p> <pre><code>npm run build &amp;&amp; npm run preview\n</code></pre> <p>You can verify the tool is running by watching for the` info messages from Vite regarding environment, rendering, and what port the tool is running on in your terminal.</p>"},{"location":"user_guide/query_tool/#developer-setup","title":"Developer setup","text":"<p>Having installed the dependencies, run the following command to enable husky <code>pre-commit</code> and <code>post-merge</code> hooks:</p> <pre><code>npx husky init\n</code></pre>"},{"location":"user_guide/query_tool/#docker-compose-testing-environment-for-development","title":"Docker compose testing environment for development","text":"<p>Since the query tool relies on other neurobagel tools to function, their presence is often required during development. To facilitate this, a docker compose containing a complete testing environment has been created. To use it follow the steps below:</p> <ol> <li>Install <code>recipes</code> and <code>neurobagel_examples</code> submodules:</li> </ol> <pre><code>git submodule init\ngit submodule update\n</code></pre> <ol> <li>Pull the latest images and bring up the stack using the <code>test</code> profile:</li> </ol> <pre><code>docker compose --profile test pull &amp;&amp; docker compose --profile test up -d\n</code></pre> <p>NOTE: Make sure your .env file in the root directory doesn't contain any of the environment variables used in the docker compose file as it will conflict with the configuration, since docker compose will try to use .env by default.</p>"},{"location":"user_guide/query_tool/#usage","title":"Usage","text":"<p>To define a cohort, set your inclusion criteria using the following:</p> <ul> <li>Age: Minimum and/or maximum age (in years) of participant that should be included in the results.</li> <li>Sex: Sex of participant that should be included in the results.</li> <li>Diagnosis: Diagnosis of participant that should be included in the results</li> <li>Minimum number of imaging sessions: Minimum number of imaging sessions that participant should have to be included in the results.</li> <li>Minimum number of phenotypic sessions: Minimum number of phenotypic sessions that participant should have to be included in the results.</li> <li>Assessment tool: Non-imaging assessment completed by participant that should be included in the results.</li> <li>Imaging modality: Imaging modality of participant scans that should be included in the results.</li> <li>Pipeline name: Name of the pipeline used to process subject scans.</li> <li>Pipeline version: Version of the pipeline used to process subject scans.</li> </ul> <p>Once you've defined your criteria, submit them as a query and the query tool will display the results.\\</p>"},{"location":"user_guide/query_tool/#downloading-query-results","title":"Downloading query results","text":"<p>For a given query, there are two formats of query results that users can download as a TSV file. At least one dataset matching the query must be selected in the results panel in order to download the query results.</p>"},{"location":"user_guide/query_tool/#harmonized-tsv-data-with-descriptive-labels","title":"Harmonized TSV data with descriptive labels","text":"<p>The default TSV available for download describes the available harmonized attributes and metadata for subjects matching the query, from the (selected) matching datasets.  Harmonized data are provided as standardized vocabulary-derived labels for readability.</p> <p>Each row corresponds to a single matching subject session, except for datasets configured to only return aggregate results.</p> Example query result TSV DatasetName PortalURI NumMatchingSubjects SubjectID SessionID ImagingSessionPath SessionType NumMatchingPhenotypicSessions NumMatchingImagingSessions Age Sex Diagnosis Assessment SessionImagingModalities SessionCompletedPipelines DatasetImagingModalities DatasetPipelines Balloon Analog Risk-taking Task https://github.com/OpenNeuroDatasets-JSONLD/ds000001.git 16 protected protected protected protected protected protected protected protected protected protected protected protected Functional MRI,T1 Weighted,T2 Weighted nan BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-01 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-01/ses-01 Imaging 2 2 nan nan nan nan T1 Weighted,Functional MRI fmriprep 23.1.3,freesurfer 7.3.2 Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-01 ses-01 nan Phenotypic 2 2 34.1 female Healthy Control Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-01 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-01/ses-02 Imaging 2 2 nan nan nan nan T1 Weighted,Functional MRI nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-01 ses-02 nan Phenotypic 2 2 35.3 female Healthy Control Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-02 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-02/ses-01 Imaging 2 2 nan nan nan nan T1 Weighted,Functional MRI fmriprep 23.1.3,freesurfer 7.3.2 Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-02 ses-01 nan Phenotypic 2 2 nan male Attention deficit hyperactivity disorder Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-02 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-02/ses-02 Imaging 2 2 nan nan nan nan T1 Weighted,Functional MRI freesurfer 7.3.2 Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-02 ses-02 nan Phenotypic 2 2 39 male Attention deficit hyperactivity disorder Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-03 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-03/ses-01 Imaging 2 2 nan nan nan nan T1 Weighted,Functional MRI nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-03 ses-01 nan Phenotypic 2 2 22.1 nan nan Montreal cognitive assessment nan nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-03 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-03/ses-02 Imaging 2 2 nan nan nan nan T1 Weighted,Functional MRI nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-03 ses-02 nan Phenotypic 2 2 23.2 nan Attention deficit hyperactivity disorder Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-04 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-04/ses-01 Imaging 2 2 nan nan nan nan T1 Weighted,Functional MRI nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-04 ses-01 nan Phenotypic 2 2 21.1 female Healthy Control Unified Parkinsons disease rating scale score nan nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-04 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-04/ses-02 Imaging 2 2 nan nan nan nan T1 Weighted,Functional MRI nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-04 ses-02 nan Phenotypic 2 2 22.3 female Healthy Control Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-05 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-05/ses-01 Imaging 2 2 nan nan nan nan T1 Weighted,Functional MRI nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-05 ses-01 nan Phenotypic 2 2 42.5 male Attention deficit hyperactivity disorder Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-05 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-05/ses-02 Imaging 2 2 nan nan nan nan T1 Weighted,Functional MRI nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-05 ses-02 nan Phenotypic 2 2 43.2 male Attention deficit hyperactivity disorder Montreal cognitive assessment,Unified Parkinsons disease rating scale score nan nan Functional MRI,T1 Weighted fmriprep 23.1.3,freesurfer 7.3.2 Classification learning https://github.com/OpenNeuroDatasets-JSONLD/ds000002.git 17 protected protected protected protected protected protected protected protected protected protected protected protected Functional MRI,T1 Weighted,T2 Weighted nan <p>Columns in the TSV are described below: * = required values</p> Column name Description DatasetName * name of the dataset PortalURI URL to a website or page about the dataset NumMatchingSubjects * (dataset-level) total number of subjects matching the query in the dataset SubjectID * subject label SessionID * session label ImagingSessionPath (imaging sessions only) path to the session directory, or subject directory if only one session exists. Either an absolute path from the filesystem root where the dataset is stored, or a relative path from the dataset root for DataLad datasets. SessionType * type of data acquired in the session, either <code>ImagingSession</code> or <code>PhenotypicSession</code>. Represents the nature of data being described, without denoting specific time or visits. e.g., A session in which both imaging and non-imaging data were acquired would be represented by separate rows, one per type. Age subject age Sex subject sex Diagnosis list of diagnoses for subject Assessment list of assessments completed by subject NumMatchingPhenotypicSessions * (subject-level) total number of phenotypic sessions for the subject which match the query NumMatchingImagingSessions * (subject-level) total number of imaging sessions for the subject which match the query SessionImagingModalities (imaging sessions only) imaging modalities acquired in the session SessionCompletedPipelines (imaging sessions only) processing pipelines completed for the session DatasetImagingModalities (dataset-level) imaging modalities acquired in at least one session in the dataset DatasetPipelines (dataset-level) processing pipelines completed for at least one session in the dataset"},{"location":"user_guide/query_tool/#harmonized-tsv-data-with-uris","title":"Harmonized TSV data with URIs","text":"<p>A machine-optimized version of the query results, containing URIs instead of descriptive labels for harmonized attributes and metadata of matching subjects, is also available for download as a TSV.</p> <p>Each row corresponds to a single matching subject session, except for datasets configured to only return aggregate results.</p> Example query result TSV DatasetName PortalURI NumMatchingSubjects SubjectID SessionID ImagingSessionPath SessionType NumMatchingPhenotypicSessions NumMatchingImagingSessions Age Sex Diagnosis Assessment SessionImagingModalities SessionCompletedPipelines DatasetImagingModalities DatasetPipelines Balloon Analog Risk-taking Task https://github.com/OpenNeuroDatasets-JSONLD/ds000001.git 16 protected protected protected protected protected protected protected protected protected protected protected protected http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#T2Weighted nan BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-01 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-01/ses-01 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-01 ses-01 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 34.1 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C94342 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-01 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-01/ses-02 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-01 ses-02 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 35.3 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C94342 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-02 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-02/ses-01 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-02 ses-01 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 nan http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/406506008 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-02 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-02/ses-02 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-02 ses-02 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 39 http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/406506008 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-03 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-03/ses-01 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-03 ses-01 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 22.1 nan nan http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-03 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-03/ses-02 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-03 ses-02 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 23.2 nan http://purl.bioontology.org/ontology/SNOMEDCT/406506008 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-04 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-04/ses-01 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-04 ses-01 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 21.1 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C94342 http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-04 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-04/ses-02 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-04 ses-02 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 22.3 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#C94342 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-05 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-05/ses-01 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-05 ses-01 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 42.5 http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/406506008 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-05 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-05/ses-02 http://neurobagel.org/vocab/ImagingSession 2 2 nan nan nan nan http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#FlowWeighted nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 BIDS synthetic https://github.com/bids-standard/bids-examples 5 sub-05 ses-02 nan http://neurobagel.org/vocab/PhenotypicSession 2 2 43.2 http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/406506008 http://purl.bioontology.org/ontology/SNOMEDCT/859351000000102,http://purl.bioontology.org/ontology/SNOMEDCT/342061000000106 nan nan http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted https://github.com/nipoppy/pipeline-catalog/tree/main/processing/fmriprep 23.1.3,https://github.com/nipoppy/pipeline-catalog/tree/main/processing/freesurfer 7.3.2 Classification learning https://github.com/OpenNeuroDatasets-JSONLD/ds000002.git 17 protected protected protected protected protected protected protected protected protected protected protected protected http://purl.org/nidash/nidm#FlowWeighted,http://purl.org/nidash/nidm#T1Weighted,http://purl.org/nidash/nidm#T2Weighted nan <p>This file contains the same columns and data as the descriptive query results TSV.  However, the harmonized terms in the following columns are provided in their raw URI form instead of as descriptive labels:</p> Column name SessionType Sex Diagnosis Assessment SessionImagingModalities SessionCompletedPipelines DatasetImagingModalities DatasetPipelines"},{"location":"user_guide/query_tool/#protected-subject-level-results-for-aggregate-datasets","title":"<code>protected</code> subject-level results for aggregate datasets","text":"<p>Example</p> <p>For examples of aggregated matching dataset results, see the last rows of the example query result TSV in the previous two sections.</p> <p>A row in a query result TSV may show <code>protected</code> for all columns except for <code>DatasetName</code>, <code>PortalURI</code>, and other dataset-level columns. This means the source graph database (node) has been configured (via its corresponding Neurobagel node API) to return only aggregate information about matching subjects e.g., for data privacy reasons.</p> <p>More information on this configuration setting, called <code>NB_RETURN_AGG</code>, and how to change it for a node can be found here.</p>"},{"location":"user_guide/query_tool/#testing","title":"Testing","text":"<p>The query tool utilizes Cypress framework for testing.</p> <p>To run the tests execute the following command:</p> <pre><code>npx cypress open\n</code></pre>"},{"location":"user_guide/query_tool/#license","title":"License","text":"<p>The query tool is released under the terms of the MIT License</p>"}]}